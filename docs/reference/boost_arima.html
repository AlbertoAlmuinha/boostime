<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>General Interface for Boosted ARIMA Regression Models — boost_arima • boostime</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="General Interface for Boosted ARIMA Regression Models — boost_arima" />
<meta property="og:description" content="boost_arima() is a way to generate a specification of a time series model
that uses boosting to improve modeling errors (residuals) on Exogenous Regressors.
It works with both &quot;automated&quot; ARIMA (auto.arima) and standard ARIMA (arima).
The main algorithms are:
Auto ARIMA + Catboost Errors (engine = auto_arima_catboost, default)
ARIMA + Catboost Errors (engine = arima_catboost)
Auto ARIMA + LightGBM Errors (engine = auto_arima_lightgbm)
ARIMA + LightGBM Errors (engine = arima_lightgbm)

" />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">boostime</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/getting-started.html">Getting Started with Boostime</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/AlbertoAlmuinha/boostime/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>General Interface for Boosted ARIMA Regression Models</h1>
    <small class="dont-index">Source: <a href='https://github.com/AlbertoAlmuinha/boostime/blob/master/R/parsnip-arima_boost.R'><code>R/parsnip-arima_boost.R</code></a></small>
    <div class="hidden name"><code>boost_arima.Rd</code></div>
    </div>

    <div class="ref-description">
    <p><code>boost_arima()</code> is a way to generate a <em>specification</em> of a time series model
that uses boosting to improve modeling errors (residuals) on Exogenous Regressors.
It works with both "automated" ARIMA (<code>auto.arima</code>) and standard ARIMA (<code>arima</code>).
The main algorithms are:</p><ul>
<li><p>Auto ARIMA + Catboost Errors (engine = <code>auto_arima_catboost</code>, default)</p></li>
<li><p>ARIMA + Catboost Errors (engine = <code>arima_catboost</code>)</p></li>
<li><p>Auto ARIMA + LightGBM Errors (engine = <code>auto_arima_lightgbm</code>)</p></li>
<li><p>ARIMA + LightGBM Errors (engine = <code>arima_lightgbm</code>)</p></li>
</ul>

    </div>

    <pre class="usage"><span class='fu'>boost_arima</span><span class='op'>(</span>
  mode <span class='op'>=</span> <span class='st'>"regression"</span>,
  seasonal_period <span class='op'>=</span> <span class='cn'>NULL</span>,
  non_seasonal_ar <span class='op'>=</span> <span class='cn'>NULL</span>,
  non_seasonal_differences <span class='op'>=</span> <span class='cn'>NULL</span>,
  non_seasonal_ma <span class='op'>=</span> <span class='cn'>NULL</span>,
  seasonal_ar <span class='op'>=</span> <span class='cn'>NULL</span>,
  seasonal_differences <span class='op'>=</span> <span class='cn'>NULL</span>,
  seasonal_ma <span class='op'>=</span> <span class='cn'>NULL</span>,
  tree_depth <span class='op'>=</span> <span class='cn'>NULL</span>,
  learn_rate <span class='op'>=</span> <span class='cn'>NULL</span>,
  mtry <span class='op'>=</span> <span class='cn'>NULL</span>,
  trees <span class='op'>=</span> <span class='cn'>NULL</span>,
  min_n <span class='op'>=</span> <span class='cn'>NULL</span>,
  sample_size <span class='op'>=</span> <span class='cn'>NULL</span>,
  loss_reduction <span class='op'>=</span> <span class='cn'>NULL</span>
<span class='op'>)</span></pre>

    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>mode</th>
      <td><p>A single character string for the type of model.
The only possible value for this model is "regression".</p></td>
    </tr>
    <tr>
      <th>seasonal_period</th>
      <td><p>A seasonal frequency. Uses "auto" by default.
A character phrase of "auto" or time-based phrase of "2 weeks"
can be used if a date or date-time variable is provided.
See Fit Details below.</p></td>
    </tr>
    <tr>
      <th>non_seasonal_ar</th>
      <td><p>The order of the non-seasonal auto-regressive (AR) terms. Often denoted "p" in pdq-notation.</p></td>
    </tr>
    <tr>
      <th>non_seasonal_differences</th>
      <td><p>The order of integration for non-seasonal differencing. Often denoted "d" in pdq-notation.</p></td>
    </tr>
    <tr>
      <th>non_seasonal_ma</th>
      <td><p>The order of the non-seasonal moving average (MA) terms. Often denoted "q" in pdq-notation.</p></td>
    </tr>
    <tr>
      <th>seasonal_ar</th>
      <td><p>The order of the seasonal auto-regressive (SAR) terms. Often denoted "P" in PDQ-notation.</p></td>
    </tr>
    <tr>
      <th>seasonal_differences</th>
      <td><p>The order of integration for seasonal differencing. Often denoted "D" in PDQ-notation.</p></td>
    </tr>
    <tr>
      <th>seasonal_ma</th>
      <td><p>The order of the seasonal moving average (SMA) terms. Often denoted "Q" in PDQ-notation.</p></td>
    </tr>
    <tr>
      <th>tree_depth</th>
      <td><p>The maximum depth of the tree (i.e. number of splits).</p></td>
    </tr>
    <tr>
      <th>learn_rate</th>
      <td><p>The rate at which the boosting algorithm adapts from iteration-to-iteration.</p></td>
    </tr>
    <tr>
      <th>mtry</th>
      <td><p>The number of predictors that will be randomly sampled at each split when creating the tree models.</p></td>
    </tr>
    <tr>
      <th>trees</th>
      <td><p>The number of trees contained in the ensemble.</p></td>
    </tr>
    <tr>
      <th>min_n</th>
      <td><p>The minimum number of data points in a node that is required for the node to be split further.</p></td>
    </tr>
    <tr>
      <th>sample_size</th>
      <td><p>The amount of data exposed to the fitting routine.</p></td>
    </tr>
    <tr>
      <th>loss_reduction</th>
      <td><p>The reduction in the loss function required to split further.</p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    <p>The data given to the function are not saved and are only used
to determine the <em>mode</em> of the model. For <code>boost_arima()</code>, the
mode will always be "regression".</p>
<p>The model can be created using the <code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code> function using the
following <em>engines</em>:</p><ul>
<li><p>"auto_arima_catboost" (default) - Connects to <code><a href='https://pkg.robjhyndman.com/forecast/reference/auto.arima.html'>forecast::auto.arima()</a></code> and <a href='https://rdrr.io/pkg/catboost/man/catboost.train.html'>catboost::catboost.train</a></p></li>
<li><p>"arima_catboost" - Connects to <code><a href='https://pkg.robjhyndman.com/forecast/reference/Arima.html'>forecast::Arima()</a></code> and <a href='https://rdrr.io/pkg/catboost/man/catboost.train.html'>catboost::catboost.train</a></p></li>
<li><p>"auto_arima_lightgbm" - Connects to <code><a href='https://pkg.robjhyndman.com/forecast/reference/auto.arima.html'>forecast::auto.arima()</a></code> and <code><a href='https://rdrr.io/pkg/lightgbm/man/lgb.train.html'>lightgbm::lgb.train()</a></code></p></li>
<li><p>"arima_lightgbm" - Connects to <code><a href='https://pkg.robjhyndman.com/forecast/reference/Arima.html'>forecast::Arima()</a></code> and <code><a href='https://rdrr.io/pkg/lightgbm/man/lgb.train.html'>lightgbm::lgb.train()</a></code></p></li>
</ul>

<p><strong>Main Arguments</strong></p>
<p>The main arguments (tuning parameters) for the <strong>ARIMA model</strong> are:</p><ul>
<li><p><code>seasonal_period</code>: The periodic nature of the seasonality. Uses "auto" by default.</p></li>
<li><p><code>non_seasonal_ar</code>: The order of the non-seasonal auto-regressive (AR) terms.</p></li>
<li><p><code>non_seasonal_differences</code>: The order of integration for non-seasonal differencing.</p></li>
<li><p><code>non_seasonal_ma</code>: The order of the non-seasonal moving average (MA) terms.</p></li>
<li><p><code>seasonal_ar</code>: The order of the seasonal auto-regressive (SAR) terms.</p></li>
<li><p><code>seasonal_differences</code>: The order of integration for seasonal differencing.</p></li>
<li><p><code>seasonal_ma</code>: The order of the seasonal moving average (SMA) terms.</p></li>
</ul>

<p>The main arguments (tuning parameters) for the model <strong>Catboost/LightGBM model</strong> are:</p><ul>
<li><p><code>tree_depth</code>: The maximum depth of the tree (i.e. number of splits).</p></li>
<li><p><code>learn_rate</code>: The rate at which the boosting algorithm adapts from iteration-to-iteration.</p></li>
<li><p><code>mtry</code>: The number of predictors that will be randomly sampled at each split when creating the tree models.</p></li>
<li><p><code>trees</code>: The number of trees contained in the ensemble.</p></li>
<li><p><code>min_n</code>: The minimum number of data points in a node that is required for the node to be split further.</p></li>
<li><p><code>sample_size</code>: The amount of data exposed to the fitting routine.</p></li>
<li><p><code>loss_reduction</code>: The reduction in the loss function required to split further.</p></li>
</ul>

<p>These arguments are converted to their specific names at the
time that the model is fit.</p>
<p>Other options and argument can be
set using <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code> (See Engine Details below).</p>
<p>If parameters need to be modified, <code><a href='https://rdrr.io/r/stats/update.html'>update()</a></code> can be used
in lieu of recreating the object from scratch.</p>
    <h2 class="hasAnchor" id="engine-details"><a class="anchor" href="#engine-details"></a>Engine Details</h2>

    


<p>The standardized parameter names in <code>boostime</code> can be mapped to their original names in each engine:</p>
<p>Model 1: ARIMA:</p><table class='table'>
<tr><td>boostime</td><td>forecast::auto.arima</td><td>forecast::Arima</td></tr>
<tr><td>seasonal_period</td><td>ts(frequency)</td><td>ts(frequency)</td></tr>
<tr><td>non_seasonal_ar, non_seasonal_differences, non_seasonal_ma</td><td>max.p(5), max.d(2), max.q(5)</td><td>order = c(p(0), d(0), q(0))</td></tr>
<tr><td>seasonal_ar, seasonal_differences, seasonal_ma</td><td>max.P(2), max.D(1), max.Q(2)</td><td>seasonal = c(P(0), D(0), Q(0))</td></tr>
</table>



<p>Model 2: Catboost / LightGBM:</p><table class='table'>
<tr><td>boostime</td><td>catboost::catboost.train</td><td>lightgbm::lgb.train</td></tr>
<tr><td>tree_depth</td><td>depth</td><td>max_depth</td></tr>
<tr><td>learn_rate</td><td>learning_rate</td><td>learning_rate</td></tr>
<tr><td>mtry</td><td>rsm</td><td>feature_fraction</td></tr>
<tr><td>trees</td><td>iterations</td><td>num_iterations</td></tr>
<tr><td>min_n</td><td>min_data_in_leaf</td><td>min_data_in_leaf</td></tr>
<tr><td>loss_reduction</td><td>None</td><td>min_gain_to_split</td></tr>
<tr><td>sample_size</td><td>subsample</td><td>bagging_fraction</td></tr>
</table>



<p>Other options can be set using <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code>.</p>
<p><strong>auto_arima_catboost (default engine)</strong></p>
<p>Model 1: Auto ARIMA (<code><a href='https://pkg.robjhyndman.com/forecast/reference/auto.arima.html'>forecast::auto.arima</a></code>):</p><pre><span class='co'>## function (y, d = NA, D = NA, max.p = 5, max.q = 5, max.P = 2, max.Q = 2, </span>
<span class='co'>##     max.order = 5, max.d = 2, max.D = 1, start.p = 2, start.q = 2, start.P = 1, </span>
<span class='co'>##     start.Q = 1, stationary = FALSE, seasonal = TRUE, ic = c("aicc", "aic", </span>
<span class='co'>##         "bic"), stepwise = TRUE, nmodels = 94, trace = FALSE, approximation = (length(x) &gt; </span>
<span class='co'>##         150 | frequency(x) &gt; 12), method = NULL, truncate = NULL, xreg = NULL, </span>
<span class='co'>##     test = c("kpss", "adf", "pp"), test.args = list(), seasonal.test = c("seas", </span>
<span class='co'>##         "ocsb", "hegy", "ch"), seasonal.test.args = list(), allowdrift = TRUE, </span>
<span class='co'>##     allowmean = TRUE, lambda = NULL, biasadj = FALSE, parallel = FALSE, </span>
<span class='co'>##     num.cores = 2, x = y, ...)</span>
</pre>

<p>Parameter Notes:</p><ul>
<li><p>All values of nonseasonal pdq and seasonal PDQ are maximums.
The <code>auto.arima</code> will select a value using these as an upper limit.</p></li>
<li><p><code>xreg</code> - This should not be used since Catboost will be doing the regression</p></li>
</ul>

<p>Model 2: Catboost (<code><a href='https://rdrr.io/pkg/catboost/man/catboost.train.html'>catboost::catboost.train</a></code>):</p><pre><span class='co'>## function (learn_pool, test_pool = NULL, params = list())</span>
</pre>

<p>Parameter Notes:</p><ul>
<li><p>Catboost uses a <code>params = list()</code> to capture.
Parsnip / Timeboost automatically sends any args provided as <code>...</code> inside of <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code> to
the <code>params = list(...)</code>.</p></li>
</ul>

    <h2 class="hasAnchor" id="fit-details"><a class="anchor" href="#fit-details"></a>Fit Details</h2>

    


<p><strong>Date and Date-Time Variable</strong></p>
<p>It's a requirement to have a date or date-time variable as a predictor.
The <code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code> interface accepts date and date-time features and handles them internally.</p><ul>
<li><p><code><a href='https://generics.r-lib.org/reference/fit.html'>fit(y ~ date)</a></code></p></li>
</ul>

<p><em>Seasonal Period Specification</em></p>
<p>The period can be non-seasonal (<code>seasonal_period = 1</code>) or seasonal (e.g. <code>seasonal_period = 12</code> or <code>seasonal_period = "12 months"</code>).
There are 3 ways to specify:</p><ol>
<li><p><code>seasonal_period = "auto"</code>: A period is selected based on the periodicity of the data (e.g. 12 if monthly)</p></li>
<li><p><code>seasonal_period = 12</code>: A numeric frequency. For example, 12 is common for monthly data</p></li>
<li><p><code>seasonal_period = "1 year"</code>: A time-based phrase. For example, "1 year" would convert to 12 for monthly data.</p></li>
</ol>

<p><strong>Univariate (No xregs, Exogenous Regressors):</strong></p>
<p>For univariate analysis, you must include a date or date-time feature. Simply use:</p><ul>
<li><p>Formula Interface (recommended): <code><a href='https://generics.r-lib.org/reference/fit.html'>fit(y ~ date)</a></code> will ignore xreg's.</p></li>
</ul>

<p><strong>Multivariate (xregs, Exogenous Regressors)</strong></p>
<p>The <code>xreg</code> parameter is populated using the <code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code> or <code><a href='https://generics.r-lib.org/reference/fit_xy.html'>fit_xy()</a></code> function:</p><ul>
<li><p>Only <code>factor</code>, <code>ordered factor</code>, and <code>numeric</code> data will be used as xregs.</p></li>
<li><p>Date and Date-time variables are not used as xregs</p></li>
<li><p><code>character</code> data should be converted to factor.</p></li>
</ul>

<p><em>Xreg Example:</em> Suppose you have 3 features:</p><ol>
<li><p><code>y</code> (target)</p></li>
<li><p><code>date</code> (time stamp),</p></li>
<li><p><code>month.lbl</code> (labeled month as a ordered factor).</p></li>
</ol>

<p>The <code>month.lbl</code> is an exogenous regressor that can be passed to the <code>arima_boost()</code> using
<code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code>:</p><ul>
<li><p><code><a href='https://generics.r-lib.org/reference/fit.html'>fit(y ~ date + month.lbl)</a></code> will pass <code>month.lbl</code> on as an exogenous regressor.</p></li>
<li><p><code><a href='https://generics.r-lib.org/reference/fit_xy.html'>fit_xy(data[,c("date", "month.lbl")], y = data$y)</a></code> will pass x, where x is a data frame containing <code>month.lbl</code>
and the <code>date</code> feature. Only <code>month.lbl</code> will be used as an exogenous regressor.</p></li>
</ul>

<p>Note that date or date-time class values are excluded from <code>xreg</code>.</p>
    <h2 class="hasAnchor" id="see-also"><a class="anchor" href="#see-also"></a>See also</h2>

    <div class='dont-index'><p><code><a href='https://parsnip.tidymodels.org/reference/fit.html'>fit.model_spec()</a></code>, <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code></p></div>

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://tidyverse.tidyverse.org'>tidyverse</a></span><span class='op'>)</span>
</div><div class='output co'>#&gt; <span class='message'>-- <span style='font-weight: bold;'>Attaching packages</span><span> --------------------------------------- tidyverse 1.3.1 --</span></div><div class='output co'>#&gt; <span class='message'></span><span style='color: #00BB00;'>v</span><span> </span><span style='color: #0000BB;'>ggplot2</span><span> 3.3.3     </span><span style='color: #00BB00;'>v</span><span> </span><span style='color: #0000BB;'>purrr  </span><span> 0.3.4</span>
#&gt; <span class='message'></span><span style='color: #00BB00;'>v</span><span> </span><span style='color: #0000BB;'>tibble </span><span> 3.1.2     </span><span style='color: #00BB00;'>v</span><span> </span><span style='color: #0000BB;'>dplyr  </span><span> 1.0.6</span>
#&gt; <span class='message'></span><span style='color: #00BB00;'>v</span><span> </span><span style='color: #0000BB;'>tidyr  </span><span> 1.1.3     </span><span style='color: #00BB00;'>v</span><span> </span><span style='color: #0000BB;'>stringr</span><span> 1.4.0</span>
#&gt; <span class='message'></span><span style='color: #00BB00;'>v</span><span> </span><span style='color: #0000BB;'>readr  </span><span> 1.4.0     </span><span style='color: #00BB00;'>v</span><span> </span><span style='color: #0000BB;'>forcats</span><span> 0.5.1</span></div><div class='output co'>#&gt; <span class='message'>-- </span><span style='font-weight: bold;'>Conflicts</span><span> ------------------------------------------ tidyverse_conflicts() --</span>
#&gt; <span class='message'></span><span style='color: #BB0000;'>x</span><span> </span><span style='color: #0000BB;'>dplyr</span><span>::</span><span style='color: #00BB00;'>filter()</span><span> masks </span><span style='color: #0000BB;'>stats</span><span>::filter()</span>
#&gt; <span class='message'></span><span style='color: #BB0000;'>x</span><span> </span><span style='color: #0000BB;'>dplyr</span><span>::</span><span style='color: #00BB00;'>lag()</span><span>    masks </span><span style='color: #0000BB;'>stats</span><span>::lag()</span></div><div class='input'><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://lubridate.tidyverse.org'>lubridate</a></span><span class='op'>)</span>
</div><div class='output co'>#&gt; <span class='message'></span>
#&gt; <span class='message'>Attaching package: 'lubridate'</span></div><div class='output co'>#&gt; <span class='message'>The following objects are masked from 'package:base':</span>
#&gt; <span class='message'></span>
#&gt; <span class='message'>    date, intersect, setdiff, union</span></div><div class='input'><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://parsnip.tidymodels.org'>parsnip</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://rsample.tidymodels.org'>rsample</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://github.com/business-science/timetk'>timetk</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://github.com/AlbertoAlmuinha/boostime'>boostime</a></span><span class='op'>)</span>


<span class='co'># Data</span>
<span class='va'>m750</span> <span class='op'>&lt;-</span> <span class='va'>m4_monthly</span> <span class='op'>%&gt;%</span> <span class='fu'><a href='https://rdrr.io/r/stats/filter.html'>filter</a></span><span class='op'>(</span><span class='va'>id</span> <span class='op'>==</span> <span class='st'>"M750"</span><span class='op'>)</span>

<span class='co'># Split Data 80/20</span>
<span class='va'>splits</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rsample.tidymodels.org/reference/initial_split.html'>initial_time_split</a></span><span class='op'>(</span><span class='va'>m750</span>, prop <span class='op'>=</span> <span class='fl'>0.9</span><span class='op'>)</span>

<span class='co'># MODEL SPEC ----</span>

<span class='co'># Set engine and boosting parameters</span>
<span class='va'>model_spec</span> <span class='op'>&lt;-</span> <span class='fu'>boost_arima</span><span class='op'>(</span>

    <span class='co'># ARIMA args</span>
    seasonal_period <span class='op'>=</span> <span class='fl'>12</span>,
    non_seasonal_ar <span class='op'>=</span> <span class='fl'>0</span>,
    non_seasonal_differences <span class='op'>=</span> <span class='fl'>1</span>,
    non_seasonal_ma <span class='op'>=</span> <span class='fl'>1</span>,
    seasonal_ar     <span class='op'>=</span> <span class='fl'>0</span>,
    seasonal_differences <span class='op'>=</span> <span class='fl'>1</span>,
    seasonal_ma     <span class='op'>=</span> <span class='fl'>1</span>,

    <span class='co'># Catboost Args</span>
    tree_depth <span class='op'>=</span> <span class='fl'>6</span>,
    learn_rate <span class='op'>=</span> <span class='fl'>0.1</span>
<span class='op'>)</span> <span class='op'>%&gt;%</span>
    <span class='fu'><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine</a></span><span class='op'>(</span>engine <span class='op'>=</span> <span class='st'>"arima_catboost"</span><span class='op'>)</span>

<span class='co'># FIT ----</span>
<span class='va'>model_fit_boosted</span> <span class='op'>&lt;-</span> <span class='va'>model_spec</span> <span class='op'>%&gt;%</span>
    <span class='fu'><a href='https://generics.r-lib.org/reference/fit.html'>fit</a></span><span class='op'>(</span><span class='va'>value</span> <span class='op'>~</span> <span class='va'>date</span> <span class='op'>+</span> <span class='fu'><a href='https://rdrr.io/r/base/numeric.html'>as.numeric</a></span><span class='op'>(</span><span class='va'>date</span><span class='op'>)</span> <span class='op'>+</span> <span class='fu'><a href='http://lubridate.tidyverse.org/reference/month.html'>month</a></span><span class='op'>(</span><span class='va'>date</span>, label <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span>,
        data <span class='op'>=</span> <span class='fu'><a href='https://rsample.tidymodels.org/reference/initial_split.html'>training</a></span><span class='op'>(</span><span class='va'>splits</span><span class='op'>)</span><span class='op'>)</span>
</div><div class='output co'>#&gt; 0:	learn: 154.0396762	total: 159ms	remaining: 2m 38s
#&gt; 1:	learn: 154.0186706	total: 167ms	remaining: 1m 23s
#&gt; 2:	learn: 154.0186705	total: 167ms	remaining: 55.4s
#&gt; 3:	learn: 153.9027550	total: 172ms	remaining: 42.7s
#&gt; 4:	learn: 153.8865200	total: 175ms	remaining: 34.8s
#&gt; 5:	learn: 153.7047078	total: 178ms	remaining: 29.4s
#&gt; 6:	learn: 153.7046835	total: 178ms	remaining: 25.2s
#&gt; 7:	learn: 153.5498784	total: 186ms	remaining: 23.1s
#&gt; 8:	learn: 153.5498571	total: 186ms	remaining: 20.5s
#&gt; 9:	learn: 153.0162352	total: 200ms	remaining: 19.8s
#&gt; 10:	learn: 153.0162090	total: 203ms	remaining: 18.2s
#&gt; 11:	learn: 152.8717763	total: 208ms	remaining: 17.1s
#&gt; 12:	learn: 152.8530544	total: 211ms	remaining: 16s
#&gt; 13:	learn: 152.6793282	total: 216ms	remaining: 15.2s
#&gt; 14:	learn: 152.6679738	total: 219ms	remaining: 14.4s
#&gt; 15:	learn: 152.5792489	total: 224ms	remaining: 13.8s
#&gt; 16:	learn: 151.8581418	total: 235ms	remaining: 13.6s
#&gt; 17:	learn: 151.8451352	total: 256ms	remaining: 14s
#&gt; 18:	learn: 151.8450738	total: 256ms	remaining: 13.2s
#&gt; 19:	learn: 151.5365933	total: 275ms	remaining: 13.5s
#&gt; 20:	learn: 151.3424470	total: 278ms	remaining: 13s
#&gt; 21:	learn: 151.3424103	total: 279ms	remaining: 12.4s
#&gt; 22:	learn: 151.3423805	total: 279ms	remaining: 11.8s
#&gt; 23:	learn: 151.3423562	total: 279ms	remaining: 11.3s
#&gt; 24:	learn: 151.3423366	total: 279ms	remaining: 10.9s
#&gt; 25:	learn: 151.3423206	total: 279ms	remaining: 10.5s
#&gt; 26:	learn: 151.3423077	total: 280ms	remaining: 10.1s
#&gt; 27:	learn: 151.3422971	total: 280ms	remaining: 9.71s
#&gt; 28:	learn: 151.0669395	total: 280ms	remaining: 9.39s
#&gt; 29:	learn: 151.0669380	total: 283ms	remaining: 9.14s
#&gt; 30:	learn: 151.0669367	total: 283ms	remaining: 8.85s
#&gt; 31:	learn: 151.0428450	total: 287ms	remaining: 8.67s
#&gt; 32:	learn: 150.7996961	total: 290ms	remaining: 8.48s
#&gt; 33:	learn: 150.7996922	total: 290ms	remaining: 8.23s
#&gt; 34:	learn: 150.5939492	total: 296ms	remaining: 8.17s
#&gt; 35:	learn: 150.5811829	total: 299ms	remaining: 8s
#&gt; 36:	learn: 150.5811803	total: 299ms	remaining: 7.78s
#&gt; 37:	learn: 150.5704244	total: 317ms	remaining: 8.03s
#&gt; 38:	learn: 150.5704221	total: 317ms	remaining: 7.82s
#&gt; 39:	learn: 149.8287940	total: 321ms	remaining: 7.7s
#&gt; 40:	learn: 149.8287706	total: 321ms	remaining: 7.51s
#&gt; 41:	learn: 149.8287516	total: 321ms	remaining: 7.33s
#&gt; 42:	learn: 149.8194916	total: 349ms	remaining: 7.76s
#&gt; 43:	learn: 149.8158446	total: 352ms	remaining: 7.64s
#&gt; 44:	learn: 149.8134243	total: 354ms	remaining: 7.51s
#&gt; 45:	learn: 149.8134199	total: 354ms	remaining: 7.34s
#&gt; 46:	learn: 149.8109909	total: 356ms	remaining: 7.23s
#&gt; 47:	learn: 149.8093197	total: 359ms	remaining: 7.12s
#&gt; 48:	learn: 149.8093174	total: 359ms	remaining: 6.97s
#&gt; 49:	learn: 149.2033289	total: 365ms	remaining: 6.94s
#&gt; 50:	learn: 149.2004208	total: 370ms	remaining: 6.88s
#&gt; 51:	learn: 149.0931837	total: 376ms	remaining: 6.86s
#&gt; 52:	learn: 148.9415485	total: 379ms	remaining: 6.76s
#&gt; 53:	learn: 148.8768985	total: 385ms	remaining: 6.74s
#&gt; 54:	learn: 148.8493851	total: 387ms	remaining: 6.65s
#&gt; 55:	learn: 148.8084014	total: 389ms	remaining: 6.56s
#&gt; 56:	learn: 148.7714608	total: 390ms	remaining: 6.45s
#&gt; 57:	learn: 148.7714244	total: 390ms	remaining: 6.33s
#&gt; 58:	learn: 148.7381157	total: 390ms	remaining: 6.22s
#&gt; 59:	learn: 148.6859657	total: 392ms	remaining: 6.15s
#&gt; 60:	learn: 148.6859261	total: 393ms	remaining: 6.04s
#&gt; 61:	learn: 148.5192069	total: 395ms	remaining: 5.98s
#&gt; 62:	learn: 148.5191933	total: 395ms	remaining: 5.88s
#&gt; 63:	learn: 148.4910803	total: 398ms	remaining: 5.82s
#&gt; 64:	learn: 148.4871672	total: 400ms	remaining: 5.76s
#&gt; 65:	learn: 148.4871561	total: 400ms	remaining: 5.67s
#&gt; 66:	learn: 148.0433200	total: 405ms	remaining: 5.64s
#&gt; 67:	learn: 148.0432848	total: 406ms	remaining: 5.56s
#&gt; 68:	learn: 147.9625398	total: 420ms	remaining: 5.67s
#&gt; 69:	learn: 147.9011974	total: 440ms	remaining: 5.85s
#&gt; 70:	learn: 147.8730265	total: 458ms	remaining: 6s
#&gt; 71:	learn: 147.8729573	total: 459ms	remaining: 5.91s
#&gt; 72:	learn: 147.8729012	total: 459ms	remaining: 5.83s
#&gt; 73:	learn: 147.8728556	total: 459ms	remaining: 5.75s
#&gt; 74:	learn: 147.8728185	total: 459ms	remaining: 5.66s
#&gt; 75:	learn: 147.8727820	total: 463ms	remaining: 5.63s
#&gt; 76:	learn: 147.8727576	total: 463ms	remaining: 5.55s
#&gt; 77:	learn: 147.8727378	total: 463ms	remaining: 5.48s
#&gt; 78:	learn: 147.8727217	total: 464ms	remaining: 5.41s
#&gt; 79:	learn: 147.4397422	total: 467ms	remaining: 5.37s
#&gt; 80:	learn: 147.4397078	total: 468ms	remaining: 5.3s
#&gt; 81:	learn: 147.4396799	total: 468ms	remaining: 5.24s
#&gt; 82:	learn: 147.1649057	total: 468ms	remaining: 5.17s
#&gt; 83:	learn: 147.1011192	total: 473ms	remaining: 5.16s
#&gt; 84:	learn: 146.9711361	total: 482ms	remaining: 5.19s
#&gt; 85:	learn: 146.9707801	total: 516ms	remaining: 5.49s
#&gt; 86:	learn: 146.7704181	total: 525ms	remaining: 5.51s
#&gt; 87:	learn: 146.7703914	total: 525ms	remaining: 5.44s
#&gt; 88:	learn: 146.7703697	total: 526ms	remaining: 5.38s
#&gt; 89:	learn: 146.3610515	total: 539ms	remaining: 5.45s
#&gt; 90:	learn: 146.3610314	total: 539ms	remaining: 5.39s
#&gt; 91:	learn: 146.2935138	total: 548ms	remaining: 5.4s
#&gt; 92:	learn: 146.2934815	total: 548ms	remaining: 5.34s
#&gt; 93:	learn: 146.2934553	total: 548ms	remaining: 5.28s
#&gt; 94:	learn: 145.5572115	total: 557ms	remaining: 5.3s
#&gt; 95:	learn: 145.5572115	total: 557ms	remaining: 5.24s
#&gt; 96:	learn: 145.5572114	total: 557ms	remaining: 5.19s
#&gt; 97:	learn: 145.5527323	total: 560ms	remaining: 5.15s
#&gt; 98:	learn: 145.5397486	total: 565ms	remaining: 5.14s
#&gt; 99:	learn: 145.5397480	total: 565ms	remaining: 5.08s
#&gt; 100:	learn: 145.5397476	total: 565ms	remaining: 5.03s
#&gt; 101:	learn: 145.5387360	total: 568ms	remaining: 5s
#&gt; 102:	learn: 145.2037369	total: 573ms	remaining: 4.99s
#&gt; 103:	learn: 144.3006331	total: 579ms	remaining: 4.99s
#&gt; 104:	learn: 144.0273450	total: 588ms	remaining: 5.01s
#&gt; 105:	learn: 144.0260418	total: 590ms	remaining: 4.98s
#&gt; 106:	learn: 144.0260346	total: 590ms	remaining: 4.93s
#&gt; 107:	learn: 144.0260287	total: 591ms	remaining: 4.88s
#&gt; 108:	learn: 144.0260239	total: 591ms	remaining: 4.83s
#&gt; 109:	learn: 144.0260200	total: 591ms	remaining: 4.78s
#&gt; 110:	learn: 143.6537228	total: 598ms	remaining: 4.79s
#&gt; 111:	learn: 143.6537221	total: 598ms	remaining: 4.74s
#&gt; 112:	learn: 143.6419717	total: 601ms	remaining: 4.72s
#&gt; 113:	learn: 143.6419716	total: 602ms	remaining: 4.67s
#&gt; 114:	learn: 143.3480063	total: 602ms	remaining: 4.63s
#&gt; 115:	learn: 143.3479863	total: 602ms	remaining: 4.59s
#&gt; 116:	learn: 143.3479694	total: 605ms	remaining: 4.57s
#&gt; 117:	learn: 143.3479561	total: 606ms	remaining: 4.53s
#&gt; 118:	learn: 143.3293666	total: 613ms	remaining: 4.54s
#&gt; 119:	learn: 143.1602980	total: 622ms	remaining: 4.56s
#&gt; 120:	learn: 143.1602976	total: 622ms	remaining: 4.52s
#&gt; 121:	learn: 143.1106499	total: 625ms	remaining: 4.5s
#&gt; 122:	learn: 143.1079845	total: 631ms	remaining: 4.5s
#&gt; 123:	learn: 142.9071755	total: 632ms	remaining: 4.47s
#&gt; 124:	learn: 142.9071752	total: 633ms	remaining: 4.43s
#&gt; 125:	learn: 142.8612470	total: 636ms	remaining: 4.41s
#&gt; 126:	learn: 142.8612470	total: 636ms	remaining: 4.37s
#&gt; 127:	learn: 142.8612470	total: 636ms	remaining: 4.33s
#&gt; 128:	learn: 142.8584188	total: 639ms	remaining: 4.31s
#&gt; 129:	learn: 142.8584188	total: 639ms	remaining: 4.27s
#&gt; 130:	learn: 142.8584187	total: 639ms	remaining: 4.24s
#&gt; 131:	learn: 142.8153245	total: 652ms	remaining: 4.29s
#&gt; 132:	learn: 142.4639139	total: 663ms	remaining: 4.32s
#&gt; 133:	learn: 142.4638975	total: 664ms	remaining: 4.29s
#&gt; 134:	learn: 142.4632575	total: 667ms	remaining: 4.27s
#&gt; 135:	learn: 142.4235273	total: 672ms	remaining: 4.27s
#&gt; 136:	learn: 142.4235124	total: 672ms	remaining: 4.23s
#&gt; 137:	learn: 142.4235003	total: 672ms	remaining: 4.2s
#&gt; 138:	learn: 141.9126283	total: 678ms	remaining: 4.2s
#&gt; 139:	learn: 141.8705162	total: 686ms	remaining: 4.22s
#&gt; 140:	learn: 141.7593852	total: 698ms	remaining: 4.25s
#&gt; 141:	learn: 141.2212302	total: 708ms	remaining: 4.28s
#&gt; 142:	learn: 141.2186958	total: 710ms	remaining: 4.26s
#&gt; 143:	learn: 141.2186872	total: 711ms	remaining: 4.22s
#&gt; 144:	learn: 141.2186801	total: 711ms	remaining: 4.19s
#&gt; 145:	learn: 140.4923956	total: 726ms	remaining: 4.24s
#&gt; 146:	learn: 140.4923818	total: 726ms	remaining: 4.21s
#&gt; 147:	learn: 140.4564217	total: 731ms	remaining: 4.21s
#&gt; 148:	learn: 140.2312133	total: 739ms	remaining: 4.22s
#&gt; 149:	learn: 140.2067307	total: 745ms	remaining: 4.22s
#&gt; 150:	learn: 139.3961807	total: 756ms	remaining: 4.25s
#&gt; 151:	learn: 139.0343660	total: 765ms	remaining: 4.27s
#&gt; 152:	learn: 139.0265469	total: 767ms	remaining: 4.25s
#&gt; 153:	learn: 139.0265469	total: 768ms	remaining: 4.22s
#&gt; 154:	learn: 139.0265468	total: 768ms	remaining: 4.19s
#&gt; 155:	learn: 139.0265468	total: 768ms	remaining: 4.16s
#&gt; 156:	learn: 139.0265468	total: 768ms	remaining: 4.13s
#&gt; 157:	learn: 138.7343172	total: 777ms	remaining: 4.14s
#&gt; 158:	learn: 138.7342958	total: 777ms	remaining: 4.11s
#&gt; 159:	learn: 138.7342785	total: 777ms	remaining: 4.08s
#&gt; 160:	learn: 138.7342644	total: 778ms	remaining: 4.05s
#&gt; 161:	learn: 138.7342530	total: 778ms	remaining: 4.02s
#&gt; 162:	learn: 138.7342437	total: 778ms	remaining: 4s
#&gt; 163:	learn: 138.1807388	total: 812ms	remaining: 4.14s
#&gt; 164:	learn: 138.0944277	total: 819ms	remaining: 4.14s
#&gt; 165:	learn: 138.0335032	total: 827ms	remaining: 4.15s
#&gt; 166:	learn: 137.9479917	total: 830ms	remaining: 4.14s
#&gt; 167:	learn: 137.7307406	total: 831ms	remaining: 4.12s
#&gt; 168:	learn: 137.7307239	total: 831ms	remaining: 4.09s
#&gt; 169:	learn: 137.7166198	total: 833ms	remaining: 4.07s
#&gt; 170:	learn: 137.0004346	total: 841ms	remaining: 4.07s
#&gt; 171:	learn: 137.0004185	total: 841ms	remaining: 4.05s
#&gt; 172:	learn: 136.9116028	total: 848ms	remaining: 4.05s
#&gt; 173:	learn: 136.9110256	total: 850ms	remaining: 4.03s
#&gt; 174:	learn: 136.9110159	total: 850ms	remaining: 4.01s
#&gt; 175:	learn: 136.8554304	total: 854ms	remaining: 4s
#&gt; 176:	learn: 136.7227018	total: 858ms	remaining: 3.99s
#&gt; 177:	learn: 136.7174484	total: 860ms	remaining: 3.97s
#&gt; 178:	learn: 136.6837266	total: 861ms	remaining: 3.95s
#&gt; 179:	learn: 136.6749016	total: 864ms	remaining: 3.94s
#&gt; 180:	learn: 136.6748946	total: 865ms	remaining: 3.91s
#&gt; 181:	learn: 136.6748890	total: 865ms	remaining: 3.89s
#&gt; 182:	learn: 136.6740036	total: 867ms	remaining: 3.87s
#&gt; 183:	learn: 136.5336494	total: 871ms	remaining: 3.86s
#&gt; 184:	learn: 136.5336467	total: 871ms	remaining: 3.84s
#&gt; 185:	learn: 136.5336446	total: 871ms	remaining: 3.81s
#&gt; 186:	learn: 136.5336428	total: 871ms	remaining: 3.79s
#&gt; 187:	learn: 136.5336414	total: 872ms	remaining: 3.76s
#&gt; 188:	learn: 136.5336402	total: 872ms	remaining: 3.74s
#&gt; 189:	learn: 136.5336393	total: 872ms	remaining: 3.72s
#&gt; 190:	learn: 136.3464760	total: 878ms	remaining: 3.72s
#&gt; 191:	learn: 136.3180668	total: 879ms	remaining: 3.7s
#&gt; 192:	learn: 136.3180658	total: 879ms	remaining: 3.67s
#&gt; 193:	learn: 136.0433527	total: 883ms	remaining: 3.67s
#&gt; 194:	learn: 136.0412647	total: 885ms	remaining: 3.65s
#&gt; 195:	learn: 136.0412444	total: 886ms	remaining: 3.63s
#&gt; 196:	learn: 135.6976628	total: 890ms	remaining: 3.63s
#&gt; 197:	learn: 135.6976296	total: 890ms	remaining: 3.6s
#&gt; 198:	learn: 135.5405509	total: 894ms	remaining: 3.6s
#&gt; 199:	learn: 135.5405193	total: 894ms	remaining: 3.58s
#&gt; 200:	learn: 135.4427270	total: 903ms	remaining: 3.59s
#&gt; 201:	learn: 135.4288179	total: 905ms	remaining: 3.58s
#&gt; 202:	learn: 135.4279174	total: 907ms	remaining: 3.56s
#&gt; 203:	learn: 135.4074894	total: 907ms	remaining: 3.54s
#&gt; 204:	learn: 135.3095079	total: 959ms	remaining: 3.72s
#&gt; 205:	learn: 135.3095052	total: 961ms	remaining: 3.7s
#&gt; 206:	learn: 135.1591160	total: 967ms	remaining: 3.7s
#&gt; 207:	learn: 134.8311110	total: 985ms	remaining: 3.75s
#&gt; 208:	learn: 134.8311038	total: 986ms	remaining: 3.73s
#&gt; 209:	learn: 134.5275896	total: 993ms	remaining: 3.74s
#&gt; 210:	learn: 134.5275742	total: 994ms	remaining: 3.72s
#&gt; 211:	learn: 134.3638018	total: 1.01s	remaining: 3.74s
#&gt; 212:	learn: 134.3637842	total: 1.01s	remaining: 3.72s
#&gt; 213:	learn: 134.3634014	total: 1.01s	remaining: 3.72s
#&gt; 214:	learn: 134.1921740	total: 1.02s	remaining: 3.71s
#&gt; 215:	learn: 134.1835046	total: 1.02s	remaining: 3.72s
#&gt; 216:	learn: 134.1831956	total: 1.03s	remaining: 3.73s
#&gt; 217:	learn: 133.5950984	total: 1.05s	remaining: 3.76s
#&gt; 218:	learn: 133.5948601	total: 1.05s	remaining: 3.76s
#&gt; 219:	learn: 133.1206766	total: 1.06s	remaining: 3.76s
#&gt; 220:	learn: 133.1206735	total: 1.06s	remaining: 3.74s
#&gt; 221:	learn: 133.1206710	total: 1.06s	remaining: 3.72s
#&gt; 222:	learn: 133.1206690	total: 1.06s	remaining: 3.69s
#&gt; 223:	learn: 133.1205919	total: 1.06s	remaining: 3.68s
#&gt; 224:	learn: 133.1205907	total: 1.06s	remaining: 3.66s
#&gt; 225:	learn: 132.6177633	total: 1.07s	remaining: 3.66s
#&gt; 226:	learn: 132.6177632	total: 1.07s	remaining: 3.64s
#&gt; 227:	learn: 132.6072096	total: 1.07s	remaining: 3.62s
#&gt; 228:	learn: 132.6072088	total: 1.07s	remaining: 3.6s
#&gt; 229:	learn: 132.5978444	total: 1.07s	remaining: 3.58s
#&gt; 230:	learn: 132.5978425	total: 1.07s	remaining: 3.56s
#&gt; 231:	learn: 132.5977860	total: 1.07s	remaining: 3.55s
#&gt; 232:	learn: 132.5671396	total: 1.07s	remaining: 3.53s
#&gt; 233:	learn: 132.5671380	total: 1.07s	remaining: 3.52s
#&gt; 234:	learn: 132.5416115	total: 1.07s	remaining: 3.5s
#&gt; 235:	learn: 132.5416111	total: 1.07s	remaining: 3.48s
#&gt; 236:	learn: 132.5416107	total: 1.07s	remaining: 3.46s
#&gt; 237:	learn: 132.5226141	total: 1.08s	remaining: 3.45s
#&gt; 238:	learn: 132.2248750	total: 1.08s	remaining: 3.45s
#&gt; 239:	learn: 132.2162382	total: 1.08s	remaining: 3.44s
#&gt; 240:	learn: 132.1978745	total: 1.09s	remaining: 3.42s
#&gt; 241:	learn: 132.1816739	total: 1.09s	remaining: 3.41s
#&gt; 242:	learn: 132.1816686	total: 1.09s	remaining: 3.39s
#&gt; 243:	learn: 132.1700611	total: 1.09s	remaining: 3.38s
#&gt; 244:	learn: 132.1700598	total: 1.09s	remaining: 3.36s
#&gt; 245:	learn: 132.1700588	total: 1.09s	remaining: 3.34s
#&gt; 246:	learn: 132.1700580	total: 1.09s	remaining: 3.33s
#&gt; 247:	learn: 132.1700573	total: 1.09s	remaining: 3.31s
#&gt; 248:	learn: 132.1700567	total: 1.09s	remaining: 3.29s
#&gt; 249:	learn: 132.1700563	total: 1.09s	remaining: 3.27s
#&gt; 250:	learn: 132.1700559	total: 1.09s	remaining: 3.26s
#&gt; 251:	learn: 131.7718257	total: 1.11s	remaining: 3.31s
#&gt; 252:	learn: 131.7718254	total: 1.11s	remaining: 3.29s
#&gt; 253:	learn: 131.7718252	total: 1.11s	remaining: 3.27s
#&gt; 254:	learn: 131.7718250	total: 1.11s	remaining: 3.26s
#&gt; 255:	learn: 131.4359062	total: 1.12s	remaining: 3.25s
#&gt; 256:	learn: 131.3124682	total: 1.12s	remaining: 3.24s
#&gt; 257:	learn: 131.3124614	total: 1.12s	remaining: 3.23s
#&gt; 258:	learn: 131.3116444	total: 1.12s	remaining: 3.21s
#&gt; 259:	learn: 131.3116395	total: 1.12s	remaining: 3.2s
#&gt; 260:	learn: 131.3116355	total: 1.12s	remaining: 3.18s
#&gt; 261:	learn: 131.3033846	total: 1.13s	remaining: 3.17s
#&gt; 262:	learn: 131.0838024	total: 1.13s	remaining: 3.16s
#&gt; 263:	learn: 131.0126680	total: 1.13s	remaining: 3.16s
#&gt; 264:	learn: 131.0126495	total: 1.13s	remaining: 3.14s
#&gt; 265:	learn: 130.6333890	total: 1.14s	remaining: 3.14s
#&gt; 266:	learn: 130.5420397	total: 1.14s	remaining: 3.13s
#&gt; 267:	learn: 130.5420379	total: 1.14s	remaining: 3.11s
#&gt; 268:	learn: 130.5420365	total: 1.14s	remaining: 3.1s
#&gt; 269:	learn: 130.5420354	total: 1.14s	remaining: 3.08s
#&gt; 270:	learn: 130.3889126	total: 1.14s	remaining: 3.08s
#&gt; 271:	learn: 130.0883115	total: 1.15s	remaining: 3.08s
#&gt; 272:	learn: 129.9537190	total: 1.16s	remaining: 3.08s
#&gt; 273:	learn: 129.8508019	total: 1.16s	remaining: 3.08s
#&gt; 274:	learn: 129.8500807	total: 1.16s	remaining: 3.07s
#&gt; 275:	learn: 129.8372198	total: 1.17s	remaining: 3.06s
#&gt; 276:	learn: 129.5727419	total: 1.17s	remaining: 3.06s
#&gt; 277:	learn: 129.5603408	total: 1.17s	remaining: 3.04s
#&gt; 278:	learn: 129.3885098	total: 1.18s	remaining: 3.05s
#&gt; 279:	learn: 129.3819141	total: 1.18s	remaining: 3.04s
#&gt; 280:	learn: 128.9418821	total: 1.19s	remaining: 3.04s
#&gt; 281:	learn: 128.9418677	total: 1.19s	remaining: 3.03s
#&gt; 282:	learn: 128.9418559	total: 1.19s	remaining: 3.01s
#&gt; 283:	learn: 128.8341531	total: 1.2s	remaining: 3.03s
#&gt; 284:	learn: 128.8341525	total: 1.2s	remaining: 3.01s
#&gt; 285:	learn: 128.3866721	total: 1.21s	remaining: 3.01s
#&gt; 286:	learn: 128.3866621	total: 1.21s	remaining: 3s
#&gt; 287:	learn: 128.3865984	total: 1.22s	remaining: 3.01s
#&gt; 288:	learn: 128.3865918	total: 1.22s	remaining: 2.99s
#&gt; 289:	learn: 128.3865864	total: 1.22s	remaining: 2.98s
#&gt; 290:	learn: 128.3865820	total: 1.22s	remaining: 2.97s
#&gt; 291:	learn: 128.3738770	total: 1.22s	remaining: 2.96s
#&gt; 292:	learn: 128.3738749	total: 1.22s	remaining: 2.94s
#&gt; 293:	learn: 128.3738732	total: 1.22s	remaining: 2.93s
#&gt; 294:	learn: 128.2676368	total: 1.22s	remaining: 2.92s
#&gt; 295:	learn: 128.1351421	total: 1.24s	remaining: 2.94s
#&gt; 296:	learn: 128.0496342	total: 1.24s	remaining: 2.93s
#&gt; 297:	learn: 128.0374828	total: 1.24s	remaining: 2.93s
#&gt; 298:	learn: 128.0254129	total: 1.24s	remaining: 2.91s
#&gt; 299:	learn: 128.0123953	total: 1.24s	remaining: 2.9s
#&gt; 300:	learn: 128.0017064	total: 1.25s	remaining: 2.9s
#&gt; 301:	learn: 127.9939971	total: 1.25s	remaining: 2.88s
#&gt; 302:	learn: 127.9928815	total: 1.25s	remaining: 2.87s
#&gt; 303:	learn: 127.9928800	total: 1.25s	remaining: 2.86s
#&gt; 304:	learn: 127.9928788	total: 1.25s	remaining: 2.85s
#&gt; 305:	learn: 127.9928778	total: 1.25s	remaining: 2.83s
#&gt; 306:	learn: 127.9789402	total: 1.25s	remaining: 2.82s
#&gt; 307:	learn: 127.7211602	total: 1.26s	remaining: 2.83s
#&gt; 308:	learn: 127.7209467	total: 1.26s	remaining: 2.82s
#&gt; 309:	learn: 127.5727509	total: 1.27s	remaining: 2.82s
#&gt; 310:	learn: 127.5727452	total: 1.27s	remaining: 2.81s
#&gt; 311:	learn: 127.5727406	total: 1.27s	remaining: 2.8s
#&gt; 312:	learn: 127.5725492	total: 1.28s	remaining: 2.8s
#&gt; 313:	learn: 127.5725465	total: 1.28s	remaining: 2.79s
#&gt; 314:	learn: 127.5725443	total: 1.28s	remaining: 2.78s
#&gt; 315:	learn: 127.5725426	total: 1.28s	remaining: 2.77s
#&gt; 316:	learn: 127.5673633	total: 1.28s	remaining: 2.76s
#&gt; 317:	learn: 127.5629211	total: 1.28s	remaining: 2.75s
#&gt; 318:	learn: 127.5629210	total: 1.28s	remaining: 2.74s
#&gt; 319:	learn: 127.4665990	total: 1.29s	remaining: 2.74s
#&gt; 320:	learn: 127.4665977	total: 1.29s	remaining: 2.73s
#&gt; 321:	learn: 127.4580195	total: 1.3s	remaining: 2.74s
#&gt; 322:	learn: 127.3028161	total: 1.31s	remaining: 2.76s
#&gt; 323:	learn: 127.3028152	total: 1.32s	remaining: 2.75s
#&gt; 324:	learn: 127.1516002	total: 1.32s	remaining: 2.74s
#&gt; 325:	learn: 127.1401358	total: 1.32s	remaining: 2.73s
#&gt; 326:	learn: 126.9407331	total: 1.33s	remaining: 2.73s
#&gt; 327:	learn: 126.9407329	total: 1.33s	remaining: 2.72s
#&gt; 328:	learn: 126.9407327	total: 1.33s	remaining: 2.71s
#&gt; 329:	learn: 126.9407325	total: 1.33s	remaining: 2.7s
#&gt; 330:	learn: 126.9407324	total: 1.33s	remaining: 2.68s
#&gt; 331:	learn: 126.9407323	total: 1.33s	remaining: 2.67s
#&gt; 332:	learn: 126.9368014	total: 1.33s	remaining: 2.66s
#&gt; 333:	learn: 126.9147522	total: 1.33s	remaining: 2.66s
#&gt; 334:	learn: 126.9000054	total: 1.33s	remaining: 2.65s
#&gt; 335:	learn: 126.8494971	total: 1.34s	remaining: 2.64s
#&gt; 336:	learn: 126.8284918	total: 1.34s	remaining: 2.63s
#&gt; 337:	learn: 126.8284917	total: 1.34s	remaining: 2.62s
#&gt; 338:	learn: 126.8284916	total: 1.34s	remaining: 2.61s
#&gt; 339:	learn: 126.7534737	total: 1.35s	remaining: 2.61s
#&gt; 340:	learn: 126.7534721	total: 1.35s	remaining: 2.6s
#&gt; 341:	learn: 126.7448402	total: 1.35s	remaining: 2.59s
#&gt; 342:	learn: 126.7448400	total: 1.35s	remaining: 2.58s
#&gt; 343:	learn: 126.7448398	total: 1.35s	remaining: 2.57s
#&gt; 344:	learn: 126.5325039	total: 1.35s	remaining: 2.57s
#&gt; 345:	learn: 126.0968229	total: 1.36s	remaining: 2.57s
#&gt; 346:	learn: 125.9494326	total: 1.37s	remaining: 2.57s
#&gt; 347:	learn: 125.9493654	total: 1.37s	remaining: 2.56s
#&gt; 348:	learn: 125.9493109	total: 1.37s	remaining: 2.55s
#&gt; 349:	learn: 125.7032274	total: 1.38s	remaining: 2.56s
#&gt; 350:	learn: 125.6917970	total: 1.38s	remaining: 2.55s
#&gt; 351:	learn: 125.6916993	total: 1.38s	remaining: 2.54s
#&gt; 352:	learn: 125.6916763	total: 1.38s	remaining: 2.53s
#&gt; 353:	learn: 125.6916575	total: 1.38s	remaining: 2.52s
#&gt; 354:	learn: 125.6916424	total: 1.38s	remaining: 2.51s
#&gt; 355:	learn: 125.6779235	total: 1.39s	remaining: 2.51s
#&gt; 356:	learn: 125.5306821	total: 1.4s	remaining: 2.52s
#&gt; 357:	learn: 125.5192204	total: 1.4s	remaining: 2.51s
#&gt; 358:	learn: 125.5151795	total: 1.41s	remaining: 2.52s
#&gt; 359:	learn: 125.3958143	total: 1.42s	remaining: 2.52s
#&gt; 360:	learn: 125.2836464	total: 1.43s	remaining: 2.53s
#&gt; 361:	learn: 125.2836458	total: 1.43s	remaining: 2.52s
#&gt; 362:	learn: 125.2836453	total: 1.43s	remaining: 2.51s
#&gt; 363:	learn: 125.2836448	total: 1.43s	remaining: 2.5s
#&gt; 364:	learn: 125.2656556	total: 1.43s	remaining: 2.49s
#&gt; 365:	learn: 125.0306990	total: 1.44s	remaining: 2.49s
#&gt; 366:	learn: 125.0306989	total: 1.44s	remaining: 2.48s
#&gt; 367:	learn: 124.7798972	total: 1.44s	remaining: 2.48s
#&gt; 368:	learn: 124.7798865	total: 1.44s	remaining: 2.47s
#&gt; 369:	learn: 124.7798778	total: 1.44s	remaining: 2.46s
#&gt; 370:	learn: 124.3638832	total: 1.45s	remaining: 2.46s
#&gt; 371:	learn: 124.2833209	total: 1.46s	remaining: 2.46s
#&gt; 372:	learn: 124.2833095	total: 1.46s	remaining: 2.45s
#&gt; 373:	learn: 124.2833003	total: 1.46s	remaining: 2.44s
#&gt; 374:	learn: 124.2832928	total: 1.46s	remaining: 2.43s
#&gt; 375:	learn: 124.2832867	total: 1.46s	remaining: 2.42s
#&gt; 376:	learn: 124.2832817	total: 1.46s	remaining: 2.41s
#&gt; 377:	learn: 123.9722443	total: 1.46s	remaining: 2.41s
#&gt; 378:	learn: 123.9722420	total: 1.46s	remaining: 2.4s
#&gt; 379:	learn: 123.9722401	total: 1.46s	remaining: 2.39s
#&gt; 380:	learn: 123.9722386	total: 1.46s	remaining: 2.38s
#&gt; 381:	learn: 123.7828075	total: 1.47s	remaining: 2.37s
#&gt; 382:	learn: 123.7828064	total: 1.47s	remaining: 2.36s
#&gt; 383:	learn: 123.7313513	total: 1.47s	remaining: 2.36s
#&gt; 384:	learn: 123.4265853	total: 1.47s	remaining: 2.35s
#&gt; 385:	learn: 123.3136265	total: 1.48s	remaining: 2.35s
#&gt; 386:	learn: 123.3085313	total: 1.48s	remaining: 2.34s
#&gt; 387:	learn: 123.3083383	total: 1.48s	remaining: 2.34s
#&gt; 388:	learn: 123.3083253	total: 1.48s	remaining: 2.33s
#&gt; 389:	learn: 123.1817245	total: 1.49s	remaining: 2.33s
#&gt; 390:	learn: 123.1817071	total: 1.49s	remaining: 2.32s
#&gt; 391:	learn: 123.1816931	total: 1.49s	remaining: 2.31s
#&gt; 392:	learn: 123.1816817	total: 1.49s	remaining: 2.3s
#&gt; 393:	learn: 123.1816724	total: 1.49s	remaining: 2.29s
#&gt; 394:	learn: 123.1816649	total: 1.49s	remaining: 2.28s
#&gt; 395:	learn: 123.1452591	total: 1.49s	remaining: 2.28s
#&gt; 396:	learn: 123.1432255	total: 1.5s	remaining: 2.27s
#&gt; 397:	learn: 123.1432142	total: 1.5s	remaining: 2.26s
#&gt; 398:	learn: 123.1344072	total: 1.5s	remaining: 2.25s
#&gt; 399:	learn: 122.9967724	total: 1.5s	remaining: 2.25s
#&gt; 400:	learn: 122.9967631	total: 1.5s	remaining: 2.24s
#&gt; 401:	learn: 122.9210522	total: 1.5s	remaining: 2.23s
#&gt; 402:	learn: 122.9210352	total: 1.5s	remaining: 2.23s
#&gt; 403:	learn: 122.8669877	total: 1.51s	remaining: 2.23s
#&gt; 404:	learn: 122.8669739	total: 1.51s	remaining: 2.22s
#&gt; 405:	learn: 122.8652958	total: 1.51s	remaining: 2.21s
#&gt; 406:	learn: 122.8608580	total: 1.51s	remaining: 2.21s
#&gt; 407:	learn: 122.8608496	total: 1.51s	remaining: 2.2s
#&gt; 408:	learn: 122.8234000	total: 1.52s	remaining: 2.2s
#&gt; 409:	learn: 122.8233831	total: 1.52s	remaining: 2.19s
#&gt; 410:	learn: 122.8233693	total: 1.52s	remaining: 2.18s
#&gt; 411:	learn: 122.8233582	total: 1.52s	remaining: 2.17s
#&gt; 412:	learn: 122.5735792	total: 1.52s	remaining: 2.17s
#&gt; 413:	learn: 122.5735791	total: 1.52s	remaining: 2.16s
#&gt; 414:	learn: 122.5735790	total: 1.52s	remaining: 2.15s
#&gt; 415:	learn: 122.5563282	total: 1.53s	remaining: 2.14s
#&gt; 416:	learn: 122.3517050	total: 1.53s	remaining: 2.14s
#&gt; 417:	learn: 122.3517036	total: 1.53s	remaining: 2.13s
#&gt; 418:	learn: 122.1818751	total: 1.53s	remaining: 2.13s
#&gt; 419:	learn: 122.1818584	total: 1.53s	remaining: 2.12s
#&gt; 420:	learn: 122.1102803	total: 1.54s	remaining: 2.11s
#&gt; 421:	learn: 122.1102754	total: 1.54s	remaining: 2.11s
#&gt; 422:	learn: 122.1027947	total: 1.54s	remaining: 2.1s
#&gt; 423:	learn: 121.9991938	total: 1.54s	remaining: 2.09s
#&gt; 424:	learn: 121.9991930	total: 1.54s	remaining: 2.08s
#&gt; 425:	learn: 121.8581368	total: 1.54s	remaining: 2.08s
#&gt; 426:	learn: 121.7300602	total: 1.55s	remaining: 2.07s
#&gt; 427:	learn: 121.6975085	total: 1.55s	remaining: 2.07s
#&gt; 428:	learn: 121.6974684	total: 1.55s	remaining: 2.07s
#&gt; 429:	learn: 121.6974675	total: 1.56s	remaining: 2.06s
#&gt; 430:	learn: 121.6974614	total: 1.56s	remaining: 2.06s
#&gt; 431:	learn: 121.2870530	total: 1.56s	remaining: 2.05s
#&gt; 432:	learn: 121.2870166	total: 1.56s	remaining: 2.05s
#&gt; 433:	learn: 121.1641547	total: 1.57s	remaining: 2.05s
#&gt; 434:	learn: 121.1641184	total: 1.57s	remaining: 2.04s
#&gt; 435:	learn: 121.1090512	total: 1.57s	remaining: 2.04s
#&gt; 436:	learn: 121.0459073	total: 1.58s	remaining: 2.04s
#&gt; 437:	learn: 121.0396307	total: 1.58s	remaining: 2.03s
#&gt; 438:	learn: 120.9593245	total: 1.59s	remaining: 2.03s
#&gt; 439:	learn: 120.9593206	total: 1.59s	remaining: 2.02s
#&gt; 440:	learn: 120.9593174	total: 1.59s	remaining: 2.01s
#&gt; 441:	learn: 120.8746025	total: 1.6s	remaining: 2.02s
#&gt; 442:	learn: 120.8746025	total: 1.6s	remaining: 2.02s
#&gt; 443:	learn: 120.8647327	total: 1.6s	remaining: 2.01s
#&gt; 444:	learn: 120.8647326	total: 1.6s	remaining: 2s
#&gt; 445:	learn: 120.8097605	total: 1.61s	remaining: 2s
#&gt; 446:	learn: 120.8097572	total: 1.61s	remaining: 1.99s
#&gt; 447:	learn: 120.8097545	total: 1.61s	remaining: 1.98s
#&gt; 448:	learn: 120.5197852	total: 1.61s	remaining: 1.98s
#&gt; 449:	learn: 120.5197850	total: 1.62s	remaining: 1.98s
#&gt; 450:	learn: 120.5139643	total: 1.62s	remaining: 1.97s
#&gt; 451:	learn: 120.2531854	total: 1.63s	remaining: 1.97s
#&gt; 452:	learn: 120.1109802	total: 1.63s	remaining: 1.97s
#&gt; 453:	learn: 120.0981277	total: 1.63s	remaining: 1.96s
#&gt; 454:	learn: 120.0954154	total: 1.63s	remaining: 1.96s
#&gt; 455:	learn: 120.0686387	total: 1.64s	remaining: 1.95s
#&gt; 456:	learn: 120.0686387	total: 1.64s	remaining: 1.94s
#&gt; 457:	learn: 120.0686387	total: 1.64s	remaining: 1.94s
#&gt; 458:	learn: 120.0658551	total: 1.64s	remaining: 1.93s
#&gt; 459:	learn: 120.0658550	total: 1.64s	remaining: 1.92s
#&gt; 460:	learn: 120.0658549	total: 1.64s	remaining: 1.92s
#&gt; 461:	learn: 120.0562592	total: 1.64s	remaining: 1.91s
#&gt; 462:	learn: 120.0562590	total: 1.64s	remaining: 1.9s
#&gt; 463:	learn: 119.8581537	total: 1.65s	remaining: 1.9s
#&gt; 464:	learn: 119.8561543	total: 1.65s	remaining: 1.9s
#&gt; 465:	learn: 119.6968128	total: 1.65s	remaining: 1.9s
#&gt; 466:	learn: 119.6634473	total: 1.65s	remaining: 1.89s
#&gt; 467:	learn: 119.5871192	total: 1.66s	remaining: 1.89s
#&gt; 468:	learn: 119.5871192	total: 1.66s	remaining: 1.88s
#&gt; 469:	learn: 119.5626617	total: 1.66s	remaining: 1.88s
#&gt; 470:	learn: 119.5626612	total: 1.66s	remaining: 1.87s
#&gt; 471:	learn: 119.5626609	total: 1.66s	remaining: 1.86s
#&gt; 472:	learn: 119.4917981	total: 1.66s	remaining: 1.85s
#&gt; 473:	learn: 119.4077519	total: 1.67s	remaining: 1.85s
#&gt; 474:	learn: 119.4062050	total: 1.67s	remaining: 1.84s
#&gt; 475:	learn: 119.4062017	total: 1.67s	remaining: 1.84s
#&gt; 476:	learn: 119.3953884	total: 1.67s	remaining: 1.83s
#&gt; 477:	learn: 119.3897282	total: 1.67s	remaining: 1.82s
#&gt; 478:	learn: 119.3799357	total: 1.68s	remaining: 1.82s
#&gt; 479:	learn: 119.2362018	total: 1.68s	remaining: 1.82s
#&gt; 480:	learn: 119.2361961	total: 1.68s	remaining: 1.81s
#&gt; 481:	learn: 119.2361915	total: 1.68s	remaining: 1.8s
#&gt; 482:	learn: 119.2348623	total: 1.68s	remaining: 1.8s
#&gt; 483:	learn: 119.2309521	total: 1.68s	remaining: 1.79s
#&gt; 484:	learn: 119.2309476	total: 1.68s	remaining: 1.79s
#&gt; 485:	learn: 119.2271890	total: 1.68s	remaining: 1.78s
#&gt; 486:	learn: 119.2162696	total: 1.68s	remaining: 1.77s
#&gt; 487:	learn: 118.9669841	total: 1.69s	remaining: 1.77s
#&gt; 488:	learn: 118.9669732	total: 1.69s	remaining: 1.76s
#&gt; 489:	learn: 118.8178679	total: 1.69s	remaining: 1.76s
#&gt; 490:	learn: 118.8168373	total: 1.69s	remaining: 1.75s
#&gt; 491:	learn: 118.7765191	total: 1.7s	remaining: 1.75s
#&gt; 492:	learn: 118.7451566	total: 1.7s	remaining: 1.75s
#&gt; 493:	learn: 118.7381107	total: 1.7s	remaining: 1.74s
#&gt; 494:	learn: 118.5659075	total: 1.71s	remaining: 1.74s
#&gt; 495:	learn: 118.5658986	total: 1.71s	remaining: 1.74s
#&gt; 496:	learn: 118.5658914	total: 1.71s	remaining: 1.73s
#&gt; 497:	learn: 118.0961651	total: 1.72s	remaining: 1.73s
#&gt; 498:	learn: 118.0961505	total: 1.72s	remaining: 1.72s
#&gt; 499:	learn: 117.8755979	total: 1.72s	remaining: 1.72s
#&gt; 500:	learn: 117.8755965	total: 1.72s	remaining: 1.71s
#&gt; 501:	learn: 117.8666852	total: 1.72s	remaining: 1.71s
#&gt; 502:	learn: 117.4443459	total: 1.73s	remaining: 1.71s
#&gt; 503:	learn: 117.4443458	total: 1.73s	remaining: 1.7s
#&gt; 504:	learn: 117.4443458	total: 1.73s	remaining: 1.7s
#&gt; 505:	learn: 117.4387180	total: 1.73s	remaining: 1.69s
#&gt; 506:	learn: 117.4387177	total: 1.73s	remaining: 1.68s
#&gt; 507:	learn: 117.3852095	total: 1.74s	remaining: 1.68s
#&gt; 508:	learn: 117.3852087	total: 1.74s	remaining: 1.67s
#&gt; 509:	learn: 117.3852081	total: 1.74s	remaining: 1.67s
#&gt; 510:	learn: 117.3852076	total: 1.74s	remaining: 1.66s
#&gt; 511:	learn: 117.3840634	total: 1.74s	remaining: 1.66s
#&gt; 512:	learn: 117.3840629	total: 1.74s	remaining: 1.65s
#&gt; 513:	learn: 117.3840625	total: 1.74s	remaining: 1.65s
#&gt; 514:	learn: 117.3840622	total: 1.74s	remaining: 1.64s
#&gt; 515:	learn: 117.3783007	total: 1.74s	remaining: 1.64s
#&gt; 516:	learn: 117.1785859	total: 1.74s	remaining: 1.63s
#&gt; 517:	learn: 116.8039681	total: 1.75s	remaining: 1.63s
#&gt; 518:	learn: 116.6441797	total: 1.76s	remaining: 1.63s
#&gt; 519:	learn: 116.6441792	total: 1.76s	remaining: 1.63s
#&gt; 520:	learn: 116.6441788	total: 1.76s	remaining: 1.62s
#&gt; 521:	learn: 116.6441785	total: 1.76s	remaining: 1.61s
#&gt; 522:	learn: 116.5355173	total: 1.76s	remaining: 1.61s
#&gt; 523:	learn: 116.3990502	total: 1.77s	remaining: 1.6s
#&gt; 524:	learn: 116.3990402	total: 1.77s	remaining: 1.6s
#&gt; 525:	learn: 116.1029801	total: 1.77s	remaining: 1.6s
#&gt; 526:	learn: 116.1029703	total: 1.77s	remaining: 1.59s
#&gt; 527:	learn: 116.1029624	total: 1.77s	remaining: 1.59s
#&gt; 528:	learn: 115.9671800	total: 1.79s	remaining: 1.59s
#&gt; 529:	learn: 115.8499324	total: 1.79s	remaining: 1.59s
#&gt; 530:	learn: 115.8499322	total: 1.79s	remaining: 1.59s
#&gt; 531:	learn: 115.8499321	total: 1.8s	remaining: 1.58s
#&gt; 532:	learn: 115.8499320	total: 1.8s	remaining: 1.57s
#&gt; 533:	learn: 115.8182684	total: 1.8s	remaining: 1.57s
#&gt; 534:	learn: 115.8087226	total: 1.83s	remaining: 1.59s
#&gt; 535:	learn: 115.8087225	total: 1.83s	remaining: 1.58s
#&gt; 536:	learn: 115.8010670	total: 1.83s	remaining: 1.58s
#&gt; 537:	learn: 115.8010669	total: 1.83s	remaining: 1.57s
#&gt; 538:	learn: 115.7987719	total: 1.83s	remaining: 1.57s
#&gt; 539:	learn: 115.5432951	total: 1.84s	remaining: 1.57s
#&gt; 540:	learn: 115.5432950	total: 1.84s	remaining: 1.56s
#&gt; 541:	learn: 115.5425844	total: 1.84s	remaining: 1.56s
#&gt; 542:	learn: 115.2178515	total: 1.85s	remaining: 1.55s
#&gt; 543:	learn: 115.2075815	total: 1.85s	remaining: 1.55s
#&gt; 544:	learn: 115.2075768	total: 1.85s	remaining: 1.54s
#&gt; 545:	learn: 114.9331802	total: 1.85s	remaining: 1.54s
#&gt; 546:	learn: 114.4087426	total: 1.86s	remaining: 1.54s
#&gt; 547:	learn: 114.4087151	total: 1.86s	remaining: 1.54s
#&gt; 548:	learn: 114.1819816	total: 1.86s	remaining: 1.53s
#&gt; 549:	learn: 114.1684080	total: 1.86s	remaining: 1.53s
#&gt; 550:	learn: 114.1681995	total: 1.87s	remaining: 1.52s
#&gt; 551:	learn: 114.1619957	total: 1.87s	remaining: 1.51s
#&gt; 552:	learn: 114.0755462	total: 1.88s	remaining: 1.52s
#&gt; 553:	learn: 114.0669939	total: 1.9s	remaining: 1.53s
#&gt; 554:	learn: 114.0669939	total: 1.9s	remaining: 1.53s
#&gt; 555:	learn: 114.0669939	total: 1.9s	remaining: 1.52s
#&gt; 556:	learn: 114.0540832	total: 1.91s	remaining: 1.51s
#&gt; 557:	learn: 113.8133071	total: 1.91s	remaining: 1.51s
#&gt; 558:	learn: 113.6316196	total: 1.92s	remaining: 1.51s
#&gt; 559:	learn: 113.6136384	total: 1.92s	remaining: 1.51s
#&gt; 560:	learn: 113.6136383	total: 1.92s	remaining: 1.5s
#&gt; 561:	learn: 113.4036425	total: 1.92s	remaining: 1.5s
#&gt; 562:	learn: 113.4036424	total: 1.92s	remaining: 1.49s
#&gt; 563:	learn: 113.4027974	total: 1.92s	remaining: 1.49s
#&gt; 564:	learn: 113.4027972	total: 1.92s	remaining: 1.48s
#&gt; 565:	learn: 113.4027971	total: 1.92s	remaining: 1.48s
#&gt; 566:	learn: 113.4027971	total: 1.92s	remaining: 1.47s
#&gt; 567:	learn: 113.4027970	total: 1.93s	remaining: 1.47s
#&gt; 568:	learn: 113.3034784	total: 1.93s	remaining: 1.46s
#&gt; 569:	learn: 113.3034771	total: 1.93s	remaining: 1.46s
#&gt; 570:	learn: 113.3034760	total: 1.93s	remaining: 1.45s
#&gt; 571:	learn: 113.3034752	total: 1.93s	remaining: 1.45s
#&gt; 572:	learn: 113.2917094	total: 1.93s	remaining: 1.44s
#&gt; 573:	learn: 113.2880567	total: 1.94s	remaining: 1.44s
#&gt; 574:	learn: 113.2876671	total: 1.94s	remaining: 1.43s
#&gt; 575:	learn: 113.1338365	total: 1.94s	remaining: 1.43s
#&gt; 576:	learn: 113.1338319	total: 1.94s	remaining: 1.42s
#&gt; 577:	learn: 113.1265203	total: 1.95s	remaining: 1.42s
#&gt; 578:	learn: 113.1265181	total: 1.95s	remaining: 1.42s
#&gt; 579:	learn: 113.1265164	total: 1.95s	remaining: 1.41s
#&gt; 580:	learn: 113.1265149	total: 1.95s	remaining: 1.41s
#&gt; 581:	learn: 113.1265137	total: 1.95s	remaining: 1.4s
#&gt; 582:	learn: 113.1265128	total: 1.95s	remaining: 1.4s
#&gt; 583:	learn: 113.1265120	total: 1.95s	remaining: 1.39s
#&gt; 584:	learn: 113.1176960	total: 1.95s	remaining: 1.39s
#&gt; 585:	learn: 112.7685407	total: 1.96s	remaining: 1.38s
#&gt; 586:	learn: 112.7667164	total: 1.96s	remaining: 1.38s
#&gt; 587:	learn: 112.7625937	total: 1.96s	remaining: 1.37s
#&gt; 588:	learn: 112.7625899	total: 1.96s	remaining: 1.36s
#&gt; 589:	learn: 112.5456925	total: 1.96s	remaining: 1.36s
#&gt; 590:	learn: 112.5402952	total: 1.97s	remaining: 1.36s
#&gt; 591:	learn: 112.4457496	total: 1.97s	remaining: 1.36s
#&gt; 592:	learn: 112.4457156	total: 1.97s	remaining: 1.35s
#&gt; 593:	learn: 112.3277942	total: 1.97s	remaining: 1.35s
#&gt; 594:	learn: 112.3201053	total: 1.98s	remaining: 1.35s
#&gt; 595:	learn: 112.3200997	total: 1.98s	remaining: 1.34s
#&gt; 596:	learn: 112.1469322	total: 2.03s	remaining: 1.37s
#&gt; 597:	learn: 112.1469302	total: 2.03s	remaining: 1.36s
#&gt; 598:	learn: 112.1429170	total: 2.03s	remaining: 1.36s
#&gt; 599:	learn: 112.1293138	total: 2.03s	remaining: 1.35s
#&gt; 600:	learn: 112.1293128	total: 2.03s	remaining: 1.35s
#&gt; 601:	learn: 112.1293121	total: 2.03s	remaining: 1.34s
#&gt; 602:	learn: 112.1201287	total: 2.03s	remaining: 1.34s
#&gt; 603:	learn: 112.1193196	total: 2.03s	remaining: 1.33s
#&gt; 604:	learn: 112.1091905	total: 2.03s	remaining: 1.33s
#&gt; 605:	learn: 111.9998504	total: 2.04s	remaining: 1.33s
#&gt; 606:	learn: 111.9994226	total: 2.05s	remaining: 1.32s
#&gt; 607:	learn: 111.9802246	total: 2.05s	remaining: 1.32s
#&gt; 608:	learn: 111.9760042	total: 2.05s	remaining: 1.32s
#&gt; 609:	learn: 111.9760021	total: 2.05s	remaining: 1.31s
#&gt; 610:	learn: 111.7215296	total: 2.06s	remaining: 1.31s
#&gt; 611:	learn: 111.5489504	total: 2.07s	remaining: 1.31s
#&gt; 612:	learn: 111.4052267	total: 2.08s	remaining: 1.31s
#&gt; 613:	learn: 111.4051998	total: 2.08s	remaining: 1.31s
#&gt; 614:	learn: 111.3193977	total: 2.08s	remaining: 1.3s
#&gt; 615:	learn: 111.3193903	total: 2.08s	remaining: 1.3s
#&gt; 616:	learn: 111.3193844	total: 2.08s	remaining: 1.29s
#&gt; 617:	learn: 111.3145599	total: 2.09s	remaining: 1.29s
#&gt; 618:	learn: 111.3145566	total: 2.09s	remaining: 1.28s
#&gt; 619:	learn: 111.3145540	total: 2.09s	remaining: 1.28s
#&gt; 620:	learn: 111.3145518	total: 2.09s	remaining: 1.27s
#&gt; 621:	learn: 111.2762124	total: 2.1s	remaining: 1.27s
#&gt; 622:	learn: 111.2762097	total: 2.1s	remaining: 1.27s
#&gt; 623:	learn: 111.0621669	total: 2.11s	remaining: 1.27s
#&gt; 624:	learn: 111.0621651	total: 2.11s	remaining: 1.26s
#&gt; 625:	learn: 111.0499162	total: 2.11s	remaining: 1.26s
#&gt; 626:	learn: 111.0299710	total: 2.11s	remaining: 1.26s
#&gt; 627:	learn: 111.0299677	total: 2.11s	remaining: 1.25s
#&gt; 628:	learn: 110.7951431	total: 2.12s	remaining: 1.25s
#&gt; 629:	learn: 110.5848893	total: 2.12s	remaining: 1.25s
#&gt; 630:	learn: 110.5786783	total: 2.12s	remaining: 1.24s
#&gt; 631:	learn: 110.5786758	total: 2.12s	remaining: 1.24s
#&gt; 632:	learn: 110.5786738	total: 2.12s	remaining: 1.23s
#&gt; 633:	learn: 110.5786721	total: 2.12s	remaining: 1.23s
#&gt; 634:	learn: 110.5786708	total: 2.12s	remaining: 1.22s
#&gt; 635:	learn: 110.5786697	total: 2.12s	remaining: 1.22s
#&gt; 636:	learn: 110.3918392	total: 2.13s	remaining: 1.22s
#&gt; 637:	learn: 110.3917861	total: 2.14s	remaining: 1.21s
#&gt; 638:	learn: 110.3917805	total: 2.14s	remaining: 1.21s
#&gt; 639:	learn: 110.3793075	total: 2.14s	remaining: 1.2s
#&gt; 640:	learn: 110.3793041	total: 2.14s	remaining: 1.2s
#&gt; 641:	learn: 110.3783096	total: 2.14s	remaining: 1.19s
#&gt; 642:	learn: 110.3783074	total: 2.14s	remaining: 1.19s
#&gt; 643:	learn: 110.3578500	total: 2.15s	remaining: 1.19s
#&gt; 644:	learn: 110.3578482	total: 2.15s	remaining: 1.18s
#&gt; 645:	learn: 110.3513507	total: 2.15s	remaining: 1.18s
#&gt; 646:	learn: 110.3513486	total: 2.15s	remaining: 1.17s
#&gt; 647:	learn: 110.2943810	total: 2.15s	remaining: 1.17s
#&gt; 648:	learn: 110.2943740	total: 2.15s	remaining: 1.16s
#&gt; 649:	learn: 110.2943684	total: 2.15s	remaining: 1.16s
#&gt; 650:	learn: 110.2798981	total: 2.15s	remaining: 1.16s
#&gt; 651:	learn: 110.2771920	total: 2.16s	remaining: 1.15s
#&gt; 652:	learn: 110.2743909	total: 2.16s	remaining: 1.15s
#&gt; 653:	learn: 110.2480787	total: 2.16s	remaining: 1.14s
#&gt; 654:	learn: 110.2480742	total: 2.16s	remaining: 1.14s
#&gt; 655:	learn: 110.2480706	total: 2.16s	remaining: 1.13s
#&gt; 656:	learn: 110.2480167	total: 2.16s	remaining: 1.13s
#&gt; 657:	learn: 110.2480142	total: 2.16s	remaining: 1.12s
#&gt; 658:	learn: 110.2412815	total: 2.17s	remaining: 1.12s
#&gt; 659:	learn: 110.2412798	total: 2.17s	remaining: 1.12s
#&gt; 660:	learn: 110.2412784	total: 2.17s	remaining: 1.11s
#&gt; 661:	learn: 110.2412773	total: 2.17s	remaining: 1.11s
#&gt; 662:	learn: 110.2273307	total: 2.18s	remaining: 1.11s
#&gt; 663:	learn: 110.2273285	total: 2.18s	remaining: 1.1s
#&gt; 664:	learn: 110.1732760	total: 2.18s	remaining: 1.1s
#&gt; 665:	learn: 110.1677913	total: 2.18s	remaining: 1.09s
#&gt; 666:	learn: 109.9856603	total: 2.22s	remaining: 1.11s
#&gt; 667:	learn: 109.9856603	total: 2.22s	remaining: 1.1s
#&gt; 668:	learn: 109.9856602	total: 2.22s	remaining: 1.1s
#&gt; 669:	learn: 109.9856602	total: 2.22s	remaining: 1.09s
#&gt; 670:	learn: 109.9218504	total: 2.22s	remaining: 1.09s
#&gt; 671:	learn: 109.9218482	total: 2.22s	remaining: 1.08s
#&gt; 672:	learn: 109.9218464	total: 2.22s	remaining: 1.08s
#&gt; 673:	learn: 109.9218449	total: 2.22s	remaining: 1.07s
#&gt; 674:	learn: 109.9198335	total: 2.22s	remaining: 1.07s
#&gt; 675:	learn: 109.7726904	total: 2.23s	remaining: 1.07s
#&gt; 676:	learn: 109.7726880	total: 2.23s	remaining: 1.06s
#&gt; 677:	learn: 109.7717878	total: 2.23s	remaining: 1.06s
#&gt; 678:	learn: 109.7717860	total: 2.23s	remaining: 1.05s
#&gt; 679:	learn: 109.7649433	total: 2.23s	remaining: 1.05s
#&gt; 680:	learn: 109.7601592	total: 2.25s	remaining: 1.05s
#&gt; 681:	learn: 109.6494651	total: 2.25s	remaining: 1.05s
#&gt; 682:	learn: 109.6494649	total: 2.25s	remaining: 1.05s
#&gt; 683:	learn: 109.6494648	total: 2.25s	remaining: 1.04s
#&gt; 684:	learn: 109.6494647	total: 2.25s	remaining: 1.04s
#&gt; 685:	learn: 109.5495646	total: 2.29s	remaining: 1.05s
#&gt; 686:	learn: 109.4875348	total: 2.29s	remaining: 1.04s
#&gt; 687:	learn: 109.4875253	total: 2.29s	remaining: 1.04s
#&gt; 688:	learn: 109.4871367	total: 2.29s	remaining: 1.03s
#&gt; 689:	learn: 109.4871301	total: 2.29s	remaining: 1.03s
#&gt; 690:	learn: 109.4871247	total: 2.29s	remaining: 1.02s
#&gt; 691:	learn: 109.4871204	total: 2.29s	remaining: 1.02s
#&gt; 692:	learn: 109.4405313	total: 2.29s	remaining: 1.02s
#&gt; 693:	learn: 109.4348367	total: 2.3s	remaining: 1.01s
#&gt; 694:	learn: 109.3440944	total: 2.3s	remaining: 1.01s
#&gt; 695:	learn: 109.3367663	total: 2.3s	remaining: 1s
#&gt; 696:	learn: 109.3367643	total: 2.3s	remaining: 1s
#&gt; 697:	learn: 109.3367627	total: 2.3s	remaining: 997ms
#&gt; 698:	learn: 109.3227567	total: 2.3s	remaining: 992ms
#&gt; 699:	learn: 109.3113001	total: 2.31s	remaining: 989ms
#&gt; 700:	learn: 109.3056221	total: 2.31s	remaining: 985ms
#&gt; 701:	learn: 109.3031302	total: 2.31s	remaining: 980ms
#&gt; 702:	learn: 109.2951782	total: 2.31s	remaining: 976ms
#&gt; 703:	learn: 109.2951763	total: 2.31s	remaining: 972ms
#&gt; 704:	learn: 109.1983621	total: 2.32s	remaining: 969ms
#&gt; 705:	learn: 109.1983616	total: 2.32s	remaining: 965ms
#&gt; 706:	learn: 109.1923073	total: 2.32s	remaining: 961ms
#&gt; 707:	learn: 108.9178908	total: 2.32s	remaining: 959ms
#&gt; 708:	learn: 108.8529843	total: 2.33s	remaining: 956ms
#&gt; 709:	learn: 108.8529825	total: 2.33s	remaining: 951ms
#&gt; 710:	learn: 108.8529811	total: 2.33s	remaining: 947ms
#&gt; 711:	learn: 108.8514041	total: 2.33s	remaining: 944ms
#&gt; 712:	learn: 108.6659249	total: 2.33s	remaining: 940ms
#&gt; 713:	learn: 108.6618573	total: 2.34s	remaining: 936ms
#&gt; 714:	learn: 108.5541019	total: 2.34s	remaining: 933ms
#&gt; 715:	learn: 108.2932546	total: 2.35s	remaining: 930ms
#&gt; 716:	learn: 108.2388328	total: 2.35s	remaining: 928ms
#&gt; 717:	learn: 108.2388297	total: 2.35s	remaining: 924ms
#&gt; 718:	learn: 108.2388272	total: 2.35s	remaining: 920ms
#&gt; 719:	learn: 108.1453280	total: 2.36s	remaining: 918ms
#&gt; 720:	learn: 108.1453206	total: 2.36s	remaining: 913ms
#&gt; 721:	learn: 108.1400598	total: 2.36s	remaining: 910ms
#&gt; 722:	learn: 108.1400559	total: 2.36s	remaining: 905ms
#&gt; 723:	learn: 108.1400527	total: 2.36s	remaining: 901ms
#&gt; 724:	learn: 108.1400502	total: 2.36s	remaining: 897ms
#&gt; 725:	learn: 108.1400481	total: 2.36s	remaining: 892ms
#&gt; 726:	learn: 108.1383877	total: 2.37s	remaining: 889ms
#&gt; 727:	learn: 108.1383866	total: 2.37s	remaining: 884ms
#&gt; 728:	learn: 108.1241247	total: 2.37s	remaining: 881ms
#&gt; 729:	learn: 107.9965044	total: 2.38s	remaining: 879ms
#&gt; 730:	learn: 107.9964942	total: 2.38s	remaining: 875ms
#&gt; 731:	learn: 107.9442682	total: 2.38s	remaining: 872ms
#&gt; 732:	learn: 107.9442565	total: 2.38s	remaining: 867ms
#&gt; 733:	learn: 107.8826941	total: 2.39s	remaining: 866ms
#&gt; 734:	learn: 107.7845476	total: 2.39s	remaining: 862ms
#&gt; 735:	learn: 107.7842887	total: 2.39s	remaining: 858ms
#&gt; 736:	learn: 107.7842884	total: 2.39s	remaining: 854ms
#&gt; 737:	learn: 107.4485055	total: 2.41s	remaining: 857ms
#&gt; 738:	learn: 107.4453209	total: 2.42s	remaining: 853ms
#&gt; 739:	learn: 107.4453169	total: 2.42s	remaining: 849ms
#&gt; 740:	learn: 107.4453137	total: 2.42s	remaining: 844ms
#&gt; 741:	learn: 107.4453111	total: 2.42s	remaining: 840ms
#&gt; 742:	learn: 107.4453090	total: 2.42s	remaining: 836ms
#&gt; 743:	learn: 107.4453073	total: 2.42s	remaining: 831ms
#&gt; 744:	learn: 107.4334242	total: 2.42s	remaining: 829ms
#&gt; 745:	learn: 107.4334201	total: 2.42s	remaining: 825ms
#&gt; 746:	learn: 107.4275917	total: 2.42s	remaining: 820ms
#&gt; 747:	learn: 107.3902819	total: 2.43s	remaining: 819ms
#&gt; 748:	learn: 107.3902806	total: 2.43s	remaining: 814ms
#&gt; 749:	learn: 107.3897959	total: 2.43s	remaining: 810ms
#&gt; 750:	learn: 107.1436829	total: 2.44s	remaining: 808ms
#&gt; 751:	learn: 107.1431040	total: 2.44s	remaining: 804ms
#&gt; 752:	learn: 107.0459756	total: 2.44s	remaining: 802ms
#&gt; 753:	learn: 106.8040361	total: 2.45s	remaining: 799ms
#&gt; 754:	learn: 106.8039807	total: 2.45s	remaining: 795ms
#&gt; 755:	learn: 106.7740704	total: 2.45s	remaining: 792ms
#&gt; 756:	learn: 106.7691980	total: 2.45s	remaining: 787ms
#&gt; 757:	learn: 106.7691768	total: 2.45s	remaining: 783ms
#&gt; 758:	learn: 106.7686299	total: 2.46s	remaining: 780ms
#&gt; 759:	learn: 106.7686158	total: 2.46s	remaining: 776ms
#&gt; 760:	learn: 106.7686044	total: 2.46s	remaining: 771ms
#&gt; 761:	learn: 106.7685951	total: 2.46s	remaining: 767ms
#&gt; 762:	learn: 106.7672768	total: 2.46s	remaining: 764ms
#&gt; 763:	learn: 106.7672712	total: 2.46s	remaining: 760ms
#&gt; 764:	learn: 106.7672667	total: 2.46s	remaining: 756ms
#&gt; 765:	learn: 106.7668421	total: 2.46s	remaining: 752ms
#&gt; 766:	learn: 106.7236259	total: 2.46s	remaining: 748ms
#&gt; 767:	learn: 106.7236175	total: 2.46s	remaining: 744ms
#&gt; 768:	learn: 106.7236107	total: 2.46s	remaining: 740ms
#&gt; 769:	learn: 106.7236051	total: 2.46s	remaining: 736ms
#&gt; 770:	learn: 106.7209849	total: 2.47s	remaining: 733ms
#&gt; 771:	learn: 106.7181330	total: 2.47s	remaining: 730ms
#&gt; 772:	learn: 106.7125720	total: 2.47s	remaining: 726ms
#&gt; 773:	learn: 106.6961334	total: 2.47s	remaining: 722ms
#&gt; 774:	learn: 106.6941309	total: 2.47s	remaining: 718ms
#&gt; 775:	learn: 106.6851719	total: 2.47s	remaining: 714ms
#&gt; 776:	learn: 106.6851676	total: 2.47s	remaining: 710ms
#&gt; 777:	learn: 106.6844277	total: 2.48s	remaining: 707ms
#&gt; 778:	learn: 106.6844249	total: 2.48s	remaining: 703ms
#&gt; 779:	learn: 106.6209092	total: 2.48s	remaining: 701ms
#&gt; 780:	learn: 106.6209065	total: 2.48s	remaining: 697ms
#&gt; 781:	learn: 106.5941228	total: 2.49s	remaining: 694ms
#&gt; 782:	learn: 106.5012178	total: 2.5s	remaining: 692ms
#&gt; 783:	learn: 106.4809282	total: 2.5s	remaining: 689ms
#&gt; 784:	learn: 106.3771627	total: 2.5s	remaining: 686ms
#&gt; 785:	learn: 106.3769842	total: 2.51s	remaining: 683ms
#&gt; 786:	learn: 106.3732720	total: 2.51s	remaining: 679ms
#&gt; 787:	learn: 106.1944083	total: 2.51s	remaining: 676ms
#&gt; 788:	learn: 106.1944031	total: 2.51s	remaining: 672ms
#&gt; 789:	learn: 106.1885434	total: 2.52s	remaining: 669ms
#&gt; 790:	learn: 106.1804624	total: 2.52s	remaining: 665ms
#&gt; 791:	learn: 105.9933213	total: 2.52s	remaining: 663ms
#&gt; 792:	learn: 105.9933175	total: 2.52s	remaining: 659ms
#&gt; 793:	learn: 105.6800316	total: 2.53s	remaining: 656ms
#&gt; 794:	learn: 105.6800307	total: 2.53s	remaining: 652ms
#&gt; 795:	learn: 105.6766275	total: 2.53s	remaining: 648ms
#&gt; 796:	learn: 105.6766269	total: 2.53s	remaining: 644ms
#&gt; 797:	learn: 105.6766264	total: 2.53s	remaining: 640ms
#&gt; 798:	learn: 105.6766259	total: 2.53s	remaining: 636ms
#&gt; 799:	learn: 105.6765778	total: 2.53s	remaining: 633ms
#&gt; 800:	learn: 105.6765775	total: 2.53s	remaining: 629ms
#&gt; 801:	learn: 105.6765773	total: 2.53s	remaining: 625ms
#&gt; 802:	learn: 105.6765772	total: 2.53s	remaining: 621ms
#&gt; 803:	learn: 105.5826615	total: 2.53s	remaining: 617ms
#&gt; 804:	learn: 105.4922214	total: 2.53s	remaining: 614ms
#&gt; 805:	learn: 105.4921061	total: 2.54s	remaining: 610ms
#&gt; 806:	learn: 105.4920974	total: 2.54s	remaining: 606ms
#&gt; 807:	learn: 105.2715881	total: 2.56s	remaining: 608ms
#&gt; 808:	learn: 105.2704577	total: 2.56s	remaining: 604ms
#&gt; 809:	learn: 105.2693988	total: 2.56s	remaining: 601ms
#&gt; 810:	learn: 105.2693874	total: 2.56s	remaining: 597ms
#&gt; 811:	learn: 105.1442859	total: 2.57s	remaining: 594ms
#&gt; 812:	learn: 105.1442817	total: 2.57s	remaining: 591ms
#&gt; 813:	learn: 105.1407024	total: 2.57s	remaining: 587ms
#&gt; 814:	learn: 104.8194443	total: 2.58s	remaining: 585ms
#&gt; 815:	learn: 104.7483446	total: 2.58s	remaining: 582ms
#&gt; 816:	learn: 104.7368173	total: 2.58s	remaining: 578ms
#&gt; 817:	learn: 104.7253548	total: 2.58s	remaining: 575ms
#&gt; 818:	learn: 104.7106050	total: 2.6s	remaining: 575ms
#&gt; 819:	learn: 104.7065616	total: 2.6s	remaining: 572ms
#&gt; 820:	learn: 104.7065574	total: 2.6s	remaining: 568ms
#&gt; 821:	learn: 104.6553867	total: 2.61s	remaining: 565ms
#&gt; 822:	learn: 104.4991482	total: 2.62s	remaining: 564ms
#&gt; 823:	learn: 104.4991451	total: 2.62s	remaining: 560ms
#&gt; 824:	learn: 104.4939881	total: 2.62s	remaining: 556ms
#&gt; 825:	learn: 104.4814206	total: 2.63s	remaining: 553ms
#&gt; 826:	learn: 104.2342472	total: 2.63s	remaining: 550ms
#&gt; 827:	learn: 104.2342472	total: 2.63s	remaining: 546ms
#&gt; 828:	learn: 104.2342210	total: 2.63s	remaining: 542ms
#&gt; 829:	learn: 104.2342210	total: 2.63s	remaining: 539ms
#&gt; 830:	learn: 104.2342210	total: 2.63s	remaining: 535ms
#&gt; 831:	learn: 104.2342209	total: 2.63s	remaining: 531ms
#&gt; 832:	learn: 104.1557617	total: 2.63s	remaining: 528ms
#&gt; 833:	learn: 104.1525587	total: 2.64s	remaining: 525ms
#&gt; 834:	learn: 104.1495559	total: 2.64s	remaining: 521ms
#&gt; 835:	learn: 104.1490593	total: 2.64s	remaining: 518ms
#&gt; 836:	learn: 104.1405730	total: 2.64s	remaining: 515ms
#&gt; 837:	learn: 104.1405722	total: 2.64s	remaining: 511ms
#&gt; 838:	learn: 104.1358674	total: 2.65s	remaining: 508ms
#&gt; 839:	learn: 104.1358673	total: 2.65s	remaining: 505ms
#&gt; 840:	learn: 104.1317766	total: 2.65s	remaining: 501ms
#&gt; 841:	learn: 104.1317766	total: 2.65s	remaining: 497ms
#&gt; 842:	learn: 104.1317766	total: 2.65s	remaining: 494ms
#&gt; 843:	learn: 103.9387441	total: 2.66s	remaining: 491ms
#&gt; 844:	learn: 103.9387441	total: 2.66s	remaining: 488ms
#&gt; 845:	learn: 103.9387441	total: 2.66s	remaining: 484ms
#&gt; 846:	learn: 103.9358583	total: 2.66s	remaining: 480ms
#&gt; 847:	learn: 103.9358583	total: 2.66s	remaining: 477ms
#&gt; 848:	learn: 103.7816425	total: 2.67s	remaining: 474ms
#&gt; 849:	learn: 103.7816420	total: 2.67s	remaining: 470ms
#&gt; 850:	learn: 103.7556588	total: 2.67s	remaining: 468ms
#&gt; 851:	learn: 103.7436641	total: 2.67s	remaining: 465ms
#&gt; 852:	learn: 103.7402941	total: 2.67s	remaining: 461ms
#&gt; 853:	learn: 103.7402902	total: 2.67s	remaining: 457ms
#&gt; 854:	learn: 103.7402871	total: 2.67s	remaining: 454ms
#&gt; 855:	learn: 103.6494278	total: 2.68s	remaining: 451ms
#&gt; 856:	learn: 103.5833530	total: 2.68s	remaining: 447ms
#&gt; 857:	learn: 103.5719418	total: 2.68s	remaining: 444ms
#&gt; 858:	learn: 103.3631033	total: 2.68s	remaining: 440ms
#&gt; 859:	learn: 103.3630880	total: 2.68s	remaining: 437ms
#&gt; 860:	learn: 103.2596882	total: 2.69s	remaining: 435ms
#&gt; 861:	learn: 103.2415641	total: 2.69s	remaining: 431ms
#&gt; 862:	learn: 103.2415499	total: 2.69s	remaining: 428ms
#&gt; 863:	learn: 103.0048057	total: 2.7s	remaining: 426ms
#&gt; 864:	learn: 102.9982123	total: 2.71s	remaining: 422ms
#&gt; 865:	learn: 102.9981970	total: 2.71s	remaining: 419ms
#&gt; 866:	learn: 102.9981845	total: 2.71s	remaining: 415ms
#&gt; 867:	learn: 102.9981744	total: 2.71s	remaining: 412ms
#&gt; 868:	learn: 102.9981662	total: 2.71s	remaining: 408ms
#&gt; 869:	learn: 102.9849334	total: 2.71s	remaining: 405ms
#&gt; 870:	learn: 102.9849250	total: 2.71s	remaining: 401ms
#&gt; 871:	learn: 102.9849181	total: 2.71s	remaining: 398ms
#&gt; 872:	learn: 102.9849063	total: 2.71s	remaining: 395ms
#&gt; 873:	learn: 102.9849017	total: 2.71s	remaining: 391ms
#&gt; 874:	learn: 102.8995349	total: 2.72s	remaining: 388ms
#&gt; 875:	learn: 102.8995348	total: 2.72s	remaining: 385ms
#&gt; 876:	learn: 102.8951233	total: 2.72s	remaining: 381ms
#&gt; 877:	learn: 102.8893081	total: 2.72s	remaining: 378ms
#&gt; 878:	learn: 102.6586508	total: 2.72s	remaining: 375ms
#&gt; 879:	learn: 102.6572877	total: 2.72s	remaining: 371ms
#&gt; 880:	learn: 102.5402772	total: 2.73s	remaining: 369ms
#&gt; 881:	learn: 102.5402764	total: 2.73s	remaining: 365ms
#&gt; 882:	learn: 102.5402756	total: 2.73s	remaining: 362ms
#&gt; 883:	learn: 102.5217692	total: 2.73s	remaining: 359ms
#&gt; 884:	learn: 102.5174942	total: 2.74s	remaining: 356ms
#&gt; 885:	learn: 102.4768991	total: 2.74s	remaining: 353ms
#&gt; 886:	learn: 102.4719448	total: 2.75s	remaining: 350ms
#&gt; 887:	learn: 102.4719439	total: 2.75s	remaining: 346ms
#&gt; 888:	learn: 102.4719432	total: 2.75s	remaining: 343ms
#&gt; 889:	learn: 102.4719426	total: 2.75s	remaining: 339ms
#&gt; 890:	learn: 102.4719422	total: 2.75s	remaining: 336ms
#&gt; 891:	learn: 102.4719418	total: 2.75s	remaining: 333ms
#&gt; 892:	learn: 102.4719415	total: 2.75s	remaining: 329ms
#&gt; 893:	learn: 102.4712386	total: 2.75s	remaining: 326ms
#&gt; 894:	learn: 102.4680472	total: 2.75s	remaining: 323ms
#&gt; 895:	learn: 102.4588102	total: 2.75s	remaining: 319ms
#&gt; 896:	learn: 102.4588101	total: 2.75s	remaining: 316ms
#&gt; 897:	learn: 102.3936442	total: 2.75s	remaining: 313ms
#&gt; 898:	learn: 102.3936413	total: 2.75s	remaining: 309ms
#&gt; 899:	learn: 102.3936390	total: 2.75s	remaining: 306ms
#&gt; 900:	learn: 102.3936334	total: 2.76s	remaining: 303ms
#&gt; 901:	learn: 102.3814564	total: 2.77s	remaining: 301ms
#&gt; 902:	learn: 102.3814550	total: 2.77s	remaining: 297ms
#&gt; 903:	learn: 102.3814538	total: 2.77s	remaining: 294ms
#&gt; 904:	learn: 102.3792393	total: 2.77s	remaining: 291ms
#&gt; 905:	learn: 102.3785973	total: 2.8s	remaining: 291ms
#&gt; 906:	learn: 102.3785968	total: 2.8s	remaining: 287ms
#&gt; 907:	learn: 102.3785964	total: 2.8s	remaining: 284ms
#&gt; 908:	learn: 102.3785960	total: 2.8s	remaining: 280ms
#&gt; 909:	learn: 102.3785832	total: 2.8s	remaining: 277ms
#&gt; 910:	learn: 102.3785830	total: 2.8s	remaining: 274ms
#&gt; 911:	learn: 102.3769680	total: 2.81s	remaining: 271ms
#&gt; 912:	learn: 102.3769677	total: 2.81s	remaining: 268ms
#&gt; 913:	learn: 102.3721131	total: 2.81s	remaining: 264ms
#&gt; 914:	learn: 102.3721119	total: 2.81s	remaining: 261ms
#&gt; 915:	learn: 102.3220796	total: 2.81s	remaining: 258ms
#&gt; 916:	learn: 102.1925187	total: 2.81s	remaining: 255ms
#&gt; 917:	learn: 102.1903891	total: 2.82s	remaining: 252ms
#&gt; 918:	learn: 102.1697213	total: 2.82s	remaining: 248ms
#&gt; 919:	learn: 102.1682371	total: 2.82s	remaining: 245ms
#&gt; 920:	learn: 102.1391129	total: 2.82s	remaining: 242ms
#&gt; 921:	learn: 101.8674755	total: 2.83s	remaining: 239ms
#&gt; 922:	learn: 101.8538748	total: 2.83s	remaining: 236ms
#&gt; 923:	learn: 101.8448687	total: 2.84s	remaining: 234ms
#&gt; 924:	learn: 101.7805765	total: 2.85s	remaining: 231ms
#&gt; 925:	learn: 101.7775553	total: 2.85s	remaining: 228ms
#&gt; 926:	learn: 101.7615941	total: 2.86s	remaining: 225ms
#&gt; 927:	learn: 101.7615941	total: 2.86s	remaining: 222ms
#&gt; 928:	learn: 101.7615941	total: 2.86s	remaining: 218ms
#&gt; 929:	learn: 101.7588311	total: 2.86s	remaining: 215ms
#&gt; 930:	learn: 101.5222030	total: 2.87s	remaining: 213ms
#&gt; 931:	learn: 101.5221984	total: 2.87s	remaining: 209ms
#&gt; 932:	learn: 101.5189363	total: 2.87s	remaining: 206ms
#&gt; 933:	learn: 101.3877384	total: 2.87s	remaining: 203ms
#&gt; 934:	learn: 101.3877378	total: 2.87s	remaining: 200ms
#&gt; 935:	learn: 101.3877374	total: 2.87s	remaining: 197ms
#&gt; 936:	learn: 101.3506375	total: 2.88s	remaining: 194ms
#&gt; 937:	learn: 101.3500880	total: 2.88s	remaining: 191ms
#&gt; 938:	learn: 101.3500878	total: 2.88s	remaining: 187ms
#&gt; 939:	learn: 101.3362643	total: 2.89s	remaining: 184ms
#&gt; 940:	learn: 101.3362643	total: 2.89s	remaining: 181ms
#&gt; 941:	learn: 101.3362314	total: 2.89s	remaining: 178ms
#&gt; 942:	learn: 101.3362314	total: 2.89s	remaining: 175ms
#&gt; 943:	learn: 101.3362314	total: 2.89s	remaining: 171ms
#&gt; 944:	learn: 101.3351433	total: 2.89s	remaining: 168ms
#&gt; 945:	learn: 101.3296777	total: 2.9s	remaining: 165ms
#&gt; 946:	learn: 100.8763593	total: 2.9s	remaining: 163ms
#&gt; 947:	learn: 100.8763561	total: 2.9s	remaining: 159ms
#&gt; 948:	learn: 100.8763535	total: 2.9s	remaining: 156ms
#&gt; 949:	learn: 100.8763514	total: 2.9s	remaining: 153ms
#&gt; 950:	learn: 100.8763497	total: 2.9s	remaining: 150ms
#&gt; 951:	learn: 100.8763483	total: 2.9s	remaining: 146ms
#&gt; 952:	learn: 100.8754964	total: 2.91s	remaining: 143ms
#&gt; 953:	learn: 100.8754955	total: 2.91s	remaining: 140ms
#&gt; 954:	learn: 100.8668946	total: 2.91s	remaining: 137ms
#&gt; 955:	learn: 100.8424146	total: 2.91s	remaining: 134ms
#&gt; 956:	learn: 100.8082663	total: 2.92s	remaining: 131ms
#&gt; 957:	learn: 100.8020267	total: 2.92s	remaining: 128ms
#&gt; 958:	learn: 100.7962112	total: 2.92s	remaining: 125ms
#&gt; 959:	learn: 100.7793002	total: 2.93s	remaining: 122ms
#&gt; 960:	learn: 100.7793001	total: 2.93s	remaining: 119ms
#&gt; 961:	learn: 100.7473526	total: 2.93s	remaining: 116ms
#&gt; 962:	learn: 100.7473518	total: 2.93s	remaining: 113ms
#&gt; 963:	learn: 100.7473511	total: 2.93s	remaining: 110ms
#&gt; 964:	learn: 100.7473505	total: 2.94s	remaining: 107ms
#&gt; 965:	learn: 100.6612971	total: 2.94s	remaining: 104ms
#&gt; 966:	learn: 100.5298954	total: 2.95s	remaining: 101ms
#&gt; 967:	learn: 100.5296187	total: 2.95s	remaining: 97.7ms
#&gt; 968:	learn: 100.5296186	total: 2.95s	remaining: 94.5ms
#&gt; 969:	learn: 100.2659267	total: 2.96s	remaining: 91.6ms
#&gt; 970:	learn: 100.2623564	total: 2.96s	remaining: 88.6ms
#&gt; 971:	learn: 100.2588936	total: 2.97s	remaining: 85.5ms
#&gt; 972:	learn: 100.2238164	total: 2.97s	remaining: 82.5ms
#&gt; 973:	learn: 100.2234678	total: 2.98s	remaining: 79.5ms
#&gt; 974:	learn: 100.1183496	total: 2.98s	remaining: 76.4ms
#&gt; 975:	learn: 100.1164142	total: 2.99s	remaining: 73.5ms
#&gt; 976:	learn: 100.1121374	total: 2.99s	remaining: 70.4ms
#&gt; 977:	learn: 100.0667855	total: 3s	remaining: 67.4ms
#&gt; 978:	learn: 100.0667854	total: 3s	remaining: 64.3ms
#&gt; 979:	learn: 100.0667853	total: 3s	remaining: 61.2ms
#&gt; 980:	learn: 100.0667853	total: 3s	remaining: 58.1ms
#&gt; 981:	learn: 99.8759117	total: 3s	remaining: 55ms
#&gt; 982:	learn: 99.8740704	total: 3s	remaining: 52ms
#&gt; 983:	learn: 99.7825510	total: 3.01s	remaining: 49ms
#&gt; 984:	learn: 99.7825490	total: 3.02s	remaining: 45.9ms
#&gt; 985:	learn: 99.7825473	total: 3.02s	remaining: 42.8ms
#&gt; 986:	learn: 99.7825459	total: 3.02s	remaining: 39.7ms
#&gt; 987:	learn: 99.7825448	total: 3.02s	remaining: 36.6ms
#&gt; 988:	learn: 99.6983938	total: 3.02s	remaining: 33.6ms
#&gt; 989:	learn: 99.6983881	total: 3.02s	remaining: 30.5ms
#&gt; 990:	learn: 99.6215226	total: 3.03s	remaining: 27.5ms
#&gt; 991:	learn: 99.5985652	total: 3.03s	remaining: 24.5ms
#&gt; 992:	learn: 99.5881560	total: 3.04s	remaining: 21.4ms
#&gt; 993:	learn: 99.5858288	total: 3.04s	remaining: 18.3ms
#&gt; 994:	learn: 99.5858263	total: 3.04s	remaining: 15.3ms
#&gt; 995:	learn: 99.5857733	total: 3.04s	remaining: 12.2ms
#&gt; 996:	learn: 99.5855811	total: 3.04s	remaining: 9.15ms
#&gt; 997:	learn: 99.5855796	total: 3.04s	remaining: 6.09ms
#&gt; 998:	learn: 99.5855783	total: 3.04s	remaining: 3.04ms
#&gt; 999:	learn: 99.5855773	total: 3.04s	remaining: 0us</div><div class='input'>
<span class='va'>model_fit_boosted</span>
</div><div class='output co'>#&gt; parsnip model object
#&gt; 
#&gt; Fit time:  4s 
#&gt; ARIMA(0,1,1)(0,1,1)[12] w/ Catboost Errors
#&gt; ---
#&gt; Model 1: Standard ARIMA
#&gt; Series: outcome 
#&gt; ARIMA(0,1,1)(0,1,1)[12] 
#&gt; 
#&gt; Coefficients:
#&gt;           ma1     sma1
#&gt;       -0.3405  -0.4781
#&gt; s.e.   0.0652   0.0628
#&gt; 
#&gt; sigma^2 estimated as 25114:  log likelihood=-1699.55
#&gt; AIC=3405.1   AICc=3405.19   BIC=3415.8
#&gt; 
#&gt; ---
#&gt; Model 2: Catboost Errors
#&gt; 
#&gt; CatBoost model (1000 trees)
#&gt; Loss function: RMSE
#&gt; Fit to 2 features</div><div class='input'>

</div></span></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Alberto Almuiña.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


