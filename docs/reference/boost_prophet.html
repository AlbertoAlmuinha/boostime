<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>General Interface for Boosted PROPHET Time Series Models — boost_prophet • boostime</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="General Interface for Boosted PROPHET Time Series Models — boost_prophet" />
<meta property="og:description" content="boost_prophet() is a way to generate a specification of a Boosted PROPHET model
before fitting and allows the model to be created using
different packages. Currently the only package is prophet." />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">boostime</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/getting-started.html">Getting Started with Boostime</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/AlbertoAlmuinha/boostime/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>General Interface for Boosted PROPHET Time Series Models</h1>
    <small class="dont-index">Source: <a href='https://github.com/AlbertoAlmuinha/boostime/blob/master/R/parsnip-prophet_boost.R'><code>R/parsnip-prophet_boost.R</code></a></small>
    <div class="hidden name"><code>boost_prophet.Rd</code></div>
    </div>

    <div class="ref-description">
    <p><code>boost_prophet()</code> is a way to generate a <em>specification</em> of a Boosted PROPHET model
before fitting and allows the model to be created using
different packages. Currently the only package is <code>prophet</code>.</p>
    </div>

    <pre class="usage"><span class='fu'>boost_prophet</span><span class='op'>(</span>
  mode <span class='op'>=</span> <span class='st'>"regression"</span>,
  growth <span class='op'>=</span> <span class='cn'>NULL</span>,
  changepoint_num <span class='op'>=</span> <span class='cn'>NULL</span>,
  changepoint_range <span class='op'>=</span> <span class='cn'>NULL</span>,
  seasonality_yearly <span class='op'>=</span> <span class='cn'>NULL</span>,
  seasonality_weekly <span class='op'>=</span> <span class='cn'>NULL</span>,
  seasonality_daily <span class='op'>=</span> <span class='cn'>NULL</span>,
  season <span class='op'>=</span> <span class='cn'>NULL</span>,
  prior_scale_changepoints <span class='op'>=</span> <span class='cn'>NULL</span>,
  prior_scale_seasonality <span class='op'>=</span> <span class='cn'>NULL</span>,
  prior_scale_holidays <span class='op'>=</span> <span class='cn'>NULL</span>,
  logistic_cap <span class='op'>=</span> <span class='cn'>NULL</span>,
  logistic_floor <span class='op'>=</span> <span class='cn'>NULL</span>,
  tree_depth <span class='op'>=</span> <span class='cn'>NULL</span>,
  learn_rate <span class='op'>=</span> <span class='cn'>NULL</span>,
  mtry <span class='op'>=</span> <span class='cn'>NULL</span>,
  trees <span class='op'>=</span> <span class='cn'>NULL</span>,
  min_n <span class='op'>=</span> <span class='cn'>NULL</span>,
  sample_size <span class='op'>=</span> <span class='cn'>NULL</span>,
  loss_reduction <span class='op'>=</span> <span class='cn'>NULL</span>
<span class='op'>)</span></pre>

    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>mode</th>
      <td><p>A single character string for the type of model.
The only possible value for this model is "regression".</p></td>
    </tr>
    <tr>
      <th>growth</th>
      <td><p>String 'linear' or 'logistic' to specify a linear or logistic trend.</p></td>
    </tr>
    <tr>
      <th>changepoint_num</th>
      <td><p>Number of potential changepoints to include for modeling trend.</p></td>
    </tr>
    <tr>
      <th>changepoint_range</th>
      <td><p>Adjusts the flexibility of the trend component by limiting to a percentage of data
before the end of the time series. 0.80 means that a changepoint cannot exist after the first 80% of the data.</p></td>
    </tr>
    <tr>
      <th>seasonality_yearly</th>
      <td><p>One of "auto", TRUE or FALSE. Toggles on/off a seasonal component that
models year-over-year seasonality.</p></td>
    </tr>
    <tr>
      <th>seasonality_weekly</th>
      <td><p>One of "auto", TRUE or FALSE. Toggles on/off a seasonal component that
models week-over-week seasonality.</p></td>
    </tr>
    <tr>
      <th>seasonality_daily</th>
      <td><p>One of "auto", TRUE or FALSE. Toggles on/off a seasonal componet that
models day-over-day seasonality.</p></td>
    </tr>
    <tr>
      <th>season</th>
      <td><p>'additive' (default) or 'multiplicative'.</p></td>
    </tr>
    <tr>
      <th>prior_scale_changepoints</th>
      <td><p>Parameter modulating the flexibility of the
automatic changepoint selection. Large values will allow many changepoints,
small values will allow few changepoints.</p></td>
    </tr>
    <tr>
      <th>prior_scale_seasonality</th>
      <td><p>Parameter modulating the strength of the
seasonality model. Larger values allow the model to fit larger seasonal
fluctuations, smaller values dampen the seasonality.</p></td>
    </tr>
    <tr>
      <th>prior_scale_holidays</th>
      <td><p>Parameter modulating the strength of the holiday components model,
unless overridden in the holidays input.</p></td>
    </tr>
    <tr>
      <th>logistic_cap</th>
      <td><p>When growth is logistic, the upper-bound for "saturation".</p></td>
    </tr>
    <tr>
      <th>logistic_floor</th>
      <td><p>When growth is logistic, the lower-bound for "saturation".</p></td>
    </tr>
    <tr>
      <th>tree_depth</th>
      <td><p>The maximum depth of the tree (i.e. number of splits).</p></td>
    </tr>
    <tr>
      <th>learn_rate</th>
      <td><p>The rate at which the boosting algorithm adapts from iteration-to-iteration.</p></td>
    </tr>
    <tr>
      <th>mtry</th>
      <td><p>The number of predictors that will be randomly sampled at each split when creating the tree models.</p></td>
    </tr>
    <tr>
      <th>trees</th>
      <td><p>The number of trees contained in the ensemble.</p></td>
    </tr>
    <tr>
      <th>min_n</th>
      <td><p>The minimum number of data points in a node that is required for the node to be split further.</p></td>
    </tr>
    <tr>
      <th>sample_size</th>
      <td><p>The amount of data exposed to the fitting routine.</p></td>
    </tr>
    <tr>
      <th>loss_reduction</th>
      <td><p>The reduction in the loss function required to split further.</p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    <p>The data given to the function are not saved and are only used
to determine the <em>mode</em> of the model. For <code>boost_prophet()</code>, the
mode will always be "regression".</p>
<p>The model can be created using the <code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code> function using the
following <em>engines</em>:</p><ul>
<li><p>"prophet_catboost" (default) - Connects to <code><a href='https://rdrr.io/pkg/prophet/man/prophet.html'>prophet::prophet()</a></code> and <code><a href='https://rdrr.io/pkg/catboost/man/catboost.train.html'>catboost::catboost.train()</a></code></p></li>
<li><p>"prophet_lightgbm" - Connects to <code><a href='https://rdrr.io/pkg/prophet/man/prophet.html'>prophet::prophet()</a></code> and <code><a href='https://rdrr.io/pkg/lightgbm/man/lgb.train.html'>lightgbm::lgb.train()</a></code></p></li>
</ul>

<p><strong>Main Arguments</strong></p>
<p>The main arguments (tuning parameters) for the <strong>PROPHET</strong> model are:</p><ul>
<li><p><code>growth</code>: String 'linear' or 'logistic' to specify a linear or logistic trend.</p></li>
<li><p><code>changepoint_num</code>: Number of potential changepoints to include for modeling trend.</p></li>
<li><p><code>changepoint_range</code>: Range changepoints that adjusts how close to the end
the last changepoint can be located.</p></li>
<li><p><code>season</code>: 'additive' (default) or 'multiplicative'.</p></li>
<li><p><code>prior_scale_changepoints</code>: Parameter modulating the flexibility of the
automatic changepoint selection. Large values will allow many changepoints,
small values will allow few changepoints.</p></li>
<li><p><code>prior_scale_seasonality</code>: Parameter modulating the strength of the
seasonality model. Larger values allow the model to fit larger seasonal
fluctuations, smaller values dampen the seasonality.</p></li>
<li><p><code>prior_scale_holidays</code>: Parameter modulating the strength of the holiday components model,
unless overridden in the holidays input.</p></li>
</ul>

<p>The main arguments (tuning parameters) for the model <strong>Catboost/LightGBM model</strong> are:</p><ul>
<li><p><code>tree_depth</code>: The maximum depth of the tree (i.e. number of splits).</p></li>
<li><p><code>learn_rate</code>: The rate at which the boosting algorithm adapts from iteration-to-iteration.</p></li>
<li><p><code>mtry</code>: The number of predictors that will be randomly sampled at each split when creating the tree models.</p></li>
<li><p><code>trees</code>: The number of trees contained in the ensemble.</p></li>
<li><p><code>min_n</code>: The minimum number of data points in a node that is required for the node to be split further.</p></li>
<li><p><code>sample_size</code>: The amount of data exposed to the fitting routine.</p></li>
<li><p><code>loss_reduction</code>: The reduction in the loss function required to split further.</p></li>
</ul>

<p>These arguments are converted to their specific names at the
time that the model is fit.</p>
<p>Other options and argument can be
set using <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code> (See Engine Details below).</p>
<p>If parameters need to be modified, <code><a href='https://rdrr.io/r/stats/update.html'>update()</a></code> can be used
in lieu of recreating the object from scratch.</p>
    <h2 class="hasAnchor" id="engine-details"><a class="anchor" href="#engine-details"></a>Engine Details</h2>

    


<p>The standardized parameter names in <code>boostime</code> can be mapped to their original
names in each engine:</p>
<p>Model 1: PROPHET:</p><table class='table'>
<tr><td>boostime</td><td>prophet</td></tr>
<tr><td>growth</td><td>growth ('linear')</td></tr>
<tr><td>changepoint_num</td><td>n.changepoints (25)</td></tr>
<tr><td>changepoint_range</td><td>changepoints.range (0.8)</td></tr>
<tr><td>seasonality_yearly</td><td>yearly.seasonality ('auto')</td></tr>
<tr><td>seasonality_weekly</td><td>weekly.seasonality ('auto')</td></tr>
<tr><td>seasonality_daily</td><td>daily.seasonality ('auto')</td></tr>
<tr><td>season</td><td>seasonality.mode ('additive')</td></tr>
<tr><td>prior_scale_changepoints</td><td>changepoint.prior.scale (0.05)</td></tr>
<tr><td>prior_scale_seasonality</td><td>seasonality.prior.scale (10)</td></tr>
<tr><td>prior_scale_holidays</td><td>holidays.prior.scale (10)</td></tr>
<tr><td>logistic_cap</td><td>df$cap (NULL)</td></tr>
<tr><td>logistic_floor</td><td>df$floor (NULL)</td></tr>
</table>



<p>Model 2: Catboost / LightGBM:</p><table class='table'>
<tr><td>boostime</td><td>catboost::catboost.train</td><td>lightgbm::lgb.train</td></tr>
<tr><td>tree_depth</td><td>depth</td><td>max_depth</td></tr>
<tr><td>learn_rate</td><td>learning_rate</td><td>learning_rate</td></tr>
<tr><td>mtry</td><td>rsm</td><td>feature_fraction</td></tr>
<tr><td>trees</td><td>iterations</td><td>num_iterations</td></tr>
<tr><td>min_n</td><td>min_data_in_leaf</td><td>min_data_in_leaf</td></tr>
<tr><td>loss_reduction</td><td>None</td><td>min_gain_to_split</td></tr>
<tr><td>sample_size</td><td>subsample</td><td>bagging_fraction</td></tr>
</table>



<p>Other options can be set using <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code>.</p>
<p><strong>prophet_catboost</strong></p>
<p>Model 1: PROPHET (<code><a href='https://rdrr.io/pkg/prophet/man/prophet.html'>prophet::prophet</a></code>):</p><pre><span class='co'>## function (df = NULL, growth = "linear", changepoints = NULL, n.changepoints = 25, </span>
<span class='co'>##     changepoint.range = 0.8, yearly.seasonality = "auto", weekly.seasonality = "auto", </span>
<span class='co'>##     daily.seasonality = "auto", holidays = NULL, seasonality.mode = "additive", </span>
<span class='co'>##     seasonality.prior.scale = 10, holidays.prior.scale = 10, changepoint.prior.scale = 0.05, </span>
<span class='co'>##     mcmc.samples = 0, interval.width = 0.8, uncertainty.samples = 1000, </span>
<span class='co'>##     fit = TRUE, ...)</span>
</pre>

<p>Parameter Notes:</p><ul>
<li><p><code>df</code>: This is supplied via the parsnip / boostime <code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code> interface
(so don't provide this manually). See Fit Details (below).</p></li>
<li><p><code>holidays</code>: A data.frame of holidays can be supplied via <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code></p></li>
<li><p><code>uncertainty.samples</code>: The default is set to 0 because the prophet
uncertainty intervals are not used as part of the Modeltime Workflow.
You can override this setting if you plan to use prophet's uncertainty tools.</p></li>
</ul>

<p>Logistic Growth and Saturation Levels:</p><ul>
<li><p>For <code>growth = "logistic"</code>, simply add numeric values for <code>logistic_cap</code> and / or
<code>logistic_floor</code>. There is <em>no need</em> to add additional columns
for "cap" and "floor" to your data frame.</p></li>
</ul>

<p>Limitations:</p><ul>
<li><p><code><a href='https://rdrr.io/pkg/prophet/man/add_seasonality.html'>prophet::add_seasonality()</a></code> is not currently implemented. It's used to
specify non-standard seasonalities using fourier series. An alternative is to use
<code><a href='https://rdrr.io/pkg/timetk/man/step_fourier.html'>step_fourier()</a></code> and supply custom seasonalities as Extra Regressors.</p></li>
</ul>

<p>Model 2: Catboost (<code><a href='https://rdrr.io/pkg/catboost/man/catboost.train.html'>catboost::catboost.train</a></code>):</p><pre><span class='co'>## function (learn_pool, test_pool = NULL, params = list())</span>
</pre>

<p>Parameter Notes:</p><ul>
<li><p>Catboost uses a <code>params = list()</code> to capture.
Parsnip / Timeboost automatically sends any args provided as <code>...</code> inside of <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code> to
the <code>params = list(...)</code>.</p></li>
</ul>

<p><strong>prophet_lightgbm</strong></p>
<p>Model 1: PROPHET (<code><a href='https://rdrr.io/pkg/prophet/man/prophet.html'>prophet::prophet</a></code>):</p><pre><span class='co'>## function (df = NULL, growth = "linear", changepoints = NULL, n.changepoints = 25, </span>
<span class='co'>##     changepoint.range = 0.8, yearly.seasonality = "auto", weekly.seasonality = "auto", </span>
<span class='co'>##     daily.seasonality = "auto", holidays = NULL, seasonality.mode = "additive", </span>
<span class='co'>##     seasonality.prior.scale = 10, holidays.prior.scale = 10, changepoint.prior.scale = 0.05, </span>
<span class='co'>##     mcmc.samples = 0, interval.width = 0.8, uncertainty.samples = 1000, </span>
<span class='co'>##     fit = TRUE, ...)</span>
</pre>

<p>Parameter Notes:</p><ul>
<li><p><code>df</code>: This is supplied via the parsnip / boostime <code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code> interface
(so don't provide this manually). See Fit Details (below).</p></li>
<li><p><code>holidays</code>: A data.frame of holidays can be supplied via <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code></p></li>
<li><p><code>uncertainty.samples</code>: The default is set to 0 because the prophet
uncertainty intervals are not used as part of the Modeltime Workflow.
You can override this setting if you plan to use prophet's uncertainty tools.</p></li>
</ul>

<p>Logistic Growth and Saturation Levels:</p><ul>
<li><p>For <code>growth = "logistic"</code>, simply add numeric values for <code>logistic_cap</code> and / or
<code>logistic_floor</code>. There is <em>no need</em> to add additional columns
for "cap" and "floor" to your data frame.</p></li>
</ul>

<p>Limitations:</p><ul>
<li><p><code><a href='https://rdrr.io/pkg/prophet/man/add_seasonality.html'>prophet::add_seasonality()</a></code> is not currently implemented. It's used to
specify non-standard seasonalities using fourier series. An alternative is to use
<code><a href='https://rdrr.io/pkg/timetk/man/step_fourier.html'>step_fourier()</a></code> and supply custom seasonalities as Extra Regressors.</p></li>
</ul>

<p>Model 2: Lightgbm (<code><a href='https://rdrr.io/pkg/catboost/man/catboost.train.html'>catboost::catboost.train</a></code>):</p><pre><span class='co'>## function (params = list(), data, nrounds = 10L, valids = list(), obj = NULL, </span>
<span class='co'>##     eval = NULL, verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL, </span>
<span class='co'>##     colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL, </span>
<span class='co'>##     callbacks = list(), reset_data = FALSE, ...)</span>
</pre>

<p>Parameter Notes:</p><ul>
<li><p>Lightgbm uses a <code>params = list()</code> to capture.
Parsnip / Timeboost automatically sends any args provided as <code>...</code> inside of <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code> to
the <code>params = list(...)</code>.</p></li>
</ul>

    <h2 class="hasAnchor" id="fit-details"><a class="anchor" href="#fit-details"></a>Fit Details</h2>

    


<p><strong>Date and Date-Time Variable</strong></p>
<p>It's a requirement to have a date or date-time variable as a predictor.
The <code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code> interface accepts date and date-time features and handles them internally.</p><ul>
<li><p><code><a href='https://generics.r-lib.org/reference/fit.html'>fit(y ~ date)</a></code></p></li>
</ul>

<p><strong>Univariate (No Extra Regressors):</strong></p>
<p>For univariate analysis, you must include a date or date-time feature. Simply use:</p><ul>
<li><p>Formula Interface (recommended): <code><a href='https://generics.r-lib.org/reference/fit.html'>fit(y ~ date)</a></code> will ignore xreg's.</p></li>
</ul>

<p><strong>Multivariate (Extra Regressors)</strong></p>
<p>Extra Regressors parameter is populated using the <code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code> or <code><a href='https://generics.r-lib.org/reference/fit_xy.html'>fit_xy()</a></code> function:</p><ul>
<li><p>Only <code>factor</code>, <code>ordered factor</code>, and <code>numeric</code> data will be used as xregs.</p></li>
<li><p>Date and Date-time variables are not used as xregs</p></li>
<li><p><code>character</code> data should be converted to factor.</p></li>
</ul>

<p><em>Xreg Example:</em> Suppose you have 3 features:</p><ol>
<li><p><code>y</code> (target)</p></li>
<li><p><code>date</code> (time stamp),</p></li>
<li><p><code>month.lbl</code> (labeled month as a ordered factor).</p></li>
</ol>

<p>The <code>month.lbl</code> is an exogenous regressor that can be passed to the <code>arima_reg()</code> using
<code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code>:</p><ul>
<li><p><code><a href='https://generics.r-lib.org/reference/fit.html'>fit(y ~ date + month.lbl)</a></code> will pass <code>month.lbl</code> on as an exogenous regressor.</p></li>
</ul>

<p>Note that date or date-time class values are excluded from <code>xreg</code>.</p>
    <h2 class="hasAnchor" id="see-also"><a class="anchor" href="#see-also"></a>See also</h2>

    <div class='dont-index'><p><code><a href='https://parsnip.tidymodels.org/reference/fit.html'>fit.model_spec()</a></code>, <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code></p></div>

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://dplyr.tidyverse.org'>dplyr</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://lubridate.tidyverse.org'>lubridate</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://parsnip.tidymodels.org'>parsnip</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://rsample.tidymodels.org'>rsample</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://github.com/business-science/timetk'>timetk</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://github.com/AlbertoAlmuinha/boostime'>boostime</a></span><span class='op'>)</span>

<span class='co'># Data</span>
<span class='va'>m750</span> <span class='op'>&lt;-</span> <span class='va'>m4_monthly</span> <span class='op'>%&gt;%</span> <span class='fu'><a href='https://dplyr.tidyverse.org/reference/filter.html'>filter</a></span><span class='op'>(</span><span class='va'>id</span> <span class='op'>==</span> <span class='st'>"M750"</span><span class='op'>)</span>
<span class='va'>m750</span>
</div><div class='output co'>#&gt; <span style='color: #949494;'># A tibble: 306 x 3</span><span>
#&gt;    id    date       value
#&gt;    </span><span style='color: #949494;font-style: italic;'>&lt;fct&gt;</span><span> </span><span style='color: #949494;font-style: italic;'>&lt;date&gt;</span><span>     </span><span style='color: #949494;font-style: italic;'>&lt;dbl&gt;</span><span>
#&gt; </span><span style='color: #BCBCBC;'> 1</span><span> M750  1990-01-01  </span><span style='text-decoration: underline;'>6</span><span>370
#&gt; </span><span style='color: #BCBCBC;'> 2</span><span> M750  1990-02-01  </span><span style='text-decoration: underline;'>6</span><span>430
#&gt; </span><span style='color: #BCBCBC;'> 3</span><span> M750  1990-03-01  </span><span style='text-decoration: underline;'>6</span><span>520
#&gt; </span><span style='color: #BCBCBC;'> 4</span><span> M750  1990-04-01  </span><span style='text-decoration: underline;'>6</span><span>580
#&gt; </span><span style='color: #BCBCBC;'> 5</span><span> M750  1990-05-01  </span><span style='text-decoration: underline;'>6</span><span>620
#&gt; </span><span style='color: #BCBCBC;'> 6</span><span> M750  1990-06-01  </span><span style='text-decoration: underline;'>6</span><span>690
#&gt; </span><span style='color: #BCBCBC;'> 7</span><span> M750  1990-07-01  </span><span style='text-decoration: underline;'>6</span><span>000
#&gt; </span><span style='color: #BCBCBC;'> 8</span><span> M750  1990-08-01  </span><span style='text-decoration: underline;'>5</span><span>450
#&gt; </span><span style='color: #BCBCBC;'> 9</span><span> M750  1990-09-01  </span><span style='text-decoration: underline;'>6</span><span>480
#&gt; </span><span style='color: #BCBCBC;'>10</span><span> M750  1990-10-01  </span><span style='text-decoration: underline;'>6</span><span>820
#&gt; </span><span style='color: #949494;'># ... with 296 more rows</span><span></div><div class='input'>
<span class='co'># Split Data 80/20</span>
<span class='va'>splits</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rsample.tidymodels.org/reference/initial_split.html'>initial_time_split</a></span><span class='op'>(</span><span class='va'>m750</span>, prop <span class='op'>=</span> <span class='fl'>0.8</span><span class='op'>)</span>

<span class='co'># ---- PROPHET ----</span>

<span class='co'># Model Spec</span>
<span class='va'>model_spec</span> <span class='op'>&lt;-</span> <span class='fu'>boost_prophet</span><span class='op'>(</span>
    learn_rate <span class='op'>=</span> <span class='fl'>0.1</span>
<span class='op'>)</span> <span class='op'>%&gt;%</span>
    <span class='fu'><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine</a></span><span class='op'>(</span><span class='st'>"prophet_catboost"</span><span class='op'>)</span>

<span class='co'># Fit Spec</span>
<span class='va'>model_fit</span> <span class='op'>&lt;-</span> <span class='va'>model_spec</span> <span class='op'>%&gt;%</span>
    <span class='fu'><a href='https://generics.r-lib.org/reference/fit.html'>fit</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/Log.html'>log</a></span><span class='op'>(</span><span class='va'>value</span><span class='op'>)</span> <span class='op'>~</span> <span class='va'>date</span> <span class='op'>+</span> <span class='fu'><a href='https://rdrr.io/r/base/numeric.html'>as.numeric</a></span><span class='op'>(</span><span class='va'>date</span><span class='op'>)</span> <span class='op'>+</span> <span class='fu'><a href='http://lubridate.tidyverse.org/reference/month.html'>month</a></span><span class='op'>(</span><span class='va'>date</span>, label <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span>,
        data <span class='op'>=</span> <span class='fu'><a href='https://rsample.tidymodels.org/reference/initial_split.html'>training</a></span><span class='op'>(</span><span class='va'>splits</span><span class='op'>)</span><span class='op'>)</span>
</div><div class='output co'>#&gt; <span class='message'>Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this.</span></div><div class='output co'>#&gt; <span class='message'>Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.</span></div><div class='output co'>#&gt; 0:	learn: 0.0184466	total: 270us	remaining: 270ms
#&gt; 1:	learn: 0.0184093	total: 3.07ms	remaining: 1.53s
#&gt; 2:	learn: 0.0183456	total: 5.58ms	remaining: 1.85s
#&gt; 3:	learn: 0.0183456	total: 5.84ms	remaining: 1.45s
#&gt; 4:	learn: 0.0181939	total: 15.1ms	remaining: 3s
#&gt; 5:	learn: 0.0181443	total: 17.8ms	remaining: 2.95s
#&gt; 6:	learn: 0.0180893	total: 24.8ms	remaining: 3.52s
#&gt; 7:	learn: 0.0180763	total: 32.1ms	remaining: 3.98s
#&gt; 8:	learn: 0.0180763	total: 32.3ms	remaining: 3.56s
#&gt; 9:	learn: 0.0180643	total: 34.7ms	remaining: 3.43s
#&gt; 10:	learn: 0.0179167	total: 35.5ms	remaining: 3.19s
#&gt; 11:	learn: 0.0178917	total: 37.8ms	remaining: 3.11s
#&gt; 12:	learn: 0.0178917	total: 38ms	remaining: 2.88s
#&gt; 13:	learn: 0.0178818	total: 40.3ms	remaining: 2.84s
#&gt; 14:	learn: 0.0178659	total: 44.4ms	remaining: 2.92s
#&gt; 15:	learn: 0.0178626	total: 47.1ms	remaining: 2.9s
#&gt; 16:	learn: 0.0178170	total: 49.9ms	remaining: 2.89s
#&gt; 17:	learn: 0.0178160	total: 54.3ms	remaining: 2.96s
#&gt; 18:	learn: 0.0177858	total: 85.3ms	remaining: 4.41s
#&gt; 19:	learn: 0.0177472	total: 96.8ms	remaining: 4.75s
#&gt; 20:	learn: 0.0177472	total: 97.1ms	remaining: 4.53s
#&gt; 21:	learn: 0.0177466	total: 100ms	remaining: 4.45s
#&gt; 22:	learn: 0.0177466	total: 100ms	remaining: 4.26s
#&gt; 23:	learn: 0.0176130	total: 107ms	remaining: 4.34s
#&gt; 24:	learn: 0.0176125	total: 112ms	remaining: 4.37s
#&gt; 25:	learn: 0.0176125	total: 112ms	remaining: 4.21s
#&gt; 26:	learn: 0.0176125	total: 113ms	remaining: 4.06s
#&gt; 27:	learn: 0.0176117	total: 131ms	remaining: 4.54s
#&gt; 28:	learn: 0.0176117	total: 131ms	remaining: 4.39s
#&gt; 29:	learn: 0.0176084	total: 137ms	remaining: 4.42s
#&gt; 30:	learn: 0.0176084	total: 137ms	remaining: 4.28s
#&gt; 31:	learn: 0.0176084	total: 137ms	remaining: 4.15s
#&gt; 32:	learn: 0.0176084	total: 137ms	remaining: 4.03s
#&gt; 33:	learn: 0.0174870	total: 138ms	remaining: 3.92s
#&gt; 34:	learn: 0.0174600	total: 144ms	remaining: 3.96s
#&gt; 35:	learn: 0.0174039	total: 155ms	remaining: 4.14s
#&gt; 36:	learn: 0.0174039	total: 155ms	remaining: 4.03s
#&gt; 37:	learn: 0.0174039	total: 155ms	remaining: 3.93s
#&gt; 38:	learn: 0.0174039	total: 155ms	remaining: 3.83s
#&gt; 39:	learn: 0.0174039	total: 156ms	remaining: 3.73s
#&gt; 40:	learn: 0.0174039	total: 156ms	remaining: 3.64s
#&gt; 41:	learn: 0.0174039	total: 156ms	remaining: 3.56s
#&gt; 42:	learn: 0.0173994	total: 161ms	remaining: 3.59s
#&gt; 43:	learn: 0.0173871	total: 166ms	remaining: 3.6s
#&gt; 44:	learn: 0.0173785	total: 172ms	remaining: 3.66s
#&gt; 45:	learn: 0.0173785	total: 173ms	remaining: 3.58s
#&gt; 46:	learn: 0.0173785	total: 173ms	remaining: 3.51s
#&gt; 47:	learn: 0.0171579	total: 176ms	remaining: 3.49s
#&gt; 48:	learn: 0.0171566	total: 192ms	remaining: 3.73s
#&gt; 49:	learn: 0.0171404	total: 208ms	remaining: 3.95s
#&gt; 50:	learn: 0.0171403	total: 208ms	remaining: 3.87s
#&gt; 51:	learn: 0.0171403	total: 208ms	remaining: 3.8s
#&gt; 52:	learn: 0.0171403	total: 209ms	remaining: 3.73s
#&gt; 53:	learn: 0.0171383	total: 211ms	remaining: 3.69s
#&gt; 54:	learn: 0.0171383	total: 211ms	remaining: 3.63s
#&gt; 55:	learn: 0.0171383	total: 211ms	remaining: 3.56s
#&gt; 56:	learn: 0.0171383	total: 212ms	remaining: 3.5s
#&gt; 57:	learn: 0.0171373	total: 214ms	remaining: 3.47s
#&gt; 58:	learn: 0.0171373	total: 214ms	remaining: 3.41s
#&gt; 59:	learn: 0.0170848	total: 219ms	remaining: 3.42s
#&gt; 60:	learn: 0.0170848	total: 219ms	remaining: 3.37s
#&gt; 61:	learn: 0.0170789	total: 221ms	remaining: 3.35s
#&gt; 62:	learn: 0.0170789	total: 221ms	remaining: 3.29s
#&gt; 63:	learn: 0.0169316	total: 228ms	remaining: 3.33s
#&gt; 64:	learn: 0.0169316	total: 228ms	remaining: 3.28s
#&gt; 65:	learn: 0.0167391	total: 238ms	remaining: 3.36s
#&gt; 66:	learn: 0.0166536	total: 245ms	remaining: 3.41s
#&gt; 67:	learn: 0.0166524	total: 253ms	remaining: 3.46s
#&gt; 68:	learn: 0.0166419	total: 254ms	remaining: 3.42s
#&gt; 69:	learn: 0.0166419	total: 254ms	remaining: 3.38s
#&gt; 70:	learn: 0.0166255	total: 255ms	remaining: 3.34s
#&gt; 71:	learn: 0.0166240	total: 263ms	remaining: 3.39s
#&gt; 72:	learn: 0.0166240	total: 264ms	remaining: 3.35s
#&gt; 73:	learn: 0.0165554	total: 266ms	remaining: 3.33s
#&gt; 74:	learn: 0.0165554	total: 267ms	remaining: 3.29s
#&gt; 75:	learn: 0.0165302	total: 269ms	remaining: 3.27s
#&gt; 76:	learn: 0.0165294	total: 273ms	remaining: 3.27s
#&gt; 77:	learn: 0.0165294	total: 274ms	remaining: 3.23s
#&gt; 78:	learn: 0.0164096	total: 279ms	remaining: 3.26s
#&gt; 79:	learn: 0.0164096	total: 280ms	remaining: 3.22s
#&gt; 80:	learn: 0.0163844	total: 305ms	remaining: 3.46s
#&gt; 81:	learn: 0.0163260	total: 306ms	remaining: 3.43s
#&gt; 82:	learn: 0.0163047	total: 313ms	remaining: 3.46s
#&gt; 83:	learn: 0.0162572	total: 331ms	remaining: 3.61s
#&gt; 84:	learn: 0.0162352	total: 337ms	remaining: 3.63s
#&gt; 85:	learn: 0.0161210	total: 343ms	remaining: 3.65s
#&gt; 86:	learn: 0.0161210	total: 344ms	remaining: 3.6s
#&gt; 87:	learn: 0.0161209	total: 344ms	remaining: 3.56s
#&gt; 88:	learn: 0.0161209	total: 344ms	remaining: 3.52s
#&gt; 89:	learn: 0.0161209	total: 344ms	remaining: 3.48s
#&gt; 90:	learn: 0.0160148	total: 350ms	remaining: 3.49s
#&gt; 91:	learn: 0.0159943	total: 355ms	remaining: 3.5s
#&gt; 92:	learn: 0.0159835	total: 357ms	remaining: 3.49s
#&gt; 93:	learn: 0.0158843	total: 365ms	remaining: 3.52s
#&gt; 94:	learn: 0.0158843	total: 368ms	remaining: 3.5s
#&gt; 95:	learn: 0.0158417	total: 371ms	remaining: 3.5s
#&gt; 96:	learn: 0.0158417	total: 374ms	remaining: 3.48s
#&gt; 97:	learn: 0.0157630	total: 379ms	remaining: 3.49s
#&gt; 98:	learn: 0.0157618	total: 384ms	remaining: 3.49s
#&gt; 99:	learn: 0.0157460	total: 386ms	remaining: 3.48s
#&gt; 100:	learn: 0.0157460	total: 387ms	remaining: 3.44s
#&gt; 101:	learn: 0.0157460	total: 389ms	remaining: 3.42s
#&gt; 102:	learn: 0.0156627	total: 396ms	remaining: 3.45s
#&gt; 103:	learn: 0.0156627	total: 396ms	remaining: 3.41s
#&gt; 104:	learn: 0.0156627	total: 396ms	remaining: 3.38s
#&gt; 105:	learn: 0.0156560	total: 401ms	remaining: 3.38s
#&gt; 106:	learn: 0.0156111	total: 415ms	remaining: 3.46s
#&gt; 107:	learn: 0.0156111	total: 415ms	remaining: 3.43s
#&gt; 108:	learn: 0.0156111	total: 415ms	remaining: 3.39s
#&gt; 109:	learn: 0.0156111	total: 415ms	remaining: 3.36s
#&gt; 110:	learn: 0.0156111	total: 416ms	remaining: 3.33s
#&gt; 111:	learn: 0.0156111	total: 416ms	remaining: 3.3s
#&gt; 112:	learn: 0.0155912	total: 419ms	remaining: 3.29s
#&gt; 113:	learn: 0.0155912	total: 419ms	remaining: 3.26s
#&gt; 114:	learn: 0.0155603	total: 433ms	remaining: 3.33s
#&gt; 115:	learn: 0.0154795	total: 444ms	remaining: 3.38s
#&gt; 116:	learn: 0.0154646	total: 461ms	remaining: 3.48s
#&gt; 117:	learn: 0.0154646	total: 462ms	remaining: 3.45s
#&gt; 118:	learn: 0.0154390	total: 519ms	remaining: 3.85s
#&gt; 119:	learn: 0.0153427	total: 529ms	remaining: 3.88s
#&gt; 120:	learn: 0.0153349	total: 537ms	remaining: 3.9s
#&gt; 121:	learn: 0.0153349	total: 538ms	remaining: 3.87s
#&gt; 122:	learn: 0.0153266	total: 549ms	remaining: 3.92s
#&gt; 123:	learn: 0.0153266	total: 550ms	remaining: 3.89s
#&gt; 124:	learn: 0.0153202	total: 555ms	remaining: 3.89s
#&gt; 125:	learn: 0.0153021	total: 566ms	remaining: 3.93s
#&gt; 126:	learn: 0.0153021	total: 567ms	remaining: 3.9s
#&gt; 127:	learn: 0.0152848	total: 579ms	remaining: 3.94s
#&gt; 128:	learn: 0.0152846	total: 582ms	remaining: 3.93s
#&gt; 129:	learn: 0.0152846	total: 585ms	remaining: 3.92s
#&gt; 130:	learn: 0.0152846	total: 585ms	remaining: 3.88s
#&gt; 131:	learn: 0.0152846	total: 585ms	remaining: 3.85s
#&gt; 132:	learn: 0.0152608	total: 590ms	remaining: 3.84s
#&gt; 133:	learn: 0.0152608	total: 590ms	remaining: 3.81s
#&gt; 134:	learn: 0.0152608	total: 590ms	remaining: 3.78s
#&gt; 135:	learn: 0.0152566	total: 590ms	remaining: 3.75s
#&gt; 136:	learn: 0.0152531	total: 591ms	remaining: 3.72s
#&gt; 137:	learn: 0.0152531	total: 591ms	remaining: 3.69s
#&gt; 138:	learn: 0.0152264	total: 594ms	remaining: 3.68s
#&gt; 139:	learn: 0.0152253	total: 597ms	remaining: 3.67s
#&gt; 140:	learn: 0.0152253	total: 597ms	remaining: 3.64s
#&gt; 141:	learn: 0.0152201	total: 600ms	remaining: 3.62s
#&gt; 142:	learn: 0.0152201	total: 600ms	remaining: 3.6s
#&gt; 143:	learn: 0.0152201	total: 600ms	remaining: 3.57s
#&gt; 144:	learn: 0.0152146	total: 605ms	remaining: 3.56s
#&gt; 145:	learn: 0.0152135	total: 622ms	remaining: 3.64s
#&gt; 146:	learn: 0.0152074	total: 640ms	remaining: 3.71s
#&gt; 147:	learn: 0.0152071	total: 654ms	remaining: 3.76s
#&gt; 148:	learn: 0.0152071	total: 654ms	remaining: 3.73s
#&gt; 149:	learn: 0.0152071	total: 654ms	remaining: 3.71s
#&gt; 150:	learn: 0.0151509	total: 666ms	remaining: 3.75s
#&gt; 151:	learn: 0.0151458	total: 669ms	remaining: 3.73s
#&gt; 152:	learn: 0.0151458	total: 669ms	remaining: 3.7s
#&gt; 153:	learn: 0.0151458	total: 669ms	remaining: 3.67s
#&gt; 154:	learn: 0.0151458	total: 669ms	remaining: 3.65s
#&gt; 155:	learn: 0.0151458	total: 670ms	remaining: 3.62s
#&gt; 156:	learn: 0.0151347	total: 678ms	remaining: 3.64s
#&gt; 157:	learn: 0.0151283	total: 683ms	remaining: 3.64s
#&gt; 158:	learn: 0.0151283	total: 685ms	remaining: 3.62s
#&gt; 159:	learn: 0.0150850	total: 688ms	remaining: 3.61s
#&gt; 160:	learn: 0.0150849	total: 690ms	remaining: 3.6s
#&gt; 161:	learn: 0.0150849	total: 690ms	remaining: 3.57s
#&gt; 162:	learn: 0.0150704	total: 693ms	remaining: 3.56s
#&gt; 163:	learn: 0.0150704	total: 693ms	remaining: 3.53s
#&gt; 164:	learn: 0.0150704	total: 699ms	remaining: 3.54s
#&gt; 165:	learn: 0.0150317	total: 702ms	remaining: 3.52s
#&gt; 166:	learn: 0.0150165	total: 704ms	remaining: 3.51s
#&gt; 167:	learn: 0.0150165	total: 704ms	remaining: 3.49s
#&gt; 168:	learn: 0.0149515	total: 715ms	remaining: 3.51s
#&gt; 169:	learn: 0.0149515	total: 715ms	remaining: 3.49s
#&gt; 170:	learn: 0.0149515	total: 715ms	remaining: 3.47s
#&gt; 171:	learn: 0.0149515	total: 716ms	remaining: 3.44s
#&gt; 172:	learn: 0.0149515	total: 718ms	remaining: 3.43s
#&gt; 173:	learn: 0.0149492	total: 720ms	remaining: 3.42s
#&gt; 174:	learn: 0.0149492	total: 720ms	remaining: 3.4s
#&gt; 175:	learn: 0.0149485	total: 723ms	remaining: 3.38s
#&gt; 176:	learn: 0.0149485	total: 723ms	remaining: 3.36s
#&gt; 177:	learn: 0.0149485	total: 723ms	remaining: 3.34s
#&gt; 178:	learn: 0.0149450	total: 723ms	remaining: 3.32s
#&gt; 179:	learn: 0.0149450	total: 726ms	remaining: 3.31s
#&gt; 180:	learn: 0.0149450	total: 726ms	remaining: 3.28s
#&gt; 181:	learn: 0.0149450	total: 726ms	remaining: 3.26s
#&gt; 182:	learn: 0.0149450	total: 726ms	remaining: 3.24s
#&gt; 183:	learn: 0.0149450	total: 726ms	remaining: 3.22s
#&gt; 184:	learn: 0.0149450	total: 729ms	remaining: 3.21s
#&gt; 185:	learn: 0.0149450	total: 729ms	remaining: 3.19s
#&gt; 186:	learn: 0.0149429	total: 729ms	remaining: 3.17s
#&gt; 187:	learn: 0.0149429	total: 730ms	remaining: 3.15s
#&gt; 188:	learn: 0.0149429	total: 730ms	remaining: 3.13s
#&gt; 189:	learn: 0.0149170	total: 734ms	remaining: 3.13s
#&gt; 190:	learn: 0.0147666	total: 739ms	remaining: 3.13s
#&gt; 191:	learn: 0.0147665	total: 742ms	remaining: 3.12s
#&gt; 192:	learn: 0.0147644	total: 742ms	remaining: 3.1s
#&gt; 193:	learn: 0.0147619	total: 763ms	remaining: 3.17s
#&gt; 194:	learn: 0.0147619	total: 763ms	remaining: 3.15s
#&gt; 195:	learn: 0.0147618	total: 763ms	remaining: 3.13s
#&gt; 196:	learn: 0.0147612	total: 765ms	remaining: 3.12s
#&gt; 197:	learn: 0.0147430	total: 770ms	remaining: 3.12s
#&gt; 198:	learn: 0.0147430	total: 770ms	remaining: 3.1s
#&gt; 199:	learn: 0.0147430	total: 770ms	remaining: 3.08s
#&gt; 200:	learn: 0.0147430	total: 771ms	remaining: 3.06s
#&gt; 201:	learn: 0.0147430	total: 771ms	remaining: 3.04s
#&gt; 202:	learn: 0.0147430	total: 771ms	remaining: 3.03s
#&gt; 203:	learn: 0.0147430	total: 771ms	remaining: 3.01s
#&gt; 204:	learn: 0.0146553	total: 778ms	remaining: 3.02s
#&gt; 205:	learn: 0.0146531	total: 783ms	remaining: 3.02s
#&gt; 206:	learn: 0.0146378	total: 793ms	remaining: 3.04s
#&gt; 207:	learn: 0.0145457	total: 808ms	remaining: 3.08s
#&gt; 208:	learn: 0.0145457	total: 808ms	remaining: 3.06s
#&gt; 209:	learn: 0.0145457	total: 809ms	remaining: 3.04s
#&gt; 210:	learn: 0.0145453	total: 811ms	remaining: 3.03s
#&gt; 211:	learn: 0.0145349	total: 821ms	remaining: 3.05s
#&gt; 212:	learn: 0.0145288	total: 824ms	remaining: 3.04s
#&gt; 213:	learn: 0.0145286	total: 827ms	remaining: 3.04s
#&gt; 214:	learn: 0.0145260	total: 827ms	remaining: 3.02s
#&gt; 215:	learn: 0.0144341	total: 836ms	remaining: 3.03s
#&gt; 216:	learn: 0.0144200	total: 841ms	remaining: 3.04s
#&gt; 217:	learn: 0.0144150	total: 856ms	remaining: 3.07s
#&gt; 218:	learn: 0.0143308	total: 869ms	remaining: 3.1s
#&gt; 219:	learn: 0.0143171	total: 870ms	remaining: 3.08s
#&gt; 220:	learn: 0.0143132	total: 887ms	remaining: 3.13s
#&gt; 221:	learn: 0.0143132	total: 887ms	remaining: 3.11s
#&gt; 222:	learn: 0.0143132	total: 887ms	remaining: 3.09s
#&gt; 223:	learn: 0.0142809	total: 892ms	remaining: 3.09s
#&gt; 224:	learn: 0.0142737	total: 896ms	remaining: 3.09s
#&gt; 225:	learn: 0.0142737	total: 898ms	remaining: 3.08s
#&gt; 226:	learn: 0.0142728	total: 904ms	remaining: 3.08s
#&gt; 227:	learn: 0.0142728	total: 906ms	remaining: 3.07s
#&gt; 228:	learn: 0.0141790	total: 913ms	remaining: 3.07s
#&gt; 229:	learn: 0.0141729	total: 931ms	remaining: 3.12s
#&gt; 230:	learn: 0.0141700	total: 934ms	remaining: 3.11s
#&gt; 231:	learn: 0.0141087	total: 946ms	remaining: 3.13s
#&gt; 232:	learn: 0.0141087	total: 947ms	remaining: 3.12s
#&gt; 233:	learn: 0.0141082	total: 949ms	remaining: 3.11s
#&gt; 234:	learn: 0.0140686	total: 955ms	remaining: 3.11s
#&gt; 235:	learn: 0.0140555	total: 960ms	remaining: 3.11s
#&gt; 236:	learn: 0.0140555	total: 960ms	remaining: 3.09s
#&gt; 237:	learn: 0.0140555	total: 963ms	remaining: 3.08s
#&gt; 238:	learn: 0.0140555	total: 963ms	remaining: 3.07s
#&gt; 239:	learn: 0.0140555	total: 963ms	remaining: 3.05s
#&gt; 240:	learn: 0.0140555	total: 963ms	remaining: 3.03s
#&gt; 241:	learn: 0.0140555	total: 964ms	remaining: 3.02s
#&gt; 242:	learn: 0.0140555	total: 964ms	remaining: 3s
#&gt; 243:	learn: 0.0140555	total: 964ms	remaining: 2.99s
#&gt; 244:	learn: 0.0140555	total: 964ms	remaining: 2.97s
#&gt; 245:	learn: 0.0140551	total: 967ms	remaining: 2.96s
#&gt; 246:	learn: 0.0140410	total: 969ms	remaining: 2.95s
#&gt; 247:	learn: 0.0140410	total: 970ms	remaining: 2.94s
#&gt; 248:	learn: 0.0140410	total: 970ms	remaining: 2.92s
#&gt; 249:	learn: 0.0140410	total: 970ms	remaining: 2.91s
#&gt; 250:	learn: 0.0140410	total: 970ms	remaining: 2.9s
#&gt; 251:	learn: 0.0140410	total: 970ms	remaining: 2.88s
#&gt; 252:	learn: 0.0140410	total: 971ms	remaining: 2.87s
#&gt; 253:	learn: 0.0140410	total: 971ms	remaining: 2.85s
#&gt; 254:	learn: 0.0140410	total: 971ms	remaining: 2.84s
#&gt; 255:	learn: 0.0140410	total: 971ms	remaining: 2.82s
#&gt; 256:	learn: 0.0140410	total: 971ms	remaining: 2.81s
#&gt; 257:	learn: 0.0140410	total: 974ms	remaining: 2.8s
#&gt; 258:	learn: 0.0140410	total: 974ms	remaining: 2.79s
#&gt; 259:	learn: 0.0140410	total: 974ms	remaining: 2.77s
#&gt; 260:	learn: 0.0140404	total: 977ms	remaining: 2.77s
#&gt; 261:	learn: 0.0140388	total: 977ms	remaining: 2.75s
#&gt; 262:	learn: 0.0140388	total: 979ms	remaining: 2.74s
#&gt; 263:	learn: 0.0140354	total: 986ms	remaining: 2.75s
#&gt; 264:	learn: 0.0140354	total: 986ms	remaining: 2.73s
#&gt; 265:	learn: 0.0140030	total: 993ms	remaining: 2.74s
#&gt; 266:	learn: 0.0140030	total: 993ms	remaining: 2.73s
#&gt; 267:	learn: 0.0140030	total: 994ms	remaining: 2.71s
#&gt; 268:	learn: 0.0139968	total: 996ms	remaining: 2.71s
#&gt; 269:	learn: 0.0139968	total: 998ms	remaining: 2.7s
#&gt; 270:	learn: 0.0139804	total: 1s	remaining: 2.69s
#&gt; 271:	learn: 0.0139615	total: 1.01s	remaining: 2.7s
#&gt; 272:	learn: 0.0139615	total: 1.01s	remaining: 2.68s
#&gt; 273:	learn: 0.0139613	total: 1.01s	remaining: 2.68s
#&gt; 274:	learn: 0.0139613	total: 1.01s	remaining: 2.66s
#&gt; 275:	learn: 0.0138954	total: 1.01s	remaining: 2.65s
#&gt; 276:	learn: 0.0138954	total: 1.01s	remaining: 2.64s
#&gt; 277:	learn: 0.0138886	total: 1.01s	remaining: 2.64s
#&gt; 278:	learn: 0.0138886	total: 1.01s	remaining: 2.62s
#&gt; 279:	learn: 0.0138886	total: 1.01s	remaining: 2.61s
#&gt; 280:	learn: 0.0138414	total: 1.02s	remaining: 2.61s
#&gt; 281:	learn: 0.0138414	total: 1.02s	remaining: 2.6s
#&gt; 282:	learn: 0.0138346	total: 1.02s	remaining: 2.59s
#&gt; 283:	learn: 0.0138168	total: 1.04s	remaining: 2.62s
#&gt; 284:	learn: 0.0138142	total: 1.04s	remaining: 2.61s
#&gt; 285:	learn: 0.0138142	total: 1.04s	remaining: 2.6s
#&gt; 286:	learn: 0.0138139	total: 1.05s	remaining: 2.62s
#&gt; 287:	learn: 0.0137760	total: 1.06s	remaining: 2.62s
#&gt; 288:	learn: 0.0137750	total: 1.06s	remaining: 2.61s
#&gt; 289:	learn: 0.0137555	total: 1.07s	remaining: 2.62s
#&gt; 290:	learn: 0.0137530	total: 1.07s	remaining: 2.61s
#&gt; 291:	learn: 0.0137384	total: 1.08s	remaining: 2.62s
#&gt; 292:	learn: 0.0137356	total: 1.08s	remaining: 2.61s
#&gt; 293:	learn: 0.0137119	total: 1.09s	remaining: 2.61s
#&gt; 294:	learn: 0.0137119	total: 1.09s	remaining: 2.6s
#&gt; 295:	learn: 0.0137022	total: 1.1s	remaining: 2.62s
#&gt; 296:	learn: 0.0137022	total: 1.1s	remaining: 2.61s
#&gt; 297:	learn: 0.0137022	total: 1.1s	remaining: 2.6s
#&gt; 298:	learn: 0.0137022	total: 1.1s	remaining: 2.59s
#&gt; 299:	learn: 0.0136997	total: 1.1s	remaining: 2.57s
#&gt; 300:	learn: 0.0135719	total: 1.11s	remaining: 2.58s
#&gt; 301:	learn: 0.0135719	total: 1.11s	remaining: 2.57s
#&gt; 302:	learn: 0.0135719	total: 1.11s	remaining: 2.56s
#&gt; 303:	learn: 0.0135719	total: 1.11s	remaining: 2.54s
#&gt; 304:	learn: 0.0135719	total: 1.11s	remaining: 2.53s
#&gt; 305:	learn: 0.0135716	total: 1.12s	remaining: 2.54s
#&gt; 306:	learn: 0.0135379	total: 1.12s	remaining: 2.53s
#&gt; 307:	learn: 0.0135379	total: 1.12s	remaining: 2.52s
#&gt; 308:	learn: 0.0135118	total: 1.13s	remaining: 2.53s
#&gt; 309:	learn: 0.0135042	total: 1.14s	remaining: 2.54s
#&gt; 310:	learn: 0.0134998	total: 1.14s	remaining: 2.54s
#&gt; 311:	learn: 0.0134996	total: 1.15s	remaining: 2.53s
#&gt; 312:	learn: 0.0134996	total: 1.15s	remaining: 2.52s
#&gt; 313:	learn: 0.0134996	total: 1.15s	remaining: 2.51s
#&gt; 314:	learn: 0.0134996	total: 1.15s	remaining: 2.5s
#&gt; 315:	learn: 0.0134996	total: 1.15s	remaining: 2.48s
#&gt; 316:	learn: 0.0134972	total: 1.15s	remaining: 2.47s
#&gt; 317:	learn: 0.0134972	total: 1.15s	remaining: 2.46s
#&gt; 318:	learn: 0.0134937	total: 1.15s	remaining: 2.46s
#&gt; 319:	learn: 0.0134937	total: 1.15s	remaining: 2.44s
#&gt; 320:	learn: 0.0134937	total: 1.15s	remaining: 2.43s
#&gt; 321:	learn: 0.0134937	total: 1.15s	remaining: 2.42s
#&gt; 322:	learn: 0.0134937	total: 1.15s	remaining: 2.41s
#&gt; 323:	learn: 0.0134937	total: 1.15s	remaining: 2.4s
#&gt; 324:	learn: 0.0134922	total: 1.15s	remaining: 2.39s
#&gt; 325:	learn: 0.0133865	total: 1.16s	remaining: 2.4s
#&gt; 326:	learn: 0.0133843	total: 1.16s	remaining: 2.39s
#&gt; 327:	learn: 0.0133783	total: 1.17s	remaining: 2.39s
#&gt; 328:	learn: 0.0133758	total: 1.17s	remaining: 2.38s
#&gt; 329:	learn: 0.0133758	total: 1.17s	remaining: 2.37s
#&gt; 330:	learn: 0.0133577	total: 1.18s	remaining: 2.38s
#&gt; 331:	learn: 0.0133090	total: 1.18s	remaining: 2.38s
#&gt; 332:	learn: 0.0133074	total: 1.19s	remaining: 2.37s
#&gt; 333:	learn: 0.0133074	total: 1.19s	remaining: 2.36s
#&gt; 334:	learn: 0.0133074	total: 1.19s	remaining: 2.35s
#&gt; 335:	learn: 0.0133074	total: 1.19s	remaining: 2.34s
#&gt; 336:	learn: 0.0132508	total: 1.19s	remaining: 2.35s
#&gt; 337:	learn: 0.0131405	total: 1.2s	remaining: 2.36s
#&gt; 338:	learn: 0.0131405	total: 1.2s	remaining: 2.35s
#&gt; 339:	learn: 0.0131403	total: 1.21s	remaining: 2.34s
#&gt; 340:	learn: 0.0131403	total: 1.21s	remaining: 2.33s
#&gt; 341:	learn: 0.0131403	total: 1.21s	remaining: 2.32s
#&gt; 342:	learn: 0.0131403	total: 1.21s	remaining: 2.31s
#&gt; 343:	learn: 0.0131400	total: 1.21s	remaining: 2.31s
#&gt; 344:	learn: 0.0130483	total: 1.22s	remaining: 2.31s
#&gt; 345:	learn: 0.0130483	total: 1.22s	remaining: 2.3s
#&gt; 346:	learn: 0.0130482	total: 1.22s	remaining: 2.29s
#&gt; 347:	learn: 0.0130470	total: 1.22s	remaining: 2.28s
#&gt; 348:	learn: 0.0130470	total: 1.22s	remaining: 2.27s
#&gt; 349:	learn: 0.0130461	total: 1.22s	remaining: 2.27s
#&gt; 350:	learn: 0.0130164	total: 1.23s	remaining: 2.27s
#&gt; 351:	learn: 0.0130164	total: 1.23s	remaining: 2.27s
#&gt; 352:	learn: 0.0129360	total: 1.23s	remaining: 2.26s
#&gt; 353:	learn: 0.0129360	total: 1.23s	remaining: 2.25s
#&gt; 354:	learn: 0.0129291	total: 1.24s	remaining: 2.25s
#&gt; 355:	learn: 0.0128904	total: 1.27s	remaining: 2.3s
#&gt; 356:	learn: 0.0128903	total: 1.27s	remaining: 2.29s
#&gt; 357:	learn: 0.0128903	total: 1.28s	remaining: 2.29s
#&gt; 358:	learn: 0.0128883	total: 1.28s	remaining: 2.28s
#&gt; 359:	learn: 0.0128868	total: 1.29s	remaining: 2.3s
#&gt; 360:	learn: 0.0128859	total: 1.31s	remaining: 2.32s
#&gt; 361:	learn: 0.0128859	total: 1.31s	remaining: 2.31s
#&gt; 362:	learn: 0.0128859	total: 1.31s	remaining: 2.3s
#&gt; 363:	learn: 0.0128859	total: 1.31s	remaining: 2.29s
#&gt; 364:	learn: 0.0128335	total: 1.32s	remaining: 2.3s
#&gt; 365:	learn: 0.0128158	total: 1.33s	remaining: 2.31s
#&gt; 366:	learn: 0.0128158	total: 1.34s	remaining: 2.31s
#&gt; 367:	learn: 0.0127791	total: 1.34s	remaining: 2.31s
#&gt; 368:	learn: 0.0127108	total: 1.37s	remaining: 2.34s
#&gt; 369:	learn: 0.0127108	total: 1.37s	remaining: 2.33s
#&gt; 370:	learn: 0.0127108	total: 1.37s	remaining: 2.33s
#&gt; 371:	learn: 0.0127108	total: 1.37s	remaining: 2.31s
#&gt; 372:	learn: 0.0127107	total: 1.37s	remaining: 2.31s
#&gt; 373:	learn: 0.0127107	total: 1.38s	remaining: 2.31s
#&gt; 374:	learn: 0.0127107	total: 1.38s	remaining: 2.3s
#&gt; 375:	learn: 0.0126832	total: 1.39s	remaining: 2.3s
#&gt; 376:	learn: 0.0126578	total: 1.4s	remaining: 2.31s
#&gt; 377:	learn: 0.0126578	total: 1.4s	remaining: 2.3s
#&gt; 378:	learn: 0.0126417	total: 1.42s	remaining: 2.32s
#&gt; 379:	learn: 0.0126417	total: 1.42s	remaining: 2.31s
#&gt; 380:	learn: 0.0126396	total: 1.42s	remaining: 2.3s
#&gt; 381:	learn: 0.0126332	total: 1.43s	remaining: 2.31s
#&gt; 382:	learn: 0.0126318	total: 1.43s	remaining: 2.31s
#&gt; 383:	learn: 0.0126187	total: 1.46s	remaining: 2.35s
#&gt; 384:	learn: 0.0126187	total: 1.46s	remaining: 2.34s
#&gt; 385:	learn: 0.0126187	total: 1.46s	remaining: 2.33s
#&gt; 386:	learn: 0.0126187	total: 1.46s	remaining: 2.32s
#&gt; 387:	learn: 0.0126187	total: 1.46s	remaining: 2.31s
#&gt; 388:	learn: 0.0126187	total: 1.46s	remaining: 2.3s
#&gt; 389:	learn: 0.0125174	total: 1.48s	remaining: 2.32s
#&gt; 390:	learn: 0.0125174	total: 1.48s	remaining: 2.31s
#&gt; 391:	learn: 0.0125052	total: 1.49s	remaining: 2.31s
#&gt; 392:	learn: 0.0125052	total: 1.49s	remaining: 2.3s
#&gt; 393:	learn: 0.0124929	total: 1.49s	remaining: 2.29s
#&gt; 394:	learn: 0.0123567	total: 1.5s	remaining: 2.29s
#&gt; 395:	learn: 0.0123425	total: 1.5s	remaining: 2.29s
#&gt; 396:	learn: 0.0123424	total: 1.51s	remaining: 2.29s
#&gt; 397:	learn: 0.0123414	total: 1.51s	remaining: 2.28s
#&gt; 398:	learn: 0.0123414	total: 1.51s	remaining: 2.27s
#&gt; 399:	learn: 0.0123370	total: 1.52s	remaining: 2.27s
#&gt; 400:	learn: 0.0123370	total: 1.52s	remaining: 2.27s
#&gt; 401:	learn: 0.0123370	total: 1.52s	remaining: 2.26s
#&gt; 402:	learn: 0.0123370	total: 1.52s	remaining: 2.25s
#&gt; 403:	learn: 0.0123370	total: 1.52s	remaining: 2.24s
#&gt; 404:	learn: 0.0123370	total: 1.52s	remaining: 2.23s
#&gt; 405:	learn: 0.0123360	total: 1.52s	remaining: 2.23s
#&gt; 406:	learn: 0.0123358	total: 1.52s	remaining: 2.22s
#&gt; 407:	learn: 0.0123358	total: 1.52s	remaining: 2.21s
#&gt; 408:	learn: 0.0123357	total: 1.53s	remaining: 2.21s
#&gt; 409:	learn: 0.0123246	total: 1.53s	remaining: 2.2s
#&gt; 410:	learn: 0.0123246	total: 1.53s	remaining: 2.19s
#&gt; 411:	learn: 0.0123244	total: 1.53s	remaining: 2.19s
#&gt; 412:	learn: 0.0123244	total: 1.53s	remaining: 2.18s
#&gt; 413:	learn: 0.0123130	total: 1.54s	remaining: 2.17s
#&gt; 414:	learn: 0.0123130	total: 1.54s	remaining: 2.17s
#&gt; 415:	learn: 0.0123130	total: 1.54s	remaining: 2.16s
#&gt; 416:	learn: 0.0123086	total: 1.54s	remaining: 2.15s
#&gt; 417:	learn: 0.0123082	total: 1.54s	remaining: 2.15s
#&gt; 418:	learn: 0.0123082	total: 1.54s	remaining: 2.14s
#&gt; 419:	learn: 0.0123082	total: 1.54s	remaining: 2.13s
#&gt; 420:	learn: 0.0123082	total: 1.54s	remaining: 2.12s
#&gt; 421:	learn: 0.0123071	total: 1.55s	remaining: 2.12s
#&gt; 422:	learn: 0.0123070	total: 1.55s	remaining: 2.11s
#&gt; 423:	learn: 0.0123070	total: 1.55s	remaining: 2.1s
#&gt; 424:	learn: 0.0123058	total: 1.57s	remaining: 2.12s
#&gt; 425:	learn: 0.0122842	total: 1.57s	remaining: 2.12s
#&gt; 426:	learn: 0.0122435	total: 1.58s	remaining: 2.12s
#&gt; 427:	learn: 0.0122324	total: 1.59s	remaining: 2.12s
#&gt; 428:	learn: 0.0122324	total: 1.59s	remaining: 2.12s
#&gt; 429:	learn: 0.0122269	total: 1.59s	remaining: 2.11s
#&gt; 430:	learn: 0.0122269	total: 1.59s	remaining: 2.1s
#&gt; 431:	learn: 0.0122269	total: 1.59s	remaining: 2.1s
#&gt; 432:	learn: 0.0122044	total: 1.61s	remaining: 2.1s
#&gt; 433:	learn: 0.0122044	total: 1.61s	remaining: 2.1s
#&gt; 434:	learn: 0.0122044	total: 1.61s	remaining: 2.09s
#&gt; 435:	learn: 0.0121984	total: 1.62s	remaining: 2.09s
#&gt; 436:	learn: 0.0121984	total: 1.62s	remaining: 2.08s
#&gt; 437:	learn: 0.0121984	total: 1.62s	remaining: 2.07s
#&gt; 438:	learn: 0.0121977	total: 1.62s	remaining: 2.07s
#&gt; 439:	learn: 0.0121840	total: 1.63s	remaining: 2.07s
#&gt; 440:	learn: 0.0121820	total: 1.63s	remaining: 2.07s
#&gt; 441:	learn: 0.0121820	total: 1.63s	remaining: 2.06s
#&gt; 442:	learn: 0.0121820	total: 1.63s	remaining: 2.05s
#&gt; 443:	learn: 0.0121818	total: 1.63s	remaining: 2.05s
#&gt; 444:	learn: 0.0121073	total: 1.64s	remaining: 2.04s
#&gt; 445:	learn: 0.0119924	total: 1.67s	remaining: 2.07s
#&gt; 446:	learn: 0.0119924	total: 1.67s	remaining: 2.06s
#&gt; 447:	learn: 0.0119924	total: 1.67s	remaining: 2.06s
#&gt; 448:	learn: 0.0119912	total: 1.67s	remaining: 2.05s
#&gt; 449:	learn: 0.0119912	total: 1.67s	remaining: 2.04s
#&gt; 450:	learn: 0.0119912	total: 1.67s	remaining: 2.03s
#&gt; 451:	learn: 0.0119912	total: 1.67s	remaining: 2.02s
#&gt; 452:	learn: 0.0119911	total: 1.67s	remaining: 2.02s
#&gt; 453:	learn: 0.0119911	total: 1.67s	remaining: 2.01s
#&gt; 454:	learn: 0.0119911	total: 1.68s	remaining: 2.01s
#&gt; 455:	learn: 0.0119825	total: 1.68s	remaining: 2s
#&gt; 456:	learn: 0.0119797	total: 1.69s	remaining: 2s
#&gt; 457:	learn: 0.0119797	total: 1.69s	remaining: 1.99s
#&gt; 458:	learn: 0.0119797	total: 1.69s	remaining: 1.99s
#&gt; 459:	learn: 0.0119797	total: 1.69s	remaining: 1.98s
#&gt; 460:	learn: 0.0119790	total: 1.69s	remaining: 1.98s
#&gt; 461:	learn: 0.0119790	total: 1.69s	remaining: 1.97s
#&gt; 462:	learn: 0.0119790	total: 1.69s	remaining: 1.96s
#&gt; 463:	learn: 0.0119786	total: 1.69s	remaining: 1.96s
#&gt; 464:	learn: 0.0119786	total: 1.69s	remaining: 1.95s
#&gt; 465:	learn: 0.0119742	total: 1.7s	remaining: 1.95s
#&gt; 466:	learn: 0.0119668	total: 1.7s	remaining: 1.94s
#&gt; 467:	learn: 0.0119668	total: 1.7s	remaining: 1.93s
#&gt; 468:	learn: 0.0119665	total: 1.7s	remaining: 1.93s
#&gt; 469:	learn: 0.0119200	total: 1.71s	remaining: 1.93s
#&gt; 470:	learn: 0.0119088	total: 1.71s	remaining: 1.92s
#&gt; 471:	learn: 0.0119085	total: 1.71s	remaining: 1.92s
#&gt; 472:	learn: 0.0119067	total: 1.71s	remaining: 1.91s
#&gt; 473:	learn: 0.0119067	total: 1.71s	remaining: 1.9s
#&gt; 474:	learn: 0.0119067	total: 1.71s	remaining: 1.89s
#&gt; 475:	learn: 0.0119063	total: 1.72s	remaining: 1.89s
#&gt; 476:	learn: 0.0119063	total: 1.72s	remaining: 1.88s
#&gt; 477:	learn: 0.0119047	total: 1.72s	remaining: 1.87s
#&gt; 478:	learn: 0.0119047	total: 1.72s	remaining: 1.87s
#&gt; 479:	learn: 0.0119042	total: 1.72s	remaining: 1.86s
#&gt; 480:	learn: 0.0119042	total: 1.72s	remaining: 1.85s
#&gt; 481:	learn: 0.0119042	total: 1.72s	remaining: 1.85s
#&gt; 482:	learn: 0.0119042	total: 1.72s	remaining: 1.84s
#&gt; 483:	learn: 0.0118956	total: 1.72s	remaining: 1.84s
#&gt; 484:	learn: 0.0118908	total: 1.73s	remaining: 1.84s
#&gt; 485:	learn: 0.0118564	total: 1.74s	remaining: 1.84s
#&gt; 486:	learn: 0.0118564	total: 1.74s	remaining: 1.83s
#&gt; 487:	learn: 0.0118564	total: 1.74s	remaining: 1.82s
#&gt; 488:	learn: 0.0118564	total: 1.74s	remaining: 1.82s
#&gt; 489:	learn: 0.0118548	total: 1.74s	remaining: 1.81s
#&gt; 490:	learn: 0.0118548	total: 1.74s	remaining: 1.81s
#&gt; 491:	learn: 0.0118548	total: 1.74s	remaining: 1.8s
#&gt; 492:	learn: 0.0118548	total: 1.75s	remaining: 1.79s
#&gt; 493:	learn: 0.0118364	total: 1.75s	remaining: 1.79s
#&gt; 494:	learn: 0.0117812	total: 1.76s	remaining: 1.79s
#&gt; 495:	learn: 0.0117700	total: 1.76s	remaining: 1.79s
#&gt; 496:	learn: 0.0117700	total: 1.76s	remaining: 1.78s
#&gt; 497:	learn: 0.0117700	total: 1.76s	remaining: 1.78s
#&gt; 498:	learn: 0.0117695	total: 1.76s	remaining: 1.77s
#&gt; 499:	learn: 0.0117695	total: 1.76s	remaining: 1.76s
#&gt; 500:	learn: 0.0117695	total: 1.76s	remaining: 1.76s
#&gt; 501:	learn: 0.0117686	total: 1.77s	remaining: 1.76s
#&gt; 502:	learn: 0.0117686	total: 1.77s	remaining: 1.75s
#&gt; 503:	learn: 0.0117290	total: 1.78s	remaining: 1.75s
#&gt; 504:	learn: 0.0117275	total: 1.78s	remaining: 1.74s
#&gt; 505:	learn: 0.0117261	total: 1.78s	remaining: 1.74s
#&gt; 506:	learn: 0.0117261	total: 1.78s	remaining: 1.73s
#&gt; 507:	learn: 0.0117247	total: 1.78s	remaining: 1.72s
#&gt; 508:	learn: 0.0117246	total: 1.78s	remaining: 1.72s
#&gt; 509:	learn: 0.0117056	total: 1.79s	remaining: 1.72s
#&gt; 510:	learn: 0.0116964	total: 1.79s	remaining: 1.71s
#&gt; 511:	learn: 0.0116952	total: 1.8s	remaining: 1.71s
#&gt; 512:	learn: 0.0116947	total: 1.8s	remaining: 1.71s
#&gt; 513:	learn: 0.0116937	total: 1.8s	remaining: 1.71s
#&gt; 514:	learn: 0.0116937	total: 1.81s	remaining: 1.7s
#&gt; 515:	learn: 0.0116444	total: 1.82s	remaining: 1.71s
#&gt; 516:	learn: 0.0116444	total: 1.83s	remaining: 1.71s
#&gt; 517:	learn: 0.0116444	total: 1.83s	remaining: 1.7s
#&gt; 518:	learn: 0.0116444	total: 1.83s	remaining: 1.69s
#&gt; 519:	learn: 0.0116273	total: 1.83s	remaining: 1.69s
#&gt; 520:	learn: 0.0116178	total: 1.84s	remaining: 1.69s
#&gt; 521:	learn: 0.0115938	total: 1.85s	remaining: 1.69s
#&gt; 522:	learn: 0.0115800	total: 1.89s	remaining: 1.73s
#&gt; 523:	learn: 0.0115702	total: 1.9s	remaining: 1.72s
#&gt; 524:	learn: 0.0115684	total: 1.9s	remaining: 1.72s
#&gt; 525:	learn: 0.0115684	total: 1.9s	remaining: 1.71s
#&gt; 526:	learn: 0.0115684	total: 1.9s	remaining: 1.7s
#&gt; 527:	learn: 0.0115684	total: 1.9s	remaining: 1.7s
#&gt; 528:	learn: 0.0115684	total: 1.9s	remaining: 1.69s
#&gt; 529:	learn: 0.0115684	total: 1.9s	remaining: 1.69s
#&gt; 530:	learn: 0.0115671	total: 1.91s	remaining: 1.68s
#&gt; 531:	learn: 0.0115665	total: 1.92s	remaining: 1.69s
#&gt; 532:	learn: 0.0115665	total: 1.92s	remaining: 1.68s
#&gt; 533:	learn: 0.0115663	total: 1.92s	remaining: 1.68s
#&gt; 534:	learn: 0.0115648	total: 1.92s	remaining: 1.67s
#&gt; 535:	learn: 0.0115504	total: 1.93s	remaining: 1.67s
#&gt; 536:	learn: 0.0115504	total: 1.94s	remaining: 1.67s
#&gt; 537:	learn: 0.0115491	total: 1.94s	remaining: 1.66s
#&gt; 538:	learn: 0.0115484	total: 1.94s	remaining: 1.66s
#&gt; 539:	learn: 0.0115404	total: 1.94s	remaining: 1.66s
#&gt; 540:	learn: 0.0115404	total: 1.95s	remaining: 1.65s
#&gt; 541:	learn: 0.0115373	total: 1.95s	remaining: 1.65s
#&gt; 542:	learn: 0.0115373	total: 1.95s	remaining: 1.64s
#&gt; 543:	learn: 0.0115373	total: 1.95s	remaining: 1.63s
#&gt; 544:	learn: 0.0115373	total: 1.95s	remaining: 1.63s
#&gt; 545:	learn: 0.0115249	total: 1.95s	remaining: 1.62s
#&gt; 546:	learn: 0.0115249	total: 1.95s	remaining: 1.62s
#&gt; 547:	learn: 0.0115238	total: 1.96s	remaining: 1.61s
#&gt; 548:	learn: 0.0115238	total: 1.96s	remaining: 1.61s
#&gt; 549:	learn: 0.0115086	total: 1.96s	remaining: 1.6s
#&gt; 550:	learn: 0.0115086	total: 1.96s	remaining: 1.6s
#&gt; 551:	learn: 0.0114995	total: 1.97s	remaining: 1.6s
#&gt; 552:	learn: 0.0114995	total: 1.97s	remaining: 1.59s
#&gt; 553:	learn: 0.0114975	total: 1.97s	remaining: 1.59s
#&gt; 554:	learn: 0.0114880	total: 1.98s	remaining: 1.59s
#&gt; 555:	learn: 0.0114880	total: 1.98s	remaining: 1.58s
#&gt; 556:	learn: 0.0114880	total: 1.98s	remaining: 1.58s
#&gt; 557:	learn: 0.0114380	total: 1.99s	remaining: 1.57s
#&gt; 558:	learn: 0.0114286	total: 1.99s	remaining: 1.57s
#&gt; 559:	learn: 0.0114286	total: 1.99s	remaining: 1.56s
#&gt; 560:	learn: 0.0114286	total: 1.99s	remaining: 1.56s
#&gt; 561:	learn: 0.0114286	total: 1.99s	remaining: 1.55s
#&gt; 562:	learn: 0.0114272	total: 2.01s	remaining: 1.56s
#&gt; 563:	learn: 0.0114272	total: 2.01s	remaining: 1.55s
#&gt; 564:	learn: 0.0114238	total: 2.02s	remaining: 1.55s
#&gt; 565:	learn: 0.0114238	total: 2.02s	remaining: 1.55s
#&gt; 566:	learn: 0.0114193	total: 2.02s	remaining: 1.54s
#&gt; 567:	learn: 0.0114193	total: 2.02s	remaining: 1.54s
#&gt; 568:	learn: 0.0114188	total: 2.06s	remaining: 1.56s
#&gt; 569:	learn: 0.0113432	total: 2.08s	remaining: 1.57s
#&gt; 570:	learn: 0.0113134	total: 2.08s	remaining: 1.56s
#&gt; 571:	learn: 0.0112374	total: 2.08s	remaining: 1.56s
#&gt; 572:	learn: 0.0112314	total: 2.09s	remaining: 1.56s
#&gt; 573:	learn: 0.0112310	total: 2.09s	remaining: 1.55s
#&gt; 574:	learn: 0.0112309	total: 2.1s	remaining: 1.55s
#&gt; 575:	learn: 0.0112309	total: 2.1s	remaining: 1.54s
#&gt; 576:	learn: 0.0112309	total: 2.1s	remaining: 1.54s
#&gt; 577:	learn: 0.0112309	total: 2.1s	remaining: 1.53s
#&gt; 578:	learn: 0.0112309	total: 2.1s	remaining: 1.53s
#&gt; 579:	learn: 0.0112106	total: 2.1s	remaining: 1.52s
#&gt; 580:	learn: 0.0112106	total: 2.1s	remaining: 1.52s
#&gt; 581:	learn: 0.0111587	total: 2.11s	remaining: 1.52s
#&gt; 582:	learn: 0.0111587	total: 2.11s	remaining: 1.51s
#&gt; 583:	learn: 0.0111559	total: 2.11s	remaining: 1.5s
#&gt; 584:	learn: 0.0111559	total: 2.11s	remaining: 1.5s
#&gt; 585:	learn: 0.0111559	total: 2.11s	remaining: 1.49s
#&gt; 586:	learn: 0.0111465	total: 2.12s	remaining: 1.49s
#&gt; 587:	learn: 0.0111465	total: 2.12s	remaining: 1.49s
#&gt; 588:	learn: 0.0111465	total: 2.12s	remaining: 1.48s
#&gt; 589:	learn: 0.0111465	total: 2.12s	remaining: 1.47s
#&gt; 590:	learn: 0.0111465	total: 2.12s	remaining: 1.47s
#&gt; 591:	learn: 0.0111464	total: 2.12s	remaining: 1.46s
#&gt; 592:	learn: 0.0111464	total: 2.12s	remaining: 1.46s
#&gt; 593:	learn: 0.0111436	total: 2.13s	remaining: 1.45s
#&gt; 594:	learn: 0.0111436	total: 2.13s	remaining: 1.45s
#&gt; 595:	learn: 0.0111419	total: 2.13s	remaining: 1.44s
#&gt; 596:	learn: 0.0111419	total: 2.13s	remaining: 1.44s
#&gt; 597:	learn: 0.0111328	total: 2.15s	remaining: 1.45s
#&gt; 598:	learn: 0.0111328	total: 2.15s	remaining: 1.44s
#&gt; 599:	learn: 0.0111326	total: 2.16s	remaining: 1.44s
#&gt; 600:	learn: 0.0111326	total: 2.17s	remaining: 1.44s
#&gt; 601:	learn: 0.0111326	total: 2.17s	remaining: 1.43s
#&gt; 602:	learn: 0.0111326	total: 2.17s	remaining: 1.43s
#&gt; 603:	learn: 0.0111326	total: 2.17s	remaining: 1.42s
#&gt; 604:	learn: 0.0111315	total: 2.18s	remaining: 1.42s
#&gt; 605:	learn: 0.0111138	total: 2.19s	remaining: 1.42s
#&gt; 606:	learn: 0.0111138	total: 2.19s	remaining: 1.42s
#&gt; 607:	learn: 0.0111138	total: 2.19s	remaining: 1.41s
#&gt; 608:	learn: 0.0110952	total: 2.19s	remaining: 1.41s
#&gt; 609:	learn: 0.0110952	total: 2.19s	remaining: 1.4s
#&gt; 610:	learn: 0.0110828	total: 2.2s	remaining: 1.4s
#&gt; 611:	learn: 0.0110708	total: 2.21s	remaining: 1.4s
#&gt; 612:	learn: 0.0110708	total: 2.21s	remaining: 1.39s
#&gt; 613:	learn: 0.0110702	total: 2.22s	remaining: 1.39s
#&gt; 614:	learn: 0.0110350	total: 2.22s	remaining: 1.39s
#&gt; 615:	learn: 0.0110350	total: 2.22s	remaining: 1.39s
#&gt; 616:	learn: 0.0110205	total: 2.23s	remaining: 1.38s
#&gt; 617:	learn: 0.0110205	total: 2.23s	remaining: 1.38s
#&gt; 618:	learn: 0.0110189	total: 2.23s	remaining: 1.37s
#&gt; 619:	learn: 0.0110189	total: 2.23s	remaining: 1.37s
#&gt; 620:	learn: 0.0109572	total: 2.23s	remaining: 1.36s
#&gt; 621:	learn: 0.0109551	total: 2.24s	remaining: 1.36s
#&gt; 622:	learn: 0.0109527	total: 2.25s	remaining: 1.36s
#&gt; 623:	learn: 0.0109527	total: 2.25s	remaining: 1.35s
#&gt; 624:	learn: 0.0109515	total: 2.25s	remaining: 1.35s
#&gt; 625:	learn: 0.0109460	total: 2.25s	remaining: 1.35s
#&gt; 626:	learn: 0.0109460	total: 2.25s	remaining: 1.34s
#&gt; 627:	learn: 0.0108927	total: 2.27s	remaining: 1.34s
#&gt; 628:	learn: 0.0108927	total: 2.27s	remaining: 1.34s
#&gt; 629:	learn: 0.0108927	total: 2.27s	remaining: 1.33s
#&gt; 630:	learn: 0.0108913	total: 2.27s	remaining: 1.33s
#&gt; 631:	learn: 0.0108913	total: 2.27s	remaining: 1.32s
#&gt; 632:	learn: 0.0108912	total: 2.27s	remaining: 1.32s
#&gt; 633:	learn: 0.0108662	total: 2.28s	remaining: 1.31s
#&gt; 634:	learn: 0.0108662	total: 2.28s	remaining: 1.31s
#&gt; 635:	learn: 0.0108662	total: 2.28s	remaining: 1.3s
#&gt; 636:	learn: 0.0108662	total: 2.28s	remaining: 1.3s
#&gt; 637:	learn: 0.0108662	total: 2.28s	remaining: 1.29s
#&gt; 638:	learn: 0.0108267	total: 2.28s	remaining: 1.29s
#&gt; 639:	learn: 0.0108267	total: 2.29s	remaining: 1.28s
#&gt; 640:	learn: 0.0108267	total: 2.29s	remaining: 1.28s
#&gt; 641:	learn: 0.0108071	total: 2.29s	remaining: 1.28s
#&gt; 642:	learn: 0.0108043	total: 2.29s	remaining: 1.27s
#&gt; 643:	learn: 0.0108043	total: 2.29s	remaining: 1.27s
#&gt; 644:	learn: 0.0108033	total: 2.29s	remaining: 1.26s
#&gt; 645:	learn: 0.0108033	total: 2.3s	remaining: 1.26s
#&gt; 646:	learn: 0.0108021	total: 2.3s	remaining: 1.25s
#&gt; 647:	learn: 0.0108021	total: 2.3s	remaining: 1.25s
#&gt; 648:	learn: 0.0107966	total: 2.3s	remaining: 1.25s
#&gt; 649:	learn: 0.0107966	total: 2.31s	remaining: 1.24s
#&gt; 650:	learn: 0.0107964	total: 2.31s	remaining: 1.24s
#&gt; 651:	learn: 0.0107953	total: 2.31s	remaining: 1.23s
#&gt; 652:	learn: 0.0107913	total: 2.32s	remaining: 1.23s
#&gt; 653:	learn: 0.0107913	total: 2.32s	remaining: 1.23s
#&gt; 654:	learn: 0.0107900	total: 2.32s	remaining: 1.22s
#&gt; 655:	learn: 0.0107893	total: 2.33s	remaining: 1.22s
#&gt; 656:	learn: 0.0107893	total: 2.33s	remaining: 1.22s
#&gt; 657:	learn: 0.0107893	total: 2.33s	remaining: 1.21s
#&gt; 658:	learn: 0.0107873	total: 2.34s	remaining: 1.21s
#&gt; 659:	learn: 0.0107873	total: 2.34s	remaining: 1.2s
#&gt; 660:	learn: 0.0107865	total: 2.34s	remaining: 1.2s
#&gt; 661:	learn: 0.0107865	total: 2.34s	remaining: 1.19s
#&gt; 662:	learn: 0.0107693	total: 2.34s	remaining: 1.19s
#&gt; 663:	learn: 0.0107680	total: 2.34s	remaining: 1.19s
#&gt; 664:	learn: 0.0107680	total: 2.34s	remaining: 1.18s
#&gt; 665:	learn: 0.0107668	total: 2.36s	remaining: 1.18s
#&gt; 666:	learn: 0.0107217	total: 2.38s	remaining: 1.19s
#&gt; 667:	learn: 0.0107217	total: 2.38s	remaining: 1.18s
#&gt; 668:	learn: 0.0107217	total: 2.38s	remaining: 1.18s
#&gt; 669:	learn: 0.0107211	total: 2.39s	remaining: 1.18s
#&gt; 670:	learn: 0.0107187	total: 2.4s	remaining: 1.18s
#&gt; 671:	learn: 0.0107176	total: 2.41s	remaining: 1.17s
#&gt; 672:	learn: 0.0107176	total: 2.41s	remaining: 1.17s
#&gt; 673:	learn: 0.0107176	total: 2.41s	remaining: 1.16s
#&gt; 674:	learn: 0.0107176	total: 2.41s	remaining: 1.16s
#&gt; 675:	learn: 0.0107176	total: 2.41s	remaining: 1.15s
#&gt; 676:	learn: 0.0107176	total: 2.41s	remaining: 1.15s
#&gt; 677:	learn: 0.0106968	total: 2.42s	remaining: 1.15s
#&gt; 678:	learn: 0.0106968	total: 2.42s	remaining: 1.15s
#&gt; 679:	learn: 0.0106968	total: 2.43s	remaining: 1.14s
#&gt; 680:	learn: 0.0106968	total: 2.43s	remaining: 1.14s
#&gt; 681:	learn: 0.0106966	total: 2.43s	remaining: 1.13s
#&gt; 682:	learn: 0.0106909	total: 2.43s	remaining: 1.13s
#&gt; 683:	learn: 0.0106909	total: 2.43s	remaining: 1.12s
#&gt; 684:	learn: 0.0106482	total: 2.44s	remaining: 1.12s
#&gt; 685:	learn: 0.0106444	total: 2.45s	remaining: 1.12s
#&gt; 686:	learn: 0.0106442	total: 2.45s	remaining: 1.12s
#&gt; 687:	learn: 0.0106442	total: 2.45s	remaining: 1.11s
#&gt; 688:	learn: 0.0106442	total: 2.45s	remaining: 1.11s
#&gt; 689:	learn: 0.0106440	total: 2.46s	remaining: 1.11s
#&gt; 690:	learn: 0.0106440	total: 2.47s	remaining: 1.1s
#&gt; 691:	learn: 0.0106431	total: 2.47s	remaining: 1.1s
#&gt; 692:	learn: 0.0106278	total: 2.47s	remaining: 1.09s
#&gt; 693:	learn: 0.0106131	total: 2.48s	remaining: 1.09s
#&gt; 694:	learn: 0.0105889	total: 2.48s	remaining: 1.09s
#&gt; 695:	learn: 0.0105889	total: 2.48s	remaining: 1.08s
#&gt; 696:	learn: 0.0105889	total: 2.48s	remaining: 1.08s
#&gt; 697:	learn: 0.0105889	total: 2.48s	remaining: 1.07s
#&gt; 698:	learn: 0.0105876	total: 2.49s	remaining: 1.07s
#&gt; 699:	learn: 0.0105876	total: 2.49s	remaining: 1.07s
#&gt; 700:	learn: 0.0105418	total: 2.5s	remaining: 1.06s
#&gt; 701:	learn: 0.0105407	total: 2.5s	remaining: 1.06s
#&gt; 702:	learn: 0.0105365	total: 2.5s	remaining: 1.06s
#&gt; 703:	learn: 0.0105280	total: 2.5s	remaining: 1.05s
#&gt; 704:	learn: 0.0105280	total: 2.5s	remaining: 1.05s
#&gt; 705:	learn: 0.0105280	total: 2.5s	remaining: 1.04s
#&gt; 706:	learn: 0.0105280	total: 2.5s	remaining: 1.04s
#&gt; 707:	learn: 0.0105280	total: 2.5s	remaining: 1.03s
#&gt; 708:	learn: 0.0105280	total: 2.5s	remaining: 1.03s
#&gt; 709:	learn: 0.0105280	total: 2.5s	remaining: 1.02s
#&gt; 710:	learn: 0.0105280	total: 2.5s	remaining: 1.02s
#&gt; 711:	learn: 0.0105200	total: 2.5s	remaining: 1.01s
#&gt; 712:	learn: 0.0105192	total: 2.5s	remaining: 1.01s
#&gt; 713:	learn: 0.0104917	total: 2.52s	remaining: 1.01s
#&gt; 714:	learn: 0.0104917	total: 2.52s	remaining: 1s
#&gt; 715:	learn: 0.0104912	total: 2.52s	remaining: 999ms
#&gt; 716:	learn: 0.0104912	total: 2.52s	remaining: 995ms
#&gt; 717:	learn: 0.0104853	total: 2.52s	remaining: 991ms
#&gt; 718:	learn: 0.0104818	total: 2.53s	remaining: 988ms
#&gt; 719:	learn: 0.0104818	total: 2.53s	remaining: 983ms
#&gt; 720:	learn: 0.0104818	total: 2.53s	remaining: 978ms
#&gt; 721:	learn: 0.0104807	total: 2.53s	remaining: 973ms
#&gt; 722:	learn: 0.0104807	total: 2.53s	remaining: 968ms
#&gt; 723:	learn: 0.0104790	total: 2.53s	remaining: 965ms
#&gt; 724:	learn: 0.0104666	total: 2.53s	remaining: 961ms
#&gt; 725:	learn: 0.0104666	total: 2.53s	remaining: 956ms
#&gt; 726:	learn: 0.0104666	total: 2.54s	remaining: 952ms
#&gt; 727:	learn: 0.0104666	total: 2.54s	remaining: 947ms
#&gt; 728:	learn: 0.0104666	total: 2.54s	remaining: 943ms
#&gt; 729:	learn: 0.0104666	total: 2.54s	remaining: 938ms
#&gt; 730:	learn: 0.0104634	total: 2.54s	remaining: 936ms
#&gt; 731:	learn: 0.0104634	total: 2.54s	remaining: 931ms
#&gt; 732:	learn: 0.0104634	total: 2.54s	remaining: 926ms
#&gt; 733:	learn: 0.0104574	total: 2.55s	remaining: 924ms
#&gt; 734:	learn: 0.0104384	total: 2.55s	remaining: 919ms
#&gt; 735:	learn: 0.0104384	total: 2.55s	remaining: 915ms
#&gt; 736:	learn: 0.0104384	total: 2.55s	remaining: 910ms
#&gt; 737:	learn: 0.0103831	total: 2.56s	remaining: 908ms
#&gt; 738:	learn: 0.0103816	total: 2.56s	remaining: 903ms
#&gt; 739:	learn: 0.0103816	total: 2.56s	remaining: 899ms
#&gt; 740:	learn: 0.0103816	total: 2.56s	remaining: 894ms
#&gt; 741:	learn: 0.0103724	total: 2.57s	remaining: 893ms
#&gt; 742:	learn: 0.0103724	total: 2.57s	remaining: 889ms
#&gt; 743:	learn: 0.0103724	total: 2.57s	remaining: 884ms
#&gt; 744:	learn: 0.0103724	total: 2.57s	remaining: 879ms
#&gt; 745:	learn: 0.0103724	total: 2.57s	remaining: 875ms
#&gt; 746:	learn: 0.0103724	total: 2.57s	remaining: 870ms
#&gt; 747:	learn: 0.0103381	total: 2.58s	remaining: 868ms
#&gt; 748:	learn: 0.0103381	total: 2.58s	remaining: 863ms
#&gt; 749:	learn: 0.0103381	total: 2.58s	remaining: 859ms
#&gt; 750:	learn: 0.0103058	total: 2.58s	remaining: 856ms
#&gt; 751:	learn: 0.0103058	total: 2.58s	remaining: 851ms
#&gt; 752:	learn: 0.0103058	total: 2.58s	remaining: 847ms
#&gt; 753:	learn: 0.0102953	total: 2.59s	remaining: 844ms
#&gt; 754:	learn: 0.0102953	total: 2.59s	remaining: 839ms
#&gt; 755:	learn: 0.0102953	total: 2.59s	remaining: 835ms
#&gt; 756:	learn: 0.0102946	total: 2.6s	remaining: 833ms
#&gt; 757:	learn: 0.0102946	total: 2.6s	remaining: 829ms
#&gt; 758:	learn: 0.0102946	total: 2.6s	remaining: 825ms
#&gt; 759:	learn: 0.0102941	total: 2.6s	remaining: 821ms
#&gt; 760:	learn: 0.0102842	total: 2.61s	remaining: 821ms
#&gt; 761:	learn: 0.0102207	total: 2.68s	remaining: 837ms
#&gt; 762:	learn: 0.0102164	total: 2.69s	remaining: 836ms
#&gt; 763:	learn: 0.0102154	total: 2.69s	remaining: 832ms
#&gt; 764:	learn: 0.0102006	total: 2.7s	remaining: 830ms
#&gt; 765:	learn: 0.0102006	total: 2.7s	remaining: 825ms
#&gt; 766:	learn: 0.0101998	total: 2.71s	remaining: 822ms
#&gt; 767:	learn: 0.0101998	total: 2.71s	remaining: 817ms
#&gt; 768:	learn: 0.0101990	total: 2.71s	remaining: 814ms
#&gt; 769:	learn: 0.0101990	total: 2.71s	remaining: 810ms
#&gt; 770:	learn: 0.0101925	total: 2.71s	remaining: 806ms
#&gt; 771:	learn: 0.0101925	total: 2.71s	remaining: 802ms
#&gt; 772:	learn: 0.0101924	total: 2.72s	remaining: 799ms
#&gt; 773:	learn: 0.0101924	total: 2.72s	remaining: 794ms
#&gt; 774:	learn: 0.0101924	total: 2.72s	remaining: 790ms
#&gt; 775:	learn: 0.0101924	total: 2.72s	remaining: 785ms
#&gt; 776:	learn: 0.0101924	total: 2.72s	remaining: 781ms
#&gt; 777:	learn: 0.0101814	total: 2.74s	remaining: 782ms
#&gt; 778:	learn: 0.0101813	total: 2.74s	remaining: 778ms
#&gt; 779:	learn: 0.0101719	total: 2.75s	remaining: 775ms
#&gt; 780:	learn: 0.0101542	total: 2.75s	remaining: 772ms
#&gt; 781:	learn: 0.0101531	total: 2.75s	remaining: 768ms
#&gt; 782:	learn: 0.0101531	total: 2.75s	remaining: 763ms
#&gt; 783:	learn: 0.0101531	total: 2.75s	remaining: 759ms
#&gt; 784:	learn: 0.0101489	total: 2.76s	remaining: 756ms
#&gt; 785:	learn: 0.0101489	total: 2.76s	remaining: 752ms
#&gt; 786:	learn: 0.0101465	total: 2.77s	remaining: 748ms
#&gt; 787:	learn: 0.0101465	total: 2.77s	remaining: 744ms
#&gt; 788:	learn: 0.0101339	total: 2.77s	remaining: 741ms
#&gt; 789:	learn: 0.0101325	total: 2.77s	remaining: 737ms
#&gt; 790:	learn: 0.0101306	total: 2.78s	remaining: 734ms
#&gt; 791:	learn: 0.0101306	total: 2.78s	remaining: 729ms
#&gt; 792:	learn: 0.0101172	total: 2.81s	remaining: 733ms
#&gt; 793:	learn: 0.0101172	total: 2.81s	remaining: 729ms
#&gt; 794:	learn: 0.0101172	total: 2.81s	remaining: 725ms
#&gt; 795:	learn: 0.0101172	total: 2.81s	remaining: 720ms
#&gt; 796:	learn: 0.0100989	total: 2.82s	remaining: 718ms
#&gt; 797:	learn: 0.0100374	total: 2.83s	remaining: 716ms
#&gt; 798:	learn: 0.0100369	total: 2.83s	remaining: 712ms
#&gt; 799:	learn: 0.0100369	total: 2.83s	remaining: 708ms
#&gt; 800:	learn: 0.0100369	total: 2.84s	remaining: 706ms
#&gt; 801:	learn: 0.0100369	total: 2.84s	remaining: 702ms
#&gt; 802:	learn: 0.0100369	total: 2.84s	remaining: 697ms
#&gt; 803:	learn: 0.0100369	total: 2.84s	remaining: 693ms
#&gt; 804:	learn: 0.0100363	total: 2.85s	remaining: 689ms
#&gt; 805:	learn: 0.0100363	total: 2.85s	remaining: 685ms
#&gt; 806:	learn: 0.0100363	total: 2.85s	remaining: 681ms
#&gt; 807:	learn: 0.0100363	total: 2.85s	remaining: 677ms
#&gt; 808:	learn: 0.0100293	total: 2.85s	remaining: 672ms
#&gt; 809:	learn: 0.0100293	total: 2.85s	remaining: 668ms
#&gt; 810:	learn: 0.0100293	total: 2.85s	remaining: 664ms
#&gt; 811:	learn: 0.0100293	total: 2.85s	remaining: 660ms
#&gt; 812:	learn: 0.0100293	total: 2.86s	remaining: 657ms
#&gt; 813:	learn: 0.0100293	total: 2.86s	remaining: 653ms
#&gt; 814:	learn: 0.0100291	total: 2.86s	remaining: 649ms
#&gt; 815:	learn: 0.0100279	total: 2.86s	remaining: 645ms
#&gt; 816:	learn: 0.0100267	total: 2.86s	remaining: 641ms
#&gt; 817:	learn: 0.0099665	total: 2.87s	remaining: 638ms
#&gt; 818:	learn: 0.0099489	total: 2.87s	remaining: 635ms
#&gt; 819:	learn: 0.0099486	total: 2.87s	remaining: 631ms
#&gt; 820:	learn: 0.0099486	total: 2.87s	remaining: 627ms
#&gt; 821:	learn: 0.0099486	total: 2.88s	remaining: 623ms
#&gt; 822:	learn: 0.0099486	total: 2.88s	remaining: 618ms
#&gt; 823:	learn: 0.0099384	total: 2.88s	remaining: 616ms
#&gt; 824:	learn: 0.0099006	total: 2.89s	remaining: 614ms
#&gt; 825:	learn: 0.0098975	total: 2.89s	remaining: 610ms
#&gt; 826:	learn: 0.0098913	total: 2.9s	remaining: 607ms
#&gt; 827:	learn: 0.0098824	total: 2.91s	remaining: 604ms
#&gt; 828:	learn: 0.0098820	total: 2.91s	remaining: 601ms
#&gt; 829:	learn: 0.0098820	total: 2.91s	remaining: 596ms
#&gt; 830:	learn: 0.0098820	total: 2.91s	remaining: 592ms
#&gt; 831:	learn: 0.0098810	total: 2.91s	remaining: 589ms
#&gt; 832:	learn: 0.0098810	total: 2.91s	remaining: 584ms
#&gt; 833:	learn: 0.0098809	total: 2.92s	remaining: 581ms
#&gt; 834:	learn: 0.0098809	total: 2.92s	remaining: 577ms
#&gt; 835:	learn: 0.0098809	total: 2.92s	remaining: 572ms
#&gt; 836:	learn: 0.0098809	total: 2.92s	remaining: 568ms
#&gt; 837:	learn: 0.0098809	total: 2.92s	remaining: 564ms
#&gt; 838:	learn: 0.0098809	total: 2.92s	remaining: 560ms
#&gt; 839:	learn: 0.0098760	total: 2.92s	remaining: 557ms
#&gt; 840:	learn: 0.0098760	total: 2.92s	remaining: 553ms
#&gt; 841:	learn: 0.0098759	total: 2.92s	remaining: 549ms
#&gt; 842:	learn: 0.0098759	total: 2.92s	remaining: 544ms
#&gt; 843:	learn: 0.0098759	total: 2.92s	remaining: 540ms
#&gt; 844:	learn: 0.0098678	total: 2.93s	remaining: 537ms
#&gt; 845:	learn: 0.0098678	total: 2.93s	remaining: 533ms
#&gt; 846:	learn: 0.0098668	total: 2.93s	remaining: 530ms
#&gt; 847:	learn: 0.0098641	total: 2.94s	remaining: 526ms
#&gt; 848:	learn: 0.0098641	total: 2.94s	remaining: 522ms
#&gt; 849:	learn: 0.0098641	total: 2.94s	remaining: 518ms
#&gt; 850:	learn: 0.0098641	total: 2.94s	remaining: 514ms
#&gt; 851:	learn: 0.0098641	total: 2.94s	remaining: 510ms
#&gt; 852:	learn: 0.0098641	total: 2.94s	remaining: 506ms
#&gt; 853:	learn: 0.0098640	total: 2.94s	remaining: 503ms
#&gt; 854:	learn: 0.0098640	total: 2.94s	remaining: 499ms
#&gt; 855:	learn: 0.0098340	total: 2.94s	remaining: 496ms
#&gt; 856:	learn: 0.0098335	total: 2.95s	remaining: 492ms
#&gt; 857:	learn: 0.0098200	total: 2.95s	remaining: 488ms
#&gt; 858:	learn: 0.0098111	total: 2.95s	remaining: 484ms
#&gt; 859:	learn: 0.0098110	total: 2.96s	remaining: 482ms
#&gt; 860:	learn: 0.0097900	total: 2.96s	remaining: 478ms
#&gt; 861:	learn: 0.0097894	total: 2.96s	remaining: 474ms
#&gt; 862:	learn: 0.0097888	total: 2.96s	remaining: 470ms
#&gt; 863:	learn: 0.0097886	total: 2.96s	remaining: 467ms
#&gt; 864:	learn: 0.0097608	total: 2.97s	remaining: 464ms
#&gt; 865:	learn: 0.0097598	total: 2.98s	remaining: 461ms
#&gt; 866:	learn: 0.0097266	total: 2.99s	remaining: 458ms
#&gt; 867:	learn: 0.0097224	total: 2.99s	remaining: 455ms
#&gt; 868:	learn: 0.0097224	total: 2.99s	remaining: 451ms
#&gt; 869:	learn: 0.0096614	total: 3s	remaining: 448ms
#&gt; 870:	learn: 0.0096614	total: 3s	remaining: 445ms
#&gt; 871:	learn: 0.0096601	total: 3s	remaining: 441ms
#&gt; 872:	learn: 0.0096301	total: 3.01s	remaining: 438ms
#&gt; 873:	learn: 0.0096301	total: 3.01s	remaining: 434ms
#&gt; 874:	learn: 0.0096288	total: 3.01s	remaining: 430ms
#&gt; 875:	learn: 0.0096288	total: 3.01s	remaining: 427ms
#&gt; 876:	learn: 0.0096282	total: 3.02s	remaining: 423ms
#&gt; 877:	learn: 0.0096279	total: 3.02s	remaining: 419ms
#&gt; 878:	learn: 0.0096279	total: 3.02s	remaining: 415ms
#&gt; 879:	learn: 0.0096270	total: 3.02s	remaining: 412ms
#&gt; 880:	learn: 0.0096270	total: 3.02s	remaining: 408ms
#&gt; 881:	learn: 0.0096270	total: 3.02s	remaining: 404ms
#&gt; 882:	learn: 0.0096270	total: 3.02s	remaining: 401ms
#&gt; 883:	learn: 0.0096021	total: 3.04s	remaining: 399ms
#&gt; 884:	learn: 0.0095917	total: 3.05s	remaining: 396ms
#&gt; 885:	learn: 0.0095915	total: 3.06s	remaining: 394ms
#&gt; 886:	learn: 0.0095833	total: 3.07s	remaining: 391ms
#&gt; 887:	learn: 0.0095774	total: 3.07s	remaining: 388ms
#&gt; 888:	learn: 0.0095774	total: 3.07s	remaining: 384ms
#&gt; 889:	learn: 0.0095774	total: 3.08s	remaining: 380ms
#&gt; 890:	learn: 0.0095761	total: 3.08s	remaining: 376ms
#&gt; 891:	learn: 0.0095761	total: 3.08s	remaining: 373ms
#&gt; 892:	learn: 0.0095761	total: 3.08s	remaining: 369ms
#&gt; 893:	learn: 0.0095705	total: 3.08s	remaining: 365ms
#&gt; 894:	learn: 0.0095652	total: 3.08s	remaining: 362ms
#&gt; 895:	learn: 0.0095652	total: 3.08s	remaining: 358ms
#&gt; 896:	learn: 0.0095490	total: 3.09s	remaining: 355ms
#&gt; 897:	learn: 0.0095480	total: 3.09s	remaining: 351ms
#&gt; 898:	learn: 0.0095479	total: 3.1s	remaining: 348ms
#&gt; 899:	learn: 0.0095038	total: 3.1s	remaining: 345ms
#&gt; 900:	learn: 0.0094762	total: 3.11s	remaining: 341ms
#&gt; 901:	learn: 0.0094762	total: 3.11s	remaining: 338ms
#&gt; 902:	learn: 0.0094580	total: 3.12s	remaining: 335ms
#&gt; 903:	learn: 0.0094297	total: 3.12s	remaining: 331ms
#&gt; 904:	learn: 0.0094296	total: 3.12s	remaining: 328ms
#&gt; 905:	learn: 0.0094296	total: 3.12s	remaining: 324ms
#&gt; 906:	learn: 0.0093994	total: 3.13s	remaining: 321ms
#&gt; 907:	learn: 0.0093994	total: 3.13s	remaining: 317ms
#&gt; 908:	learn: 0.0093986	total: 3.13s	remaining: 313ms
#&gt; 909:	learn: 0.0093954	total: 3.13s	remaining: 310ms
#&gt; 910:	learn: 0.0093578	total: 3.13s	remaining: 306ms
#&gt; 911:	learn: 0.0093568	total: 3.13s	remaining: 303ms
#&gt; 912:	learn: 0.0093544	total: 3.14s	remaining: 299ms
#&gt; 913:	learn: 0.0093544	total: 3.14s	remaining: 296ms
#&gt; 914:	learn: 0.0093544	total: 3.14s	remaining: 292ms
#&gt; 915:	learn: 0.0093544	total: 3.14s	remaining: 288ms
#&gt; 916:	learn: 0.0093544	total: 3.14s	remaining: 285ms
#&gt; 917:	learn: 0.0093544	total: 3.14s	remaining: 281ms
#&gt; 918:	learn: 0.0093518	total: 3.15s	remaining: 277ms
#&gt; 919:	learn: 0.0093518	total: 3.15s	remaining: 274ms
#&gt; 920:	learn: 0.0093518	total: 3.15s	remaining: 270ms
#&gt; 921:	learn: 0.0093361	total: 3.17s	remaining: 268ms
#&gt; 922:	learn: 0.0093325	total: 3.17s	remaining: 265ms
#&gt; 923:	learn: 0.0093325	total: 3.17s	remaining: 261ms
#&gt; 924:	learn: 0.0093325	total: 3.17s	remaining: 257ms
#&gt; 925:	learn: 0.0093323	total: 3.17s	remaining: 254ms
#&gt; 926:	learn: 0.0093321	total: 3.18s	remaining: 250ms
#&gt; 927:	learn: 0.0093298	total: 3.18s	remaining: 247ms
#&gt; 928:	learn: 0.0093298	total: 3.18s	remaining: 243ms
#&gt; 929:	learn: 0.0093140	total: 3.18s	remaining: 239ms
#&gt; 930:	learn: 0.0093140	total: 3.18s	remaining: 236ms
#&gt; 931:	learn: 0.0093140	total: 3.18s	remaining: 232ms
#&gt; 932:	learn: 0.0093140	total: 3.18s	remaining: 229ms
#&gt; 933:	learn: 0.0093139	total: 3.19s	remaining: 225ms
#&gt; 934:	learn: 0.0093139	total: 3.19s	remaining: 221ms
#&gt; 935:	learn: 0.0093139	total: 3.19s	remaining: 218ms
#&gt; 936:	learn: 0.0093139	total: 3.19s	remaining: 214ms
#&gt; 937:	learn: 0.0093131	total: 3.19s	remaining: 211ms
#&gt; 938:	learn: 0.0093131	total: 3.19s	remaining: 207ms
#&gt; 939:	learn: 0.0093131	total: 3.19s	remaining: 204ms
#&gt; 940:	learn: 0.0093131	total: 3.19s	remaining: 200ms
#&gt; 941:	learn: 0.0093129	total: 3.19s	remaining: 197ms
#&gt; 942:	learn: 0.0093129	total: 3.19s	remaining: 193ms
#&gt; 943:	learn: 0.0093099	total: 3.2s	remaining: 190ms
#&gt; 944:	learn: 0.0093099	total: 3.2s	remaining: 186ms
#&gt; 945:	learn: 0.0093041	total: 3.2s	remaining: 183ms
#&gt; 946:	learn: 0.0093041	total: 3.2s	remaining: 179ms
#&gt; 947:	learn: 0.0092980	total: 3.2s	remaining: 176ms
#&gt; 948:	learn: 0.0092923	total: 3.21s	remaining: 172ms
#&gt; 949:	learn: 0.0092899	total: 3.22s	remaining: 169ms
#&gt; 950:	learn: 0.0092898	total: 3.22s	remaining: 166ms
#&gt; 951:	learn: 0.0092898	total: 3.22s	remaining: 162ms
#&gt; 952:	learn: 0.0092755	total: 3.23s	remaining: 159ms
#&gt; 953:	learn: 0.0092562	total: 3.26s	remaining: 157ms
#&gt; 954:	learn: 0.0092474	total: 3.27s	remaining: 154ms
#&gt; 955:	learn: 0.0092453	total: 3.27s	remaining: 150ms
#&gt; 956:	learn: 0.0092453	total: 3.27s	remaining: 147ms
#&gt; 957:	learn: 0.0092385	total: 3.27s	remaining: 143ms
#&gt; 958:	learn: 0.0092385	total: 3.27s	remaining: 140ms
#&gt; 959:	learn: 0.0092382	total: 3.27s	remaining: 136ms
#&gt; 960:	learn: 0.0092199	total: 3.28s	remaining: 133ms
#&gt; 961:	learn: 0.0092199	total: 3.28s	remaining: 130ms
#&gt; 962:	learn: 0.0092198	total: 3.28s	remaining: 126ms
#&gt; 963:	learn: 0.0092191	total: 3.28s	remaining: 123ms
#&gt; 964:	learn: 0.0092191	total: 3.28s	remaining: 119ms
#&gt; 965:	learn: 0.0092187	total: 3.28s	remaining: 116ms
#&gt; 966:	learn: 0.0092187	total: 3.28s	remaining: 112ms
#&gt; 967:	learn: 0.0092187	total: 3.28s	remaining: 109ms
#&gt; 968:	learn: 0.0092187	total: 3.29s	remaining: 105ms
#&gt; 969:	learn: 0.0092177	total: 3.29s	remaining: 102ms
#&gt; 970:	learn: 0.0092177	total: 3.29s	remaining: 98.1ms
#&gt; 971:	learn: 0.0092177	total: 3.29s	remaining: 94.7ms
#&gt; 972:	learn: 0.0092043	total: 3.3s	remaining: 91.6ms
#&gt; 973:	learn: 0.0091935	total: 3.31s	remaining: 88.2ms
#&gt; 974:	learn: 0.0091890	total: 3.31s	remaining: 84.9ms
#&gt; 975:	learn: 0.0091889	total: 3.31s	remaining: 81.5ms
#&gt; 976:	learn: 0.0091854	total: 3.32s	remaining: 78.1ms
#&gt; 977:	learn: 0.0091854	total: 3.32s	remaining: 74.7ms
#&gt; 978:	learn: 0.0091851	total: 3.32s	remaining: 71.2ms
#&gt; 979:	learn: 0.0091851	total: 3.32s	remaining: 67.8ms
#&gt; 980:	learn: 0.0091851	total: 3.32s	remaining: 64.3ms
#&gt; 981:	learn: 0.0091824	total: 3.32s	remaining: 60.9ms
#&gt; 982:	learn: 0.0091824	total: 3.32s	remaining: 57.5ms
#&gt; 983:	learn: 0.0091824	total: 3.32s	remaining: 54.1ms
#&gt; 984:	learn: 0.0091824	total: 3.32s	remaining: 50.6ms
#&gt; 985:	learn: 0.0091822	total: 3.33s	remaining: 47.2ms
#&gt; 986:	learn: 0.0091604	total: 3.33s	remaining: 43.9ms
#&gt; 987:	learn: 0.0091604	total: 3.33s	remaining: 40.5ms
#&gt; 988:	learn: 0.0091604	total: 3.33s	remaining: 37.1ms
#&gt; 989:	learn: 0.0091603	total: 3.34s	remaining: 33.7ms
#&gt; 990:	learn: 0.0090928	total: 3.35s	remaining: 30.4ms
#&gt; 991:	learn: 0.0090793	total: 3.35s	remaining: 27ms
#&gt; 992:	learn: 0.0090793	total: 3.35s	remaining: 23.6ms
#&gt; 993:	learn: 0.0090793	total: 3.35s	remaining: 20.2ms
#&gt; 994:	learn: 0.0090793	total: 3.35s	remaining: 16.8ms
#&gt; 995:	learn: 0.0090793	total: 3.35s	remaining: 13.5ms
#&gt; 996:	learn: 0.0090760	total: 3.37s	remaining: 10.1ms
#&gt; 997:	learn: 0.0090564	total: 3.37s	remaining: 6.75ms
#&gt; 998:	learn: 0.0090562	total: 3.37s	remaining: 3.37ms
#&gt; 999:	learn: 0.0090562	total: 3.37s	remaining: 0us</div><div class='input'><span class='va'>model_fit</span>
</div><div class='output co'>#&gt; parsnip model object
#&gt; 
#&gt; Fit time:  5.3s 
#&gt; Prophet Model w/ Catboost Error Specification
#&gt; ---
#&gt; Model 1: PROPHET
#&gt; $growth
#&gt; [1] "linear"
#&gt; 
#&gt; $changepoints
#&gt;  [1] "1990-09-01 GMT" "1991-05-01 GMT" "1991-12-01 GMT" "1992-08-01 GMT"
#&gt;  [5] "1993-04-01 GMT" "1993-12-01 GMT" "1994-07-01 GMT" "1995-03-01 GMT"
#&gt;  [9] "1995-11-01 GMT" "1996-07-01 GMT" "1997-02-01 GMT" "1997-10-01 GMT"
#&gt; [13] "1998-06-01 GMT" "1999-02-01 GMT" "1999-09-01 GMT" "2000-05-01 GMT"
#&gt; [17] "2001-01-01 GMT" "2001-09-01 GMT" "2002-04-01 GMT" "2002-12-01 GMT"
#&gt; [21] "2003-08-01 GMT" "2004-04-01 GMT" "2004-11-01 GMT" "2005-07-01 GMT"
#&gt; [25] "2006-03-01 GMT"
#&gt; 
#&gt; $n.changepoints
#&gt; [1] 25
#&gt; 
#&gt; $changepoint.range
#&gt; [1] 0.8
#&gt; 
#&gt; $yearly.seasonality
#&gt; [1] "auto"
#&gt; 
#&gt; $weekly.seasonality
#&gt; [1] "auto"
#&gt; 
#&gt; $daily.seasonality
#&gt; [1] "auto"
#&gt; 
#&gt; $holidays
#&gt; NULL
#&gt; 
#&gt; $seasonality.mode
#&gt; [1] "additive"
#&gt; 
#&gt; $seasonality.prior.scale
#&gt; [1] 10
#&gt; 
#&gt; $changepoint.prior.scale
#&gt; [1] 0.05
#&gt; 
#&gt; $holidays.prior.scale
#&gt; [1] 10
#&gt; 
#&gt; $mcmc.samples
#&gt; [1] 0
#&gt; 
#&gt; $interval.width
#&gt; [1] 0.8
#&gt; 
#&gt; $uncertainty.samples
#&gt; [1] 1000
#&gt; 
#&gt; $specified.changepoints
#&gt; [1] FALSE
#&gt; 
#&gt; $start
#&gt; [1] "1990-01-01 GMT"
#&gt; 
#&gt; $y.scale
#&gt; [1] 9.289152
#&gt; 
#&gt; $logistic.floor
#&gt; [1] FALSE
#&gt; 
#&gt; $t.scale
#&gt; [1] 638928000
#&gt; 
#&gt; $changepoints.t
#&gt;  [1] 0.03286004 0.06558485 0.09452333 0.12751859 0.16037863 0.19337390
#&gt;  [7] 0.22204192 0.25490196 0.28803245 0.32089249 0.34996619 0.38269101
#&gt; [13] 0.41555105 0.44868154 0.47734956 0.51020960 0.54334009 0.57620014
#&gt; [19] 0.60486815 0.63786342 0.67072346 0.70371873 0.73265720 0.76538201
#&gt; [25] 0.79824206
#&gt; 
#&gt; $seasonalities
#&gt; $seasonalities$yearly
#&gt; $seasonalities$yearly$period
#&gt; [1] 365.25
#&gt; 
#&gt; $seasonalities$yearly$fourier.order
#&gt; [1] 10
#&gt; 
#&gt; $seasonalities$yearly$prior.scale
#&gt; [1] 10
#&gt; 
#&gt; $seasonalities$yearly$mode
#&gt; [1] "additive"
#&gt; 
#&gt; $seasonalities$yearly$condition.name
#&gt; NULL
#&gt; 
#&gt; 
#&gt; 
#&gt; $extra_regressors
#&gt; list()
#&gt; 
#&gt; $country_holidays
#&gt; NULL
#&gt; 
#&gt; $stan.fit
#&gt; $stan.fit$par
#&gt; $stan.fit$par$k
#&gt; [1] 0.1382606
#&gt; 
#&gt; $stan.fit$par$m
#&gt; [1] 0.9414838
#&gt; 
#&gt; $stan.fit$par$delta
#&gt;  [1] -3.981145e-02 -3.461406e-02 -6.720849e-02 -1.708857e-02  6.159133e-02
#&gt;  [6]  6.287647e-03  4.777434e-03  2.978141e-02  3.109902e-02  1.286811e-07
#&gt; [11] -2.183630e-02 -7.459047e-07  4.688770e-02  2.626749e-08 -9.953301e-02
#&gt; [16] -5.723185e-07  1.325338e-01  1.703221e-02 -2.264345e-01 -1.090711e-07
#&gt; [21]  4.337023e-02  6.817898e-08 -1.201858e-02 -1.395609e-01  1.935643e-01
#&gt; 
#&gt; $stan.fit$par$sigma_obs
#&gt; [1] 0.00198912
#&gt; 
#&gt; $stan.fit$par$beta
#&gt;  [1]  0.0025594440  0.0048331897 -0.0029333266 -0.0054109202  0.0037265969
#&gt;  [6]  0.0035866751 -0.0040690340 -0.0022434447  0.0027617291  0.0019509049
#&gt; [11] -0.0044701719 -0.0002731660  0.0025341267 -0.0012136102 -0.0030890779
#&gt; [16]  0.0010835001  0.0024279372 -0.0021301133 -0.0003352578  0.0020623055
#&gt; 
#&gt; 
#&gt; $stan.fit$value
#&gt; [1] 1371.581
#&gt; 
#&gt; $stan.fit$return_code
#&gt; [1] 0
#&gt; 
#&gt; $stan.fit$theta_tilde
#&gt;              k         m    delta[1]    delta[2]    delta[3]    delta[4]
#&gt; [1,] 0.1382606 0.9414838 -0.03981145 -0.03461406 -0.06720849 -0.01708857
#&gt;        delta[5]    delta[6]    delta[7]   delta[8]   delta[9]    delta[10]
#&gt; [1,] 0.06159133 0.006287647 0.004777434 0.02978141 0.03109902 1.286811e-07
#&gt;       delta[11]     delta[12] delta[13]    delta[14]   delta[15]     delta[16]
#&gt; [1,] -0.0218363 -7.459047e-07 0.0468877 2.626749e-08 -0.09953301 -5.723185e-07
#&gt;      delta[17]  delta[18]  delta[19]     delta[20]  delta[21]    delta[22]
#&gt; [1,] 0.1325338 0.01703221 -0.2264345 -1.090711e-07 0.04337023 6.817898e-08
#&gt;        delta[23]  delta[24] delta[25]  sigma_obs     beta[1]    beta[2]
#&gt; [1,] -0.01201858 -0.1395609 0.1935643 0.00198912 0.002559444 0.00483319
#&gt;           beta[3]     beta[4]     beta[5]     beta[6]      beta[7]      beta[8]
#&gt; [1,] -0.002933327 -0.00541092 0.003726597 0.003586675 -0.004069034 -0.002243445
#&gt;          beta[9]    beta[10]     beta[11]     beta[12]    beta[13]    beta[14]
#&gt; [1,] 0.002761729 0.001950905 -0.004470172 -0.000273166 0.002534127 -0.00121361
#&gt;          beta[15]  beta[16]    beta[17]     beta[18]      beta[19]    beta[20]
#&gt; [1,] -0.003089078 0.0010835 0.002427937 -0.002130113 -0.0003352578 0.002062305
#&gt; 
#&gt; 
#&gt; $params
#&gt; $params$k
#&gt; [1] 0.1382606
#&gt; 
#&gt; $params$m
#&gt; [1] 0.9414838
#&gt; 
#&gt; $params$delta
#&gt;             [,1]        [,2]        [,3]        [,4]       [,5]        [,6]
#&gt; [1,] -0.03981145 -0.03461406 -0.06720849 -0.01708857 0.06159133 0.006287647
#&gt;             [,7]       [,8]       [,9]        [,10]      [,11]         [,12]
#&gt; [1,] 0.004777434 0.02978141 0.03109902 1.286811e-07 -0.0218363 -7.459047e-07
#&gt;          [,13]        [,14]       [,15]         [,16]     [,17]      [,18]
#&gt; [1,] 0.0468877 2.626749e-08 -0.09953301 -5.723185e-07 0.1325338 0.01703221
#&gt;           [,19]         [,20]      [,21]        [,22]       [,23]      [,24]
#&gt; [1,] -0.2264345 -1.090711e-07 0.04337023 6.817898e-08 -0.01201858 -0.1395609
#&gt;          [,25]
#&gt; [1,] 0.1935643
#&gt; 
#&gt; $params$sigma_obs
#&gt; [1] 0.00198912
#&gt; 
#&gt; $params$beta
#&gt;             [,1]       [,2]         [,3]        [,4]        [,5]        [,6]
#&gt; [1,] 0.002559444 0.00483319 -0.002933327 -0.00541092 0.003726597 0.003586675
#&gt;              [,7]         [,8]        [,9]       [,10]        [,11]
#&gt; [1,] -0.004069034 -0.002243445 0.002761729 0.001950905 -0.004470172
#&gt;             [,12]       [,13]       [,14]        [,15]     [,16]       [,17]
#&gt; [1,] -0.000273166 0.002534127 -0.00121361 -0.003089078 0.0010835 0.002427937
#&gt;             [,18]         [,19]       [,20]
#&gt; [1,] -0.002130113 -0.0003352578 0.002062305
#&gt; 
#&gt; 
#&gt; $history
#&gt; </span><span style='color: #949494;'># A tibble: 244 x 5</span><span>
#&gt;        y ds                  floor       t y_scaled
#&gt;    </span><span style='color: #949494;font-style: italic;'>&lt;dbl&gt;</span><span> </span><span style='color: #949494;font-style: italic;'>&lt;dttm&gt;</span><span>              </span><span style='color: #949494;font-style: italic;'>&lt;dbl&gt;</span><span>   </span><span style='color: #949494;font-style: italic;'>&lt;dbl&gt;</span><span>    </span><span style='color: #949494;font-style: italic;'>&lt;dbl&gt;</span><span>
#&gt; </span><span style='color: #BCBCBC;'> 1</span><span>  8.76 1990-01-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0          0.943
#&gt; </span><span style='color: #BCBCBC;'> 2</span><span>  8.77 1990-02-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.004</span><span style='text-decoration: underline;'>19</span><span>    0.944
#&gt; </span><span style='color: #BCBCBC;'> 3</span><span>  8.78 1990-03-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.007</span><span style='text-decoration: underline;'>98</span><span>    0.945
#&gt; </span><span style='color: #BCBCBC;'> 4</span><span>  8.79 1990-04-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.012</span><span style='text-decoration: underline;'>2</span><span>     0.946
#&gt; </span><span style='color: #BCBCBC;'> 5</span><span>  8.80 1990-05-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.016</span><span style='text-decoration: underline;'>2</span><span>     0.947
#&gt; </span><span style='color: #BCBCBC;'> 6</span><span>  8.81 1990-06-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.020</span><span style='text-decoration: underline;'>4</span><span>     0.948
#&gt; </span><span style='color: #BCBCBC;'> 7</span><span>  8.70 1990-07-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.024</span><span style='text-decoration: underline;'>5</span><span>     0.937
#&gt; </span><span style='color: #BCBCBC;'> 8</span><span>  8.60 1990-08-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.028</span><span style='text-decoration: underline;'>7</span><span>     0.926
#&gt; </span><span style='color: #BCBCBC;'> 9</span><span>  8.78 1990-09-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.032</span><span style='text-decoration: underline;'>9</span><span>     0.945
#&gt; </span><span style='color: #BCBCBC;'>10</span><span>  8.83 1990-10-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.036</span><span style='text-decoration: underline;'>9</span><span>     0.950
#&gt; </span><span style='color: #949494;'># ... with 234 more rows</span><span>
#&gt; 
#&gt; $history.dates
#&gt;   [1] "1990-01-01 GMT" "1990-02-01 GMT" "1990-03-01 GMT" "1990-04-01 GMT"
#&gt;   [5] "1990-05-01 GMT" "1990-06-01 GMT" "1990-07-01 GMT" "1990-08-01 GMT"
#&gt;   [9] "1990-09-01 GMT" "1990-10-01 GMT" "1990-11-01 GMT" "1990-12-01 GMT"
#&gt;  [13] "1991-01-01 GMT" "1991-02-01 GMT" "1991-03-01 GMT" "1991-04-01 GMT"
#&gt;  [17] "1991-05-01 GMT" "1991-06-01 GMT" "1991-07-01 GMT" "1991-08-01 GMT"
#&gt;  [21] "1991-09-01 GMT" "1991-10-01 GMT" "1991-11-01 GMT" "1991-12-01 GMT"
#&gt;  [25] "1992-01-01 GMT" "1992-02-01 GMT" "1992-03-01 GMT" "1992-04-01 GMT"
#&gt;  [29] "1992-05-01 GMT" "1992-06-01 GMT" "1992-07-01 GMT" "1992-08-01 GMT"
#&gt;  [33] "1992-09-01 GMT" "1992-10-01 GMT" "1992-11-01 GMT" "1992-12-01 GMT"
#&gt;  [37] "1993-01-01 GMT" "1993-02-01 GMT" "1993-03-01 GMT" "1993-04-01 GMT"
#&gt;  [41] "1993-05-01 GMT" "1993-06-01 GMT" "1993-07-01 GMT" "1993-08-01 GMT"
#&gt;  [45] "1993-09-01 GMT" "1993-10-01 GMT" "1993-11-01 GMT" "1993-12-01 GMT"
#&gt;  [49] "1994-01-01 GMT" "1994-02-01 GMT" "1994-03-01 GMT" "1994-04-01 GMT"
#&gt;  [53] "1994-05-01 GMT" "1994-06-01 GMT" "1994-07-01 GMT" "1994-08-01 GMT"
#&gt;  [57] "1994-09-01 GMT" "1994-10-01 GMT" "1994-11-01 GMT" "1994-12-01 GMT"
#&gt;  [61] "1995-01-01 GMT" "1995-02-01 GMT" "1995-03-01 GMT" "1995-04-01 GMT"
#&gt;  [65] "1995-05-01 GMT" "1995-06-01 GMT" "1995-07-01 GMT" "1995-08-01 GMT"
#&gt;  [69] "1995-09-01 GMT" "1995-10-01 GMT" "1995-11-01 GMT" "1995-12-01 GMT"
#&gt;  [73] "1996-01-01 GMT" "1996-02-01 GMT" "1996-03-01 GMT" "1996-04-01 GMT"
#&gt;  [77] "1996-05-01 GMT" "1996-06-01 GMT" "1996-07-01 GMT" "1996-08-01 GMT"
#&gt;  [81] "1996-09-01 GMT" "1996-10-01 GMT" "1996-11-01 GMT" "1996-12-01 GMT"
#&gt;  [85] "1997-01-01 GMT" "1997-02-01 GMT" "1997-03-01 GMT" "1997-04-01 GMT"
#&gt;  [89] "1997-05-01 GMT" "1997-06-01 GMT" "1997-07-01 GMT" "1997-08-01 GMT"
#&gt;  [93] "1997-09-01 GMT" "1997-10-01 GMT" "1997-11-01 GMT" "1997-12-01 GMT"
#&gt;  [97] "1998-01-01 GMT" "1998-02-01 GMT" "1998-03-01 GMT" "1998-04-01 GMT"
#&gt; [101] "1998-05-01 GMT" "1998-06-01 GMT" "1998-07-01 GMT" "1998-08-01 GMT"
#&gt; [105] "1998-09-01 GMT" "1998-10-01 GMT" "1998-11-01 GMT" "1998-12-01 GMT"
#&gt; [109] "1999-01-01 GMT" "1999-02-01 GMT" "1999-03-01 GMT" "1999-04-01 GMT"
#&gt; [113] "1999-05-01 GMT" "1999-06-01 GMT" "1999-07-01 GMT" "1999-08-01 GMT"
#&gt; [117] "1999-09-01 GMT" "1999-10-01 GMT" "1999-11-01 GMT" "1999-12-01 GMT"
#&gt; [121] "2000-01-01 GMT" "2000-02-01 GMT" "2000-03-01 GMT" "2000-04-01 GMT"
#&gt; [125] "2000-05-01 GMT" "2000-06-01 GMT" "2000-07-01 GMT" "2000-08-01 GMT"
#&gt; [129] "2000-09-01 GMT" "2000-10-01 GMT" "2000-11-01 GMT" "2000-12-01 GMT"
#&gt; [133] "2001-01-01 GMT" "2001-02-01 GMT" "2001-03-01 GMT" "2001-04-01 GMT"
#&gt; [137] "2001-05-01 GMT" "2001-06-01 GMT" "2001-07-01 GMT" "2001-08-01 GMT"
#&gt; [141] "2001-09-01 GMT" "2001-10-01 GMT" "2001-11-01 GMT" "2001-12-01 GMT"
#&gt; [145] "2002-01-01 GMT" "2002-02-01 GMT" "2002-03-01 GMT" "2002-04-01 GMT"
#&gt; [149] "2002-05-01 GMT" "2002-06-01 GMT" "2002-07-01 GMT" "2002-08-01 GMT"
#&gt; [153] "2002-09-01 GMT" "2002-10-01 GMT" "2002-11-01 GMT" "2002-12-01 GMT"
#&gt; [157] "2003-01-01 GMT" "2003-02-01 GMT" "2003-03-01 GMT" "2003-04-01 GMT"
#&gt; [161] "2003-05-01 GMT" "2003-06-01 GMT" "2003-07-01 GMT" "2003-08-01 GMT"
#&gt; [165] "2003-09-01 GMT" "2003-10-01 GMT" "2003-11-01 GMT" "2003-12-01 GMT"
#&gt; [169] "2004-01-01 GMT" "2004-02-01 GMT" "2004-03-01 GMT" "2004-04-01 GMT"
#&gt; [173] "2004-05-01 GMT" "2004-06-01 GMT" "2004-07-01 GMT" "2004-08-01 GMT"
#&gt; [177] "2004-09-01 GMT" "2004-10-01 GMT" "2004-11-01 GMT" "2004-12-01 GMT"
#&gt; [181] "2005-01-01 GMT" "2005-02-01 GMT" "2005-03-01 GMT" "2005-04-01 GMT"
#&gt; [185] "2005-05-01 GMT" "2005-06-01 GMT" "2005-07-01 GMT" "2005-08-01 GMT"
#&gt; [189] "2005-09-01 GMT" "2005-10-01 GMT" "2005-11-01 GMT" "2005-12-01 GMT"
#&gt; [193] "2006-01-01 GMT" "2006-02-01 GMT" "2006-03-01 GMT" "2006-04-01 GMT"
#&gt; [197] "2006-05-01 GMT" "2006-06-01 GMT" "2006-07-01 GMT" "2006-08-01 GMT"
#&gt; [201] "2006-09-01 GMT" "2006-10-01 GMT" "2006-11-01 GMT" "2006-12-01 GMT"
#&gt; [205] "2007-01-01 GMT" "2007-02-01 GMT" "2007-03-01 GMT" "2007-04-01 GMT"
#&gt; [209] "2007-05-01 GMT" "2007-06-01 GMT" "2007-07-01 GMT" "2007-08-01 GMT"
#&gt; [213] "2007-09-01 GMT" "2007-10-01 GMT" "2007-11-01 GMT" "2007-12-01 GMT"
#&gt; [217] "2008-01-01 GMT" "2008-02-01 GMT" "2008-03-01 GMT" "2008-04-01 GMT"
#&gt; [221] "2008-05-01 GMT" "2008-06-01 GMT" "2008-07-01 GMT" "2008-08-01 GMT"
#&gt; [225] "2008-09-01 GMT" "2008-10-01 GMT" "2008-11-01 GMT" "2008-12-01 GMT"
#&gt; [229] "2009-01-01 GMT" "2009-02-01 GMT" "2009-03-01 GMT" "2009-04-01 GMT"
#&gt; [233] "2009-05-01 GMT" "2009-06-01 GMT" "2009-07-01 GMT" "2009-08-01 GMT"
#&gt; [237] "2009-09-01 GMT" "2009-10-01 GMT" "2009-11-01 GMT" "2009-12-01 GMT"
#&gt; [241] "2010-01-01 GMT" "2010-02-01 GMT" "2010-03-01 GMT" "2010-04-01 GMT"
#&gt; 
#&gt; $train.holiday.names
#&gt; NULL
#&gt; 
#&gt; $train.component.cols
#&gt;    additive_terms yearly multiplicative_terms
#&gt; 1               1      1                    0
#&gt; 2               1      1                    0
#&gt; 3               1      1                    0
#&gt; 4               1      1                    0
#&gt; 5               1      1                    0
#&gt; 6               1      1                    0
#&gt; 7               1      1                    0
#&gt; 8               1      1                    0
#&gt; 9               1      1                    0
#&gt; 10              1      1                    0
#&gt; 11              1      1                    0
#&gt; 12              1      1                    0
#&gt; 13              1      1                    0
#&gt; 14              1      1                    0
#&gt; 15              1      1                    0
#&gt; 16              1      1                    0
#&gt; 17              1      1                    0
#&gt; 18              1      1                    0
#&gt; 19              1      1                    0
#&gt; 20              1      1                    0
#&gt; 
#&gt; $component.modes
#&gt; $component.modes$additive
#&gt; [1] "yearly"                    "additive_terms"           
#&gt; [3] "extra_regressors_additive" "holidays"                 
#&gt; 
#&gt; $component.modes$multiplicative
#&gt; [1] "multiplicative_terms"            "extra_regressors_multiplicative"
#&gt; 
#&gt; 
#&gt; $fit.kwargs
#&gt; list()
#&gt; 
#&gt; attr(,"class")
#&gt; [1] "prophet" "list"   
#&gt; 
#&gt; ---
#&gt; Model 2: Catboost Errors
#&gt; 
#&gt; CatBoost model (1000 trees)
#&gt; Loss function: RMSE
#&gt; Fit to 2 features</div><div class='input'>
</div></span></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Alberto Almuiña.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


