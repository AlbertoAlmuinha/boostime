<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>General Interface for Boosted PROPHET Time Series Models — boost_prophet • boostime</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="General Interface for Boosted PROPHET Time Series Models — boost_prophet" />
<meta property="og:description" content="boost_prophet() is a way to generate a specification of a Boosted PROPHET model
before fitting and allows the model to be created using
different packages. Currently the only package is prophet." />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">boostime</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/getting-started.html">Getting Started with Boostime</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/AlbertoAlmuinha/boostime/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>General Interface for Boosted PROPHET Time Series Models</h1>
    <small class="dont-index">Source: <a href='https://github.com/AlbertoAlmuinha/boostime/blob/master/R/parsnip-prophet_boost.R'><code>R/parsnip-prophet_boost.R</code></a></small>
    <div class="hidden name"><code>boost_prophet.Rd</code></div>
    </div>

    <div class="ref-description">
    <p><code>boost_prophet()</code> is a way to generate a <em>specification</em> of a Boosted PROPHET model
before fitting and allows the model to be created using
different packages. Currently the only package is <code>prophet</code>.</p>
    </div>

    <pre class="usage"><span class='fu'>boost_prophet</span><span class='op'>(</span>
  mode <span class='op'>=</span> <span class='st'>"regression"</span>,
  growth <span class='op'>=</span> <span class='cn'>NULL</span>,
  changepoint_num <span class='op'>=</span> <span class='cn'>NULL</span>,
  changepoint_range <span class='op'>=</span> <span class='cn'>NULL</span>,
  seasonality_yearly <span class='op'>=</span> <span class='cn'>NULL</span>,
  seasonality_weekly <span class='op'>=</span> <span class='cn'>NULL</span>,
  seasonality_daily <span class='op'>=</span> <span class='cn'>NULL</span>,
  season <span class='op'>=</span> <span class='cn'>NULL</span>,
  prior_scale_changepoints <span class='op'>=</span> <span class='cn'>NULL</span>,
  prior_scale_seasonality <span class='op'>=</span> <span class='cn'>NULL</span>,
  prior_scale_holidays <span class='op'>=</span> <span class='cn'>NULL</span>,
  logistic_cap <span class='op'>=</span> <span class='cn'>NULL</span>,
  logistic_floor <span class='op'>=</span> <span class='cn'>NULL</span>,
  tree_depth <span class='op'>=</span> <span class='cn'>NULL</span>,
  learn_rate <span class='op'>=</span> <span class='cn'>NULL</span>,
  mtry <span class='op'>=</span> <span class='cn'>NULL</span>,
  trees <span class='op'>=</span> <span class='cn'>NULL</span>,
  min_n <span class='op'>=</span> <span class='cn'>NULL</span>,
  sample_size <span class='op'>=</span> <span class='cn'>NULL</span>,
  loss_reduction <span class='op'>=</span> <span class='cn'>NULL</span>
<span class='op'>)</span></pre>

    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>mode</th>
      <td><p>A single character string for the type of model.
The only possible value for this model is "regression".</p></td>
    </tr>
    <tr>
      <th>growth</th>
      <td><p>String 'linear' or 'logistic' to specify a linear or logistic trend.</p></td>
    </tr>
    <tr>
      <th>changepoint_num</th>
      <td><p>Number of potential changepoints to include for modeling trend.</p></td>
    </tr>
    <tr>
      <th>changepoint_range</th>
      <td><p>Adjusts the flexibility of the trend component by limiting to a percentage of data
before the end of the time series. 0.80 means that a changepoint cannot exist after the first 80% of the data.</p></td>
    </tr>
    <tr>
      <th>seasonality_yearly</th>
      <td><p>One of "auto", TRUE or FALSE. Toggles on/off a seasonal component that
models year-over-year seasonality.</p></td>
    </tr>
    <tr>
      <th>seasonality_weekly</th>
      <td><p>One of "auto", TRUE or FALSE. Toggles on/off a seasonal component that
models week-over-week seasonality.</p></td>
    </tr>
    <tr>
      <th>seasonality_daily</th>
      <td><p>One of "auto", TRUE or FALSE. Toggles on/off a seasonal componet that
models day-over-day seasonality.</p></td>
    </tr>
    <tr>
      <th>season</th>
      <td><p>'additive' (default) or 'multiplicative'.</p></td>
    </tr>
    <tr>
      <th>prior_scale_changepoints</th>
      <td><p>Parameter modulating the flexibility of the
automatic changepoint selection. Large values will allow many changepoints,
small values will allow few changepoints.</p></td>
    </tr>
    <tr>
      <th>prior_scale_seasonality</th>
      <td><p>Parameter modulating the strength of the
seasonality model. Larger values allow the model to fit larger seasonal
fluctuations, smaller values dampen the seasonality.</p></td>
    </tr>
    <tr>
      <th>prior_scale_holidays</th>
      <td><p>Parameter modulating the strength of the holiday components model,
unless overridden in the holidays input.</p></td>
    </tr>
    <tr>
      <th>logistic_cap</th>
      <td><p>When growth is logistic, the upper-bound for "saturation".</p></td>
    </tr>
    <tr>
      <th>logistic_floor</th>
      <td><p>When growth is logistic, the lower-bound for "saturation".</p></td>
    </tr>
    <tr>
      <th>tree_depth</th>
      <td><p>The maximum depth of the tree (i.e. number of splits).</p></td>
    </tr>
    <tr>
      <th>learn_rate</th>
      <td><p>The rate at which the boosting algorithm adapts from iteration-to-iteration.</p></td>
    </tr>
    <tr>
      <th>mtry</th>
      <td><p>The number of predictors that will be randomly sampled at each split when creating the tree models.</p></td>
    </tr>
    <tr>
      <th>trees</th>
      <td><p>The number of trees contained in the ensemble.</p></td>
    </tr>
    <tr>
      <th>min_n</th>
      <td><p>The minimum number of data points in a node that is required for the node to be split further.</p></td>
    </tr>
    <tr>
      <th>sample_size</th>
      <td><p>The amount of data exposed to the fitting routine.</p></td>
    </tr>
    <tr>
      <th>loss_reduction</th>
      <td><p>The reduction in the loss function required to split further.</p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    <p>The data given to the function are not saved and are only used
to determine the <em>mode</em> of the model. For <code>boost_prophet()</code>, the
mode will always be "regression".</p>
<p>The model can be created using the <code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code> function using the
following <em>engines</em>:</p><ul>
<li><p>"prophet_catboost" (default) - Connects to <code><a href='https://rdrr.io/pkg/prophet/man/prophet.html'>prophet::prophet()</a></code> and <code><a href='https://rdrr.io/pkg/catboost/man/catboost.train.html'>catboost::catboost.train()</a></code></p></li>
<li><p>"prophet_lightgbm" - Connects to <code><a href='https://rdrr.io/pkg/prophet/man/prophet.html'>prophet::prophet()</a></code> and <code><a href='https://rdrr.io/pkg/lightgbm/man/lgb.train.html'>lightgbm::lgb.train()</a></code></p></li>
</ul>

<p><strong>Main Arguments</strong></p>
<p>The main arguments (tuning parameters) for the <strong>PROPHET</strong> model are:</p><ul>
<li><p><code>growth</code>: String 'linear' or 'logistic' to specify a linear or logistic trend.</p></li>
<li><p><code>changepoint_num</code>: Number of potential changepoints to include for modeling trend.</p></li>
<li><p><code>changepoint_range</code>: Range changepoints that adjusts how close to the end
the last changepoint can be located.</p></li>
<li><p><code>season</code>: 'additive' (default) or 'multiplicative'.</p></li>
<li><p><code>prior_scale_changepoints</code>: Parameter modulating the flexibility of the
automatic changepoint selection. Large values will allow many changepoints,
small values will allow few changepoints.</p></li>
<li><p><code>prior_scale_seasonality</code>: Parameter modulating the strength of the
seasonality model. Larger values allow the model to fit larger seasonal
fluctuations, smaller values dampen the seasonality.</p></li>
<li><p><code>prior_scale_holidays</code>: Parameter modulating the strength of the holiday components model,
unless overridden in the holidays input.</p></li>
</ul>

<p>The main arguments (tuning parameters) for the model <strong>Catboost/LightGBM model</strong> are:</p><ul>
<li><p><code>tree_depth</code>: The maximum depth of the tree (i.e. number of splits).</p></li>
<li><p><code>learn_rate</code>: The rate at which the boosting algorithm adapts from iteration-to-iteration.</p></li>
<li><p><code>mtry</code>: The number of predictors that will be randomly sampled at each split when creating the tree models.</p></li>
<li><p><code>trees</code>: The number of trees contained in the ensemble.</p></li>
<li><p><code>min_n</code>: The minimum number of data points in a node that is required for the node to be split further.</p></li>
<li><p><code>sample_size</code>: The amount of data exposed to the fitting routine.</p></li>
<li><p><code>loss_reduction</code>: The reduction in the loss function required to split further.</p></li>
</ul>

<p>These arguments are converted to their specific names at the
time that the model is fit.</p>
<p>Other options and argument can be
set using <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code> (See Engine Details below).</p>
<p>If parameters need to be modified, <code><a href='https://rdrr.io/r/stats/update.html'>update()</a></code> can be used
in lieu of recreating the object from scratch.</p>
    <h2 class="hasAnchor" id="engine-details"><a class="anchor" href="#engine-details"></a>Engine Details</h2>

    


<p>The standardized parameter names in <code>boostime</code> can be mapped to their original
names in each engine:</p>
<p>Model 1: PROPHET:</p><table class='table'>
<tr><td>boostime</td><td>prophet</td></tr>
<tr><td>growth</td><td>growth ('linear')</td></tr>
<tr><td>changepoint_num</td><td>n.changepoints (25)</td></tr>
<tr><td>changepoint_range</td><td>changepoints.range (0.8)</td></tr>
<tr><td>seasonality_yearly</td><td>yearly.seasonality ('auto')</td></tr>
<tr><td>seasonality_weekly</td><td>weekly.seasonality ('auto')</td></tr>
<tr><td>seasonality_daily</td><td>daily.seasonality ('auto')</td></tr>
<tr><td>season</td><td>seasonality.mode ('additive')</td></tr>
<tr><td>prior_scale_changepoints</td><td>changepoint.prior.scale (0.05)</td></tr>
<tr><td>prior_scale_seasonality</td><td>seasonality.prior.scale (10)</td></tr>
<tr><td>prior_scale_holidays</td><td>holidays.prior.scale (10)</td></tr>
<tr><td>logistic_cap</td><td>df$cap (NULL)</td></tr>
<tr><td>logistic_floor</td><td>df$floor (NULL)</td></tr>
</table>



<p>Model 2: Catboost / LightGBM:</p><table class='table'>
<tr><td>boostime</td><td>catboost::catboost.train</td><td>lightgbm::lgb.train</td></tr>
<tr><td>tree_depth</td><td>depth</td><td>max_depth</td></tr>
<tr><td>learn_rate</td><td>learning_rate</td><td>learning_rate</td></tr>
<tr><td>mtry</td><td>rsm</td><td>feature_fraction</td></tr>
<tr><td>trees</td><td>iterations</td><td>num_iterations</td></tr>
<tr><td>min_n</td><td>min_data_in_leaf</td><td>min_data_in_leaf</td></tr>
<tr><td>loss_reduction</td><td>None</td><td>min_gain_to_split</td></tr>
<tr><td>sample_size</td><td>subsample</td><td>bagging_fraction</td></tr>
</table>



<p>Other options can be set using <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code>.</p>
<p><strong>prophet_catboost</strong></p>
<p>Model 1: PROPHET (<code><a href='https://rdrr.io/pkg/prophet/man/prophet.html'>prophet::prophet</a></code>):</p><pre><span class='co'>## function (df = NULL, growth = "linear", changepoints = NULL, n.changepoints = 25, </span>
<span class='co'>##     changepoint.range = 0.8, yearly.seasonality = "auto", weekly.seasonality = "auto", </span>
<span class='co'>##     daily.seasonality = "auto", holidays = NULL, seasonality.mode = "additive", </span>
<span class='co'>##     seasonality.prior.scale = 10, holidays.prior.scale = 10, changepoint.prior.scale = 0.05, </span>
<span class='co'>##     mcmc.samples = 0, interval.width = 0.8, uncertainty.samples = 1000, </span>
<span class='co'>##     fit = TRUE, ...)</span>
</pre>

<p>Parameter Notes:</p><ul>
<li><p><code>df</code>: This is supplied via the parsnip / boostime <code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code> interface
(so don't provide this manually). See Fit Details (below).</p></li>
<li><p><code>holidays</code>: A data.frame of holidays can be supplied via <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code></p></li>
<li><p><code>uncertainty.samples</code>: The default is set to 0 because the prophet
uncertainty intervals are not used as part of the Modeltime Workflow.
You can override this setting if you plan to use prophet's uncertainty tools.</p></li>
</ul>

<p>Logistic Growth and Saturation Levels:</p><ul>
<li><p>For <code>growth = "logistic"</code>, simply add numeric values for <code>logistic_cap</code> and / or
<code>logistic_floor</code>. There is <em>no need</em> to add additional columns
for "cap" and "floor" to your data frame.</p></li>
</ul>

<p>Limitations:</p><ul>
<li><p><code><a href='https://rdrr.io/pkg/prophet/man/add_seasonality.html'>prophet::add_seasonality()</a></code> is not currently implemented. It's used to
specify non-standard seasonalities using fourier series. An alternative is to use
<code><a href='https://rdrr.io/pkg/timetk/man/step_fourier.html'>step_fourier()</a></code> and supply custom seasonalities as Extra Regressors.</p></li>
</ul>

<p>Model 2: Catboost (<code><a href='https://rdrr.io/pkg/catboost/man/catboost.train.html'>catboost::catboost.train</a></code>):</p><pre><span class='co'>## function (learn_pool, test_pool = NULL, params = list())</span>
</pre>

<p>Parameter Notes:</p><ul>
<li><p>Catboost uses a <code>params = list()</code> to capture.
Parsnip / Timeboost automatically sends any args provided as <code>...</code> inside of <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code> to
the <code>params = list(...)</code>.</p></li>
</ul>

<p><strong>prophet_lightgbm</strong></p>
<p>Model 1: PROPHET (<code><a href='https://rdrr.io/pkg/prophet/man/prophet.html'>prophet::prophet</a></code>):</p><pre><span class='co'>## function (df = NULL, growth = "linear", changepoints = NULL, n.changepoints = 25, </span>
<span class='co'>##     changepoint.range = 0.8, yearly.seasonality = "auto", weekly.seasonality = "auto", </span>
<span class='co'>##     daily.seasonality = "auto", holidays = NULL, seasonality.mode = "additive", </span>
<span class='co'>##     seasonality.prior.scale = 10, holidays.prior.scale = 10, changepoint.prior.scale = 0.05, </span>
<span class='co'>##     mcmc.samples = 0, interval.width = 0.8, uncertainty.samples = 1000, </span>
<span class='co'>##     fit = TRUE, ...)</span>
</pre>

<p>Parameter Notes:</p><ul>
<li><p><code>df</code>: This is supplied via the parsnip / boostime <code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code> interface
(so don't provide this manually). See Fit Details (below).</p></li>
<li><p><code>holidays</code>: A data.frame of holidays can be supplied via <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code></p></li>
<li><p><code>uncertainty.samples</code>: The default is set to 0 because the prophet
uncertainty intervals are not used as part of the Modeltime Workflow.
You can override this setting if you plan to use prophet's uncertainty tools.</p></li>
</ul>

<p>Logistic Growth and Saturation Levels:</p><ul>
<li><p>For <code>growth = "logistic"</code>, simply add numeric values for <code>logistic_cap</code> and / or
<code>logistic_floor</code>. There is <em>no need</em> to add additional columns
for "cap" and "floor" to your data frame.</p></li>
</ul>

<p>Limitations:</p><ul>
<li><p><code><a href='https://rdrr.io/pkg/prophet/man/add_seasonality.html'>prophet::add_seasonality()</a></code> is not currently implemented. It's used to
specify non-standard seasonalities using fourier series. An alternative is to use
<code><a href='https://rdrr.io/pkg/timetk/man/step_fourier.html'>step_fourier()</a></code> and supply custom seasonalities as Extra Regressors.</p></li>
</ul>

<p>Model 2: Lightgbm (<code><a href='https://rdrr.io/pkg/catboost/man/catboost.train.html'>catboost::catboost.train</a></code>):</p><pre><span class='co'>## function (params = list(), data, nrounds = 10L, valids = list(), obj = NULL, </span>
<span class='co'>##     eval = NULL, verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL, </span>
<span class='co'>##     colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL, </span>
<span class='co'>##     callbacks = list(), reset_data = FALSE, ...)</span>
</pre>

<p>Parameter Notes:</p><ul>
<li><p>Lightgbm uses a <code>params = list()</code> to capture.
Parsnip / Timeboost automatically sends any args provided as <code>...</code> inside of <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code> to
the <code>params = list(...)</code>.</p></li>
</ul>

    <h2 class="hasAnchor" id="fit-details"><a class="anchor" href="#fit-details"></a>Fit Details</h2>

    


<p><strong>Date and Date-Time Variable</strong></p>
<p>It's a requirement to have a date or date-time variable as a predictor.
The <code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code> interface accepts date and date-time features and handles them internally.</p><ul>
<li><p><code><a href='https://generics.r-lib.org/reference/fit.html'>fit(y ~ date)</a></code></p></li>
</ul>

<p><strong>Univariate (No Extra Regressors):</strong></p>
<p>For univariate analysis, you must include a date or date-time feature. Simply use:</p><ul>
<li><p>Formula Interface (recommended): <code><a href='https://generics.r-lib.org/reference/fit.html'>fit(y ~ date)</a></code> will ignore xreg's.</p></li>
</ul>

<p><strong>Multivariate (Extra Regressors)</strong></p>
<p>Extra Regressors parameter is populated using the <code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code> or <code><a href='https://generics.r-lib.org/reference/fit_xy.html'>fit_xy()</a></code> function:</p><ul>
<li><p>Only <code>factor</code>, <code>ordered factor</code>, and <code>numeric</code> data will be used as xregs.</p></li>
<li><p>Date and Date-time variables are not used as xregs</p></li>
<li><p><code>character</code> data should be converted to factor.</p></li>
</ul>

<p><em>Xreg Example:</em> Suppose you have 3 features:</p><ol>
<li><p><code>y</code> (target)</p></li>
<li><p><code>date</code> (time stamp),</p></li>
<li><p><code>month.lbl</code> (labeled month as a ordered factor).</p></li>
</ol>

<p>The <code>month.lbl</code> is an exogenous regressor that can be passed to the <code>arima_reg()</code> using
<code><a href='https://generics.r-lib.org/reference/fit.html'>fit()</a></code>:</p><ul>
<li><p><code><a href='https://generics.r-lib.org/reference/fit.html'>fit(y ~ date + month.lbl)</a></code> will pass <code>month.lbl</code> on as an exogenous regressor.</p></li>
</ul>

<p>Note that date or date-time class values are excluded from <code>xreg</code>.</p>
    <h2 class="hasAnchor" id="see-also"><a class="anchor" href="#see-also"></a>See also</h2>

    <div class='dont-index'><p><code><a href='https://parsnip.tidymodels.org/reference/fit.html'>fit.model_spec()</a></code>, <code><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine()</a></code></p></div>

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://dplyr.tidyverse.org'>dplyr</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://lubridate.tidyverse.org'>lubridate</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://parsnip.tidymodels.org'>parsnip</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://rsample.tidymodels.org'>rsample</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://github.com/business-science/timetk'>timetk</a></span><span class='op'>)</span>
<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://github.com/AlbertoAlmuinha/boostime'>boostime</a></span><span class='op'>)</span>

<span class='co'># Data</span>
<span class='va'>m750</span> <span class='op'>&lt;-</span> <span class='va'>m4_monthly</span> <span class='op'>%&gt;%</span> <span class='fu'><a href='https://dplyr.tidyverse.org/reference/filter.html'>filter</a></span><span class='op'>(</span><span class='va'>id</span> <span class='op'>==</span> <span class='st'>"M750"</span><span class='op'>)</span>
<span class='va'>m750</span>
</div><div class='output co'>#&gt; <span style='color: #949494;'># A tibble: 306 x 3</span><span>
#&gt;    id    date       value
#&gt;    </span><span style='color: #949494;font-style: italic;'>&lt;fct&gt;</span><span> </span><span style='color: #949494;font-style: italic;'>&lt;date&gt;</span><span>     </span><span style='color: #949494;font-style: italic;'>&lt;dbl&gt;</span><span>
#&gt; </span><span style='color: #BCBCBC;'> 1</span><span> M750  1990-01-01  </span><span style='text-decoration: underline;'>6</span><span>370
#&gt; </span><span style='color: #BCBCBC;'> 2</span><span> M750  1990-02-01  </span><span style='text-decoration: underline;'>6</span><span>430
#&gt; </span><span style='color: #BCBCBC;'> 3</span><span> M750  1990-03-01  </span><span style='text-decoration: underline;'>6</span><span>520
#&gt; </span><span style='color: #BCBCBC;'> 4</span><span> M750  1990-04-01  </span><span style='text-decoration: underline;'>6</span><span>580
#&gt; </span><span style='color: #BCBCBC;'> 5</span><span> M750  1990-05-01  </span><span style='text-decoration: underline;'>6</span><span>620
#&gt; </span><span style='color: #BCBCBC;'> 6</span><span> M750  1990-06-01  </span><span style='text-decoration: underline;'>6</span><span>690
#&gt; </span><span style='color: #BCBCBC;'> 7</span><span> M750  1990-07-01  </span><span style='text-decoration: underline;'>6</span><span>000
#&gt; </span><span style='color: #BCBCBC;'> 8</span><span> M750  1990-08-01  </span><span style='text-decoration: underline;'>5</span><span>450
#&gt; </span><span style='color: #BCBCBC;'> 9</span><span> M750  1990-09-01  </span><span style='text-decoration: underline;'>6</span><span>480
#&gt; </span><span style='color: #BCBCBC;'>10</span><span> M750  1990-10-01  </span><span style='text-decoration: underline;'>6</span><span>820
#&gt; </span><span style='color: #949494;'># ... with 296 more rows</span><span></div><div class='input'>
<span class='co'># Split Data 80/20</span>
<span class='va'>splits</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rsample.tidymodels.org/reference/initial_split.html'>initial_time_split</a></span><span class='op'>(</span><span class='va'>m750</span>, prop <span class='op'>=</span> <span class='fl'>0.8</span><span class='op'>)</span>

<span class='co'># ---- PROPHET ----</span>

<span class='co'># Model Spec</span>
<span class='va'>model_spec</span> <span class='op'>&lt;-</span> <span class='fu'>boost_prophet</span><span class='op'>(</span>
    learn_rate <span class='op'>=</span> <span class='fl'>0.1</span>
<span class='op'>)</span> <span class='op'>%&gt;%</span>
    <span class='fu'><a href='https://parsnip.tidymodels.org/reference/set_engine.html'>set_engine</a></span><span class='op'>(</span><span class='st'>"prophet_catboost"</span><span class='op'>)</span>

<span class='co'># Fit Spec</span>
<span class='va'>model_fit</span> <span class='op'>&lt;-</span> <span class='va'>model_spec</span> <span class='op'>%&gt;%</span>
    <span class='fu'><a href='https://generics.r-lib.org/reference/fit.html'>fit</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/Log.html'>log</a></span><span class='op'>(</span><span class='va'>value</span><span class='op'>)</span> <span class='op'>~</span> <span class='va'>date</span> <span class='op'>+</span> <span class='fu'><a href='https://rdrr.io/r/base/numeric.html'>as.numeric</a></span><span class='op'>(</span><span class='va'>date</span><span class='op'>)</span> <span class='op'>+</span> <span class='fu'><a href='http://lubridate.tidyverse.org/reference/month.html'>month</a></span><span class='op'>(</span><span class='va'>date</span>, label <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span>,
        data <span class='op'>=</span> <span class='fu'><a href='https://rsample.tidymodels.org/reference/initial_split.html'>training</a></span><span class='op'>(</span><span class='va'>splits</span><span class='op'>)</span><span class='op'>)</span>
</div><div class='output co'>#&gt; <span class='message'>Disabling weekly seasonality. Run prophet with weekly.seasonality=TRUE to override this.</span></div><div class='output co'>#&gt; <span class='message'>Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this.</span></div><div class='output co'>#&gt; 0:	learn: 0.0184749	total: 294us	remaining: 294ms
#&gt; 1:	learn: 0.0184320	total: 2.98ms	remaining: 1.49s
#&gt; 2:	learn: 0.0183684	total: 5.19ms	remaining: 1.72s
#&gt; 3:	learn: 0.0183684	total: 5.41ms	remaining: 1.35s
#&gt; 4:	learn: 0.0182161	total: 13.7ms	remaining: 2.72s
#&gt; 5:	learn: 0.0181648	total: 17.1ms	remaining: 2.83s
#&gt; 6:	learn: 0.0181046	total: 23.4ms	remaining: 3.31s
#&gt; 7:	learn: 0.0180918	total: 29.1ms	remaining: 3.6s
#&gt; 8:	learn: 0.0180918	total: 29.3ms	remaining: 3.23s
#&gt; 9:	learn: 0.0180779	total: 31.3ms	remaining: 3.1s
#&gt; 10:	learn: 0.0179316	total: 32.1ms	remaining: 2.88s
#&gt; 11:	learn: 0.0179065	total: 34.1ms	remaining: 2.8s
#&gt; 12:	learn: 0.0179064	total: 34.3ms	remaining: 2.6s
#&gt; 13:	learn: 0.0178947	total: 36.1ms	remaining: 2.54s
#&gt; 14:	learn: 0.0178787	total: 39.6ms	remaining: 2.6s
#&gt; 15:	learn: 0.0178756	total: 41.8ms	remaining: 2.57s
#&gt; 16:	learn: 0.0178302	total: 44.1ms	remaining: 2.55s
#&gt; 17:	learn: 0.0178293	total: 46.1ms	remaining: 2.52s
#&gt; 18:	learn: 0.0177993	total: 48.5ms	remaining: 2.5s
#&gt; 19:	learn: 0.0177607	total: 54.1ms	remaining: 2.65s
#&gt; 20:	learn: 0.0177607	total: 54.3ms	remaining: 2.53s
#&gt; 21:	learn: 0.0177601	total: 56.2ms	remaining: 2.5s
#&gt; 22:	learn: 0.0177601	total: 56.4ms	remaining: 2.4s
#&gt; 23:	learn: 0.0176265	total: 60.4ms	remaining: 2.46s
#&gt; 24:	learn: 0.0176260	total: 62.6ms	remaining: 2.44s
#&gt; 25:	learn: 0.0176260	total: 62.8ms	remaining: 2.35s
#&gt; 26:	learn: 0.0176260	total: 63ms	remaining: 2.27s
#&gt; 27:	learn: 0.0176240	total: 65ms	remaining: 2.25s
#&gt; 28:	learn: 0.0176240	total: 65.2ms	remaining: 2.18s
#&gt; 29:	learn: 0.0176209	total: 105ms	remaining: 3.39s
#&gt; 30:	learn: 0.0176209	total: 106ms	remaining: 3.31s
#&gt; 31:	learn: 0.0176209	total: 106ms	remaining: 3.22s
#&gt; 32:	learn: 0.0176209	total: 107ms	remaining: 3.14s
#&gt; 33:	learn: 0.0175005	total: 108ms	remaining: 3.08s
#&gt; 34:	learn: 0.0174721	total: 119ms	remaining: 3.27s
#&gt; 35:	learn: 0.0174144	total: 139ms	remaining: 3.73s
#&gt; 36:	learn: 0.0174144	total: 140ms	remaining: 3.65s
#&gt; 37:	learn: 0.0174144	total: 141ms	remaining: 3.56s
#&gt; 38:	learn: 0.0174144	total: 141ms	remaining: 3.47s
#&gt; 39:	learn: 0.0174144	total: 141ms	remaining: 3.39s
#&gt; 40:	learn: 0.0174144	total: 142ms	remaining: 3.32s
#&gt; 41:	learn: 0.0174144	total: 142ms	remaining: 3.25s
#&gt; 42:	learn: 0.0174103	total: 146ms	remaining: 3.26s
#&gt; 43:	learn: 0.0173987	total: 157ms	remaining: 3.42s
#&gt; 44:	learn: 0.0173884	total: 166ms	remaining: 3.52s
#&gt; 45:	learn: 0.0173884	total: 166ms	remaining: 3.44s
#&gt; 46:	learn: 0.0173884	total: 166ms	remaining: 3.37s
#&gt; 47:	learn: 0.0171686	total: 170ms	remaining: 3.37s
#&gt; 48:	learn: 0.0171674	total: 176ms	remaining: 3.42s
#&gt; 49:	learn: 0.0171507	total: 179ms	remaining: 3.39s
#&gt; 50:	learn: 0.0171507	total: 179ms	remaining: 3.33s
#&gt; 51:	learn: 0.0171507	total: 179ms	remaining: 3.27s
#&gt; 52:	learn: 0.0171507	total: 179ms	remaining: 3.21s
#&gt; 53:	learn: 0.0171474	total: 182ms	remaining: 3.18s
#&gt; 54:	learn: 0.0171474	total: 182ms	remaining: 3.13s
#&gt; 55:	learn: 0.0171474	total: 182ms	remaining: 3.07s
#&gt; 56:	learn: 0.0171474	total: 182ms	remaining: 3.02s
#&gt; 57:	learn: 0.0171466	total: 185ms	remaining: 3s
#&gt; 58:	learn: 0.0171466	total: 185ms	remaining: 2.95s
#&gt; 59:	learn: 0.0170944	total: 192ms	remaining: 3s
#&gt; 60:	learn: 0.0170944	total: 192ms	remaining: 2.95s
#&gt; 61:	learn: 0.0170890	total: 194ms	remaining: 2.94s
#&gt; 62:	learn: 0.0170890	total: 194ms	remaining: 2.89s
#&gt; 63:	learn: 0.0169700	total: 202ms	remaining: 2.95s
#&gt; 64:	learn: 0.0169700	total: 202ms	remaining: 2.91s
#&gt; 65:	learn: 0.0167812	total: 209ms	remaining: 2.96s
#&gt; 66:	learn: 0.0166890	total: 219ms	remaining: 3.04s
#&gt; 67:	learn: 0.0166877	total: 221ms	remaining: 3.03s
#&gt; 68:	learn: 0.0166724	total: 222ms	remaining: 2.99s
#&gt; 69:	learn: 0.0166724	total: 222ms	remaining: 2.95s
#&gt; 70:	learn: 0.0166556	total: 222ms	remaining: 2.91s
#&gt; 71:	learn: 0.0166542	total: 225ms	remaining: 2.9s
#&gt; 72:	learn: 0.0166542	total: 225ms	remaining: 2.86s
#&gt; 73:	learn: 0.0165874	total: 228ms	remaining: 2.85s
#&gt; 74:	learn: 0.0165874	total: 228ms	remaining: 2.81s
#&gt; 75:	learn: 0.0165623	total: 233ms	remaining: 2.83s
#&gt; 76:	learn: 0.0165614	total: 236ms	remaining: 2.82s
#&gt; 77:	learn: 0.0165614	total: 236ms	remaining: 2.79s
#&gt; 78:	learn: 0.0164899	total: 241ms	remaining: 2.81s
#&gt; 79:	learn: 0.0164899	total: 242ms	remaining: 2.78s
#&gt; 80:	learn: 0.0164895	total: 249ms	remaining: 2.82s
#&gt; 81:	learn: 0.0164895	total: 249ms	remaining: 2.79s
#&gt; 82:	learn: 0.0164342	total: 250ms	remaining: 2.76s
#&gt; 83:	learn: 0.0164342	total: 250ms	remaining: 2.72s
#&gt; 84:	learn: 0.0164342	total: 250ms	remaining: 2.69s
#&gt; 85:	learn: 0.0164342	total: 250ms	remaining: 2.66s
#&gt; 86:	learn: 0.0164154	total: 257ms	remaining: 2.7s
#&gt; 87:	learn: 0.0163790	total: 267ms	remaining: 2.77s
#&gt; 88:	learn: 0.0163680	total: 287ms	remaining: 2.93s
#&gt; 89:	learn: 0.0163680	total: 287ms	remaining: 2.9s
#&gt; 90:	learn: 0.0162504	total: 297ms	remaining: 2.97s
#&gt; 91:	learn: 0.0161613	total: 303ms	remaining: 2.99s
#&gt; 92:	learn: 0.0161613	total: 304ms	remaining: 2.96s
#&gt; 93:	learn: 0.0161518	total: 311ms	remaining: 3s
#&gt; 94:	learn: 0.0161411	total: 323ms	remaining: 3.08s
#&gt; 95:	learn: 0.0161411	total: 323ms	remaining: 3.05s
#&gt; 96:	learn: 0.0161411	total: 324ms	remaining: 3.01s
#&gt; 97:	learn: 0.0161214	total: 344ms	remaining: 3.17s
#&gt; 98:	learn: 0.0161110	total: 357ms	remaining: 3.25s
#&gt; 99:	learn: 0.0160023	total: 383ms	remaining: 3.44s
#&gt; 100:	learn: 0.0160023	total: 386ms	remaining: 3.43s
#&gt; 101:	learn: 0.0159606	total: 391ms	remaining: 3.44s
#&gt; 102:	learn: 0.0159195	total: 403ms	remaining: 3.51s
#&gt; 103:	learn: 0.0159195	total: 403ms	remaining: 3.48s
#&gt; 104:	learn: 0.0159184	total: 409ms	remaining: 3.48s
#&gt; 105:	learn: 0.0159184	total: 411ms	remaining: 3.47s
#&gt; 106:	learn: 0.0159125	total: 412ms	remaining: 3.44s
#&gt; 107:	learn: 0.0159125	total: 412ms	remaining: 3.4s
#&gt; 108:	learn: 0.0157806	total: 435ms	remaining: 3.55s
#&gt; 109:	learn: 0.0157806	total: 435ms	remaining: 3.52s
#&gt; 110:	learn: 0.0157532	total: 442ms	remaining: 3.54s
#&gt; 111:	learn: 0.0157458	total: 442ms	remaining: 3.51s
#&gt; 112:	learn: 0.0157458	total: 445ms	remaining: 3.49s
#&gt; 113:	learn: 0.0157199	total: 446ms	remaining: 3.46s
#&gt; 114:	learn: 0.0157199	total: 446ms	remaining: 3.43s
#&gt; 115:	learn: 0.0157199	total: 448ms	remaining: 3.42s
#&gt; 116:	learn: 0.0157199	total: 449ms	remaining: 3.38s
#&gt; 117:	learn: 0.0156851	total: 449ms	remaining: 3.36s
#&gt; 118:	learn: 0.0156851	total: 449ms	remaining: 3.33s
#&gt; 119:	learn: 0.0156851	total: 454ms	remaining: 3.33s
#&gt; 120:	learn: 0.0156851	total: 454ms	remaining: 3.3s
#&gt; 121:	learn: 0.0156806	total: 472ms	remaining: 3.4s
#&gt; 122:	learn: 0.0156800	total: 485ms	remaining: 3.46s
#&gt; 123:	learn: 0.0156539	total: 490ms	remaining: 3.46s
#&gt; 124:	learn: 0.0156539	total: 491ms	remaining: 3.44s
#&gt; 125:	learn: 0.0156457	total: 491ms	remaining: 3.41s
#&gt; 126:	learn: 0.0156457	total: 494ms	remaining: 3.4s
#&gt; 127:	learn: 0.0156326	total: 497ms	remaining: 3.39s
#&gt; 128:	learn: 0.0156157	total: 502ms	remaining: 3.39s
#&gt; 129:	learn: 0.0156157	total: 502ms	remaining: 3.36s
#&gt; 130:	learn: 0.0156157	total: 502ms	remaining: 3.33s
#&gt; 131:	learn: 0.0156157	total: 505ms	remaining: 3.32s
#&gt; 132:	learn: 0.0156157	total: 505ms	remaining: 3.29s
#&gt; 133:	learn: 0.0156157	total: 505ms	remaining: 3.27s
#&gt; 134:	learn: 0.0156092	total: 512ms	remaining: 3.28s
#&gt; 135:	learn: 0.0155318	total: 545ms	remaining: 3.46s
#&gt; 136:	learn: 0.0154597	total: 554ms	remaining: 3.49s
#&gt; 137:	learn: 0.0154140	total: 567ms	remaining: 3.54s
#&gt; 138:	learn: 0.0154140	total: 568ms	remaining: 3.52s
#&gt; 139:	learn: 0.0154140	total: 568ms	remaining: 3.49s
#&gt; 140:	learn: 0.0154085	total: 574ms	remaining: 3.5s
#&gt; 141:	learn: 0.0154011	total: 580ms	remaining: 3.5s
#&gt; 142:	learn: 0.0153978	total: 580ms	remaining: 3.48s
#&gt; 143:	learn: 0.0153978	total: 580ms	remaining: 3.45s
#&gt; 144:	learn: 0.0153978	total: 581ms	remaining: 3.42s
#&gt; 145:	learn: 0.0153481	total: 583ms	remaining: 3.41s
#&gt; 146:	learn: 0.0153481	total: 583ms	remaining: 3.38s
#&gt; 147:	learn: 0.0153481	total: 584ms	remaining: 3.36s
#&gt; 148:	learn: 0.0153446	total: 584ms	remaining: 3.33s
#&gt; 149:	learn: 0.0153411	total: 584ms	remaining: 3.31s
#&gt; 150:	learn: 0.0153411	total: 584ms	remaining: 3.28s
#&gt; 151:	learn: 0.0153138	total: 587ms	remaining: 3.27s
#&gt; 152:	learn: 0.0153137	total: 590ms	remaining: 3.26s
#&gt; 153:	learn: 0.0153137	total: 590ms	remaining: 3.24s
#&gt; 154:	learn: 0.0152715	total: 592ms	remaining: 3.23s
#&gt; 155:	learn: 0.0152684	total: 593ms	remaining: 3.21s
#&gt; 156:	learn: 0.0152174	total: 596ms	remaining: 3.2s
#&gt; 157:	learn: 0.0152135	total: 598ms	remaining: 3.19s
#&gt; 158:	learn: 0.0152125	total: 608ms	remaining: 3.21s
#&gt; 159:	learn: 0.0152124	total: 620ms	remaining: 3.25s
#&gt; 160:	learn: 0.0152124	total: 620ms	remaining: 3.23s
#&gt; 161:	learn: 0.0152124	total: 620ms	remaining: 3.21s
#&gt; 162:	learn: 0.0151765	total: 632ms	remaining: 3.25s
#&gt; 163:	learn: 0.0151712	total: 634ms	remaining: 3.23s
#&gt; 164:	learn: 0.0151712	total: 635ms	remaining: 3.21s
#&gt; 165:	learn: 0.0151712	total: 635ms	remaining: 3.19s
#&gt; 166:	learn: 0.0151712	total: 635ms	remaining: 3.17s
#&gt; 167:	learn: 0.0151712	total: 635ms	remaining: 3.15s
#&gt; 168:	learn: 0.0151210	total: 642ms	remaining: 3.15s
#&gt; 169:	learn: 0.0151025	total: 642ms	remaining: 3.13s
#&gt; 170:	learn: 0.0151025	total: 643ms	remaining: 3.11s
#&gt; 171:	learn: 0.0151025	total: 645ms	remaining: 3.1s
#&gt; 172:	learn: 0.0151025	total: 645ms	remaining: 3.08s
#&gt; 173:	learn: 0.0151025	total: 645ms	remaining: 3.06s
#&gt; 174:	learn: 0.0151020	total: 647ms	remaining: 3.05s
#&gt; 175:	learn: 0.0151020	total: 647ms	remaining: 3.03s
#&gt; 176:	learn: 0.0151020	total: 648ms	remaining: 3.01s
#&gt; 177:	learn: 0.0150976	total: 650ms	remaining: 3s
#&gt; 178:	learn: 0.0150762	total: 654ms	remaining: 3s
#&gt; 179:	learn: 0.0150762	total: 654ms	remaining: 2.98s
#&gt; 180:	learn: 0.0150762	total: 654ms	remaining: 2.96s
#&gt; 181:	learn: 0.0150762	total: 654ms	remaining: 2.94s
#&gt; 182:	learn: 0.0150643	total: 655ms	remaining: 2.92s
#&gt; 183:	learn: 0.0149936	total: 659ms	remaining: 2.92s
#&gt; 184:	learn: 0.0149936	total: 660ms	remaining: 2.9s
#&gt; 185:	learn: 0.0149936	total: 660ms	remaining: 2.89s
#&gt; 186:	learn: 0.0149891	total: 660ms	remaining: 2.87s
#&gt; 187:	learn: 0.0149891	total: 660ms	remaining: 2.85s
#&gt; 188:	learn: 0.0149889	total: 664ms	remaining: 2.85s
#&gt; 189:	learn: 0.0149889	total: 665ms	remaining: 2.83s
#&gt; 190:	learn: 0.0149889	total: 665ms	remaining: 2.82s
#&gt; 191:	learn: 0.0149860	total: 681ms	remaining: 2.87s
#&gt; 192:	learn: 0.0149822	total: 683ms	remaining: 2.86s
#&gt; 193:	learn: 0.0149822	total: 683ms	remaining: 2.84s
#&gt; 194:	learn: 0.0149247	total: 688ms	remaining: 2.84s
#&gt; 195:	learn: 0.0149247	total: 688ms	remaining: 2.82s
#&gt; 196:	learn: 0.0149247	total: 688ms	remaining: 2.81s
#&gt; 197:	learn: 0.0149247	total: 689ms	remaining: 2.79s
#&gt; 198:	learn: 0.0149247	total: 689ms	remaining: 2.77s
#&gt; 199:	learn: 0.0149247	total: 689ms	remaining: 2.76s
#&gt; 200:	learn: 0.0149247	total: 689ms	remaining: 2.74s
#&gt; 201:	learn: 0.0149247	total: 690ms	remaining: 2.72s
#&gt; 202:	learn: 0.0149247	total: 692ms	remaining: 2.72s
#&gt; 203:	learn: 0.0149247	total: 692ms	remaining: 2.7s
#&gt; 204:	learn: 0.0149247	total: 692ms	remaining: 2.68s
#&gt; 205:	learn: 0.0149247	total: 693ms	remaining: 2.67s
#&gt; 206:	learn: 0.0149247	total: 693ms	remaining: 2.65s
#&gt; 207:	learn: 0.0149201	total: 693ms	remaining: 2.64s
#&gt; 208:	learn: 0.0148949	total: 696ms	remaining: 2.63s
#&gt; 209:	learn: 0.0148769	total: 698ms	remaining: 2.63s
#&gt; 210:	learn: 0.0148769	total: 698ms	remaining: 2.61s
#&gt; 211:	learn: 0.0148656	total: 700ms	remaining: 2.6s
#&gt; 212:	learn: 0.0148316	total: 704ms	remaining: 2.6s
#&gt; 213:	learn: 0.0148315	total: 706ms	remaining: 2.59s
#&gt; 214:	learn: 0.0148315	total: 707ms	remaining: 2.58s
#&gt; 215:	learn: 0.0147806	total: 714ms	remaining: 2.59s
#&gt; 216:	learn: 0.0147792	total: 716ms	remaining: 2.58s
#&gt; 217:	learn: 0.0146864	total: 719ms	remaining: 2.58s
#&gt; 218:	learn: 0.0146864	total: 719ms	remaining: 2.56s
#&gt; 219:	learn: 0.0146419	total: 727ms	remaining: 2.58s
#&gt; 220:	learn: 0.0146414	total: 729ms	remaining: 2.57s
#&gt; 221:	learn: 0.0146414	total: 730ms	remaining: 2.56s
#&gt; 222:	learn: 0.0146414	total: 732ms	remaining: 2.55s
#&gt; 223:	learn: 0.0146403	total: 734ms	remaining: 2.54s
#&gt; 224:	learn: 0.0146403	total: 734ms	remaining: 2.53s
#&gt; 225:	learn: 0.0146403	total: 734ms	remaining: 2.51s
#&gt; 226:	learn: 0.0146403	total: 734ms	remaining: 2.5s
#&gt; 227:	learn: 0.0146403	total: 736ms	remaining: 2.49s
#&gt; 228:	learn: 0.0146394	total: 739ms	remaining: 2.49s
#&gt; 229:	learn: 0.0145567	total: 744ms	remaining: 2.49s
#&gt; 230:	learn: 0.0145567	total: 744ms	remaining: 2.48s
#&gt; 231:	learn: 0.0145567	total: 744ms	remaining: 2.46s
#&gt; 232:	learn: 0.0145551	total: 746ms	remaining: 2.46s
#&gt; 233:	learn: 0.0145551	total: 748ms	remaining: 2.45s
#&gt; 234:	learn: 0.0145551	total: 748ms	remaining: 2.44s
#&gt; 235:	learn: 0.0144981	total: 758ms	remaining: 2.45s
#&gt; 236:	learn: 0.0144981	total: 759ms	remaining: 2.44s
#&gt; 237:	learn: 0.0144981	total: 759ms	remaining: 2.43s
#&gt; 238:	learn: 0.0144690	total: 763ms	remaining: 2.43s
#&gt; 239:	learn: 0.0144690	total: 763ms	remaining: 2.42s
#&gt; 240:	learn: 0.0144690	total: 763ms	remaining: 2.4s
#&gt; 241:	learn: 0.0144479	total: 771ms	remaining: 2.41s
#&gt; 242:	learn: 0.0144479	total: 771ms	remaining: 2.4s
#&gt; 243:	learn: 0.0144479	total: 771ms	remaining: 2.39s
#&gt; 244:	learn: 0.0144451	total: 774ms	remaining: 2.38s
#&gt; 245:	learn: 0.0144402	total: 778ms	remaining: 2.38s
#&gt; 246:	learn: 0.0144387	total: 780ms	remaining: 2.38s
#&gt; 247:	learn: 0.0144362	total: 780ms	remaining: 2.36s
#&gt; 248:	learn: 0.0144362	total: 780ms	remaining: 2.35s
#&gt; 249:	learn: 0.0144262	total: 784ms	remaining: 2.35s
#&gt; 250:	learn: 0.0144262	total: 787ms	remaining: 2.35s
#&gt; 251:	learn: 0.0143240	total: 805ms	remaining: 2.39s
#&gt; 252:	learn: 0.0143237	total: 807ms	remaining: 2.38s
#&gt; 253:	learn: 0.0143213	total: 810ms	remaining: 2.38s
#&gt; 254:	learn: 0.0142536	total: 830ms	remaining: 2.42s
#&gt; 255:	learn: 0.0142536	total: 831ms	remaining: 2.41s
#&gt; 256:	learn: 0.0142536	total: 831ms	remaining: 2.4s
#&gt; 257:	learn: 0.0142379	total: 846ms	remaining: 2.43s
#&gt; 258:	learn: 0.0141258	total: 867ms	remaining: 2.48s
#&gt; 259:	learn: 0.0141257	total: 872ms	remaining: 2.48s
#&gt; 260:	learn: 0.0141229	total: 882ms	remaining: 2.5s
#&gt; 261:	learn: 0.0141027	total: 889ms	remaining: 2.5s
#&gt; 262:	learn: 0.0141027	total: 889ms	remaining: 2.49s
#&gt; 263:	learn: 0.0141027	total: 891ms	remaining: 2.48s
#&gt; 264:	learn: 0.0141027	total: 892ms	remaining: 2.47s
#&gt; 265:	learn: 0.0141027	total: 892ms	remaining: 2.46s
#&gt; 266:	learn: 0.0141027	total: 892ms	remaining: 2.45s
#&gt; 267:	learn: 0.0141027	total: 892ms	remaining: 2.44s
#&gt; 268:	learn: 0.0141027	total: 892ms	remaining: 2.42s
#&gt; 269:	learn: 0.0141027	total: 893ms	remaining: 2.41s
#&gt; 270:	learn: 0.0141027	total: 893ms	remaining: 2.4s
#&gt; 271:	learn: 0.0141024	total: 895ms	remaining: 2.39s
#&gt; 272:	learn: 0.0141000	total: 897ms	remaining: 2.39s
#&gt; 273:	learn: 0.0141000	total: 897ms	remaining: 2.38s
#&gt; 274:	learn: 0.0141000	total: 897ms	remaining: 2.37s
#&gt; 275:	learn: 0.0141000	total: 898ms	remaining: 2.35s
#&gt; 276:	learn: 0.0141000	total: 898ms	remaining: 2.34s
#&gt; 277:	learn: 0.0141000	total: 898ms	remaining: 2.33s
#&gt; 278:	learn: 0.0141000	total: 898ms	remaining: 2.32s
#&gt; 279:	learn: 0.0141000	total: 899ms	remaining: 2.31s
#&gt; 280:	learn: 0.0141000	total: 899ms	remaining: 2.3s
#&gt; 281:	learn: 0.0141000	total: 899ms	remaining: 2.29s
#&gt; 282:	learn: 0.0141000	total: 899ms	remaining: 2.28s
#&gt; 283:	learn: 0.0141000	total: 901ms	remaining: 2.27s
#&gt; 284:	learn: 0.0141000	total: 902ms	remaining: 2.26s
#&gt; 285:	learn: 0.0141000	total: 902ms	remaining: 2.25s
#&gt; 286:	learn: 0.0140998	total: 904ms	remaining: 2.25s
#&gt; 287:	learn: 0.0140972	total: 904ms	remaining: 2.23s
#&gt; 288:	learn: 0.0140972	total: 906ms	remaining: 2.23s
#&gt; 289:	learn: 0.0140468	total: 912ms	remaining: 2.23s
#&gt; 290:	learn: 0.0140468	total: 912ms	remaining: 2.22s
#&gt; 291:	learn: 0.0140140	total: 920ms	remaining: 2.23s
#&gt; 292:	learn: 0.0140140	total: 921ms	remaining: 2.22s
#&gt; 293:	learn: 0.0140140	total: 921ms	remaining: 2.21s
#&gt; 294:	learn: 0.0140068	total: 923ms	remaining: 2.21s
#&gt; 295:	learn: 0.0140068	total: 925ms	remaining: 2.2s
#&gt; 296:	learn: 0.0140033	total: 928ms	remaining: 2.19s
#&gt; 297:	learn: 0.0139843	total: 933ms	remaining: 2.2s
#&gt; 298:	learn: 0.0139843	total: 933ms	remaining: 2.19s
#&gt; 299:	learn: 0.0139842	total: 935ms	remaining: 2.18s
#&gt; 300:	learn: 0.0139842	total: 936ms	remaining: 2.17s
#&gt; 301:	learn: 0.0139601	total: 938ms	remaining: 2.17s
#&gt; 302:	learn: 0.0139601	total: 938ms	remaining: 2.16s
#&gt; 303:	learn: 0.0139589	total: 942ms	remaining: 2.16s
#&gt; 304:	learn: 0.0139589	total: 942ms	remaining: 2.15s
#&gt; 305:	learn: 0.0139589	total: 942ms	remaining: 2.14s
#&gt; 306:	learn: 0.0139437	total: 952ms	remaining: 2.15s
#&gt; 307:	learn: 0.0139437	total: 952ms	remaining: 2.14s
#&gt; 308:	learn: 0.0138994	total: 958ms	remaining: 2.14s
#&gt; 309:	learn: 0.0138994	total: 959ms	remaining: 2.13s
#&gt; 310:	learn: 0.0138994	total: 959ms	remaining: 2.12s
#&gt; 311:	learn: 0.0138994	total: 959ms	remaining: 2.12s
#&gt; 312:	learn: 0.0138830	total: 965ms	remaining: 2.12s
#&gt; 313:	learn: 0.0138806	total: 966ms	remaining: 2.11s
#&gt; 314:	learn: 0.0138806	total: 966ms	remaining: 2.1s
#&gt; 315:	learn: 0.0138745	total: 981ms	remaining: 2.12s
#&gt; 316:	learn: 0.0138745	total: 981ms	remaining: 2.11s
#&gt; 317:	learn: 0.0138716	total: 986ms	remaining: 2.12s
#&gt; 318:	learn: 0.0138716	total: 987ms	remaining: 2.11s
#&gt; 319:	learn: 0.0138716	total: 987ms	remaining: 2.1s
#&gt; 320:	learn: 0.0138716	total: 987ms	remaining: 2.09s
#&gt; 321:	learn: 0.0138716	total: 988ms	remaining: 2.08s
#&gt; 322:	learn: 0.0138716	total: 988ms	remaining: 2.07s
#&gt; 323:	learn: 0.0138699	total: 991ms	remaining: 2.07s
#&gt; 324:	learn: 0.0138401	total: 1.01s	remaining: 2.11s
#&gt; 325:	learn: 0.0138295	total: 1.02s	remaining: 2.12s
#&gt; 326:	learn: 0.0138285	total: 1.02s	remaining: 2.11s
#&gt; 327:	learn: 0.0137940	total: 1.07s	remaining: 2.18s
#&gt; 328:	learn: 0.0137940	total: 1.07s	remaining: 2.17s
#&gt; 329:	learn: 0.0137940	total: 1.07s	remaining: 2.17s
#&gt; 330:	learn: 0.0137940	total: 1.07s	remaining: 2.16s
#&gt; 331:	learn: 0.0137940	total: 1.07s	remaining: 2.15s
#&gt; 332:	learn: 0.0137878	total: 1.07s	remaining: 2.15s
#&gt; 333:	learn: 0.0137690	total: 1.08s	remaining: 2.15s
#&gt; 334:	learn: 0.0137690	total: 1.08s	remaining: 2.14s
#&gt; 335:	learn: 0.0136740	total: 1.08s	remaining: 2.13s
#&gt; 336:	learn: 0.0136630	total: 1.09s	remaining: 2.14s
#&gt; 337:	learn: 0.0136630	total: 1.09s	remaining: 2.13s
#&gt; 338:	learn: 0.0136630	total: 1.09s	remaining: 2.13s
#&gt; 339:	learn: 0.0136630	total: 1.09s	remaining: 2.12s
#&gt; 340:	learn: 0.0136630	total: 1.09s	remaining: 2.11s
#&gt; 341:	learn: 0.0136630	total: 1.09s	remaining: 2.1s
#&gt; 342:	learn: 0.0136610	total: 1.09s	remaining: 2.09s
#&gt; 343:	learn: 0.0136586	total: 1.09s	remaining: 2.08s
#&gt; 344:	learn: 0.0136586	total: 1.09s	remaining: 2.08s
#&gt; 345:	learn: 0.0136343	total: 1.1s	remaining: 2.08s
#&gt; 346:	learn: 0.0136342	total: 1.1s	remaining: 2.07s
#&gt; 347:	learn: 0.0136335	total: 1.1s	remaining: 2.07s
#&gt; 348:	learn: 0.0136179	total: 1.11s	remaining: 2.07s
#&gt; 349:	learn: 0.0136156	total: 1.11s	remaining: 2.07s
#&gt; 350:	learn: 0.0136067	total: 1.12s	remaining: 2.07s
#&gt; 351:	learn: 0.0136046	total: 1.12s	remaining: 2.06s
#&gt; 352:	learn: 0.0136046	total: 1.12s	remaining: 2.05s
#&gt; 353:	learn: 0.0135611	total: 1.13s	remaining: 2.06s
#&gt; 354:	learn: 0.0135471	total: 1.13s	remaining: 2.05s
#&gt; 355:	learn: 0.0133987	total: 1.14s	remaining: 2.06s
#&gt; 356:	learn: 0.0133987	total: 1.14s	remaining: 2.05s
#&gt; 357:	learn: 0.0133987	total: 1.14s	remaining: 2.04s
#&gt; 358:	learn: 0.0133987	total: 1.14s	remaining: 2.03s
#&gt; 359:	learn: 0.0133980	total: 1.14s	remaining: 2.03s
#&gt; 360:	learn: 0.0133964	total: 1.14s	remaining: 2.02s
#&gt; 361:	learn: 0.0133964	total: 1.14s	remaining: 2.01s
#&gt; 362:	learn: 0.0133964	total: 1.14s	remaining: 2.01s
#&gt; 363:	learn: 0.0133964	total: 1.14s	remaining: 2s
#&gt; 364:	learn: 0.0133964	total: 1.14s	remaining: 1.99s
#&gt; 365:	learn: 0.0133964	total: 1.15s	remaining: 1.98s
#&gt; 366:	learn: 0.0133942	total: 1.15s	remaining: 1.98s
#&gt; 367:	learn: 0.0133936	total: 1.15s	remaining: 1.97s
#&gt; 368:	learn: 0.0133825	total: 1.15s	remaining: 1.97s
#&gt; 369:	learn: 0.0133825	total: 1.15s	remaining: 1.96s
#&gt; 370:	learn: 0.0133825	total: 1.15s	remaining: 1.96s
#&gt; 371:	learn: 0.0133825	total: 1.15s	remaining: 1.95s
#&gt; 372:	learn: 0.0133825	total: 1.15s	remaining: 1.94s
#&gt; 373:	learn: 0.0133822	total: 1.16s	remaining: 1.94s
#&gt; 374:	learn: 0.0133227	total: 1.16s	remaining: 1.94s
#&gt; 375:	learn: 0.0133227	total: 1.16s	remaining: 1.93s
#&gt; 376:	learn: 0.0133218	total: 1.16s	remaining: 1.92s
#&gt; 377:	learn: 0.0133218	total: 1.17s	remaining: 1.92s
#&gt; 378:	learn: 0.0133218	total: 1.17s	remaining: 1.91s
#&gt; 379:	learn: 0.0133194	total: 1.17s	remaining: 1.9s
#&gt; 380:	learn: 0.0132936	total: 1.17s	remaining: 1.9s
#&gt; 381:	learn: 0.0132936	total: 1.17s	remaining: 1.9s
#&gt; 382:	learn: 0.0132497	total: 1.18s	remaining: 1.9s
#&gt; 383:	learn: 0.0132497	total: 1.18s	remaining: 1.89s
#&gt; 384:	learn: 0.0132430	total: 1.18s	remaining: 1.89s
#&gt; 385:	learn: 0.0132122	total: 1.19s	remaining: 1.89s
#&gt; 386:	learn: 0.0130901	total: 1.2s	remaining: 1.9s
#&gt; 387:	learn: 0.0130901	total: 1.2s	remaining: 1.9s
#&gt; 388:	learn: 0.0130901	total: 1.2s	remaining: 1.89s
#&gt; 389:	learn: 0.0130726	total: 1.23s	remaining: 1.92s
#&gt; 390:	learn: 0.0129377	total: 1.24s	remaining: 1.93s
#&gt; 391:	learn: 0.0129369	total: 1.24s	remaining: 1.92s
#&gt; 392:	learn: 0.0129369	total: 1.24s	remaining: 1.92s
#&gt; 393:	learn: 0.0129369	total: 1.24s	remaining: 1.91s
#&gt; 394:	learn: 0.0129348	total: 1.24s	remaining: 1.91s
#&gt; 395:	learn: 0.0129341	total: 1.25s	remaining: 1.91s
#&gt; 396:	learn: 0.0129023	total: 1.26s	remaining: 1.92s
#&gt; 397:	learn: 0.0129005	total: 1.27s	remaining: 1.92s
#&gt; 398:	learn: 0.0128973	total: 1.27s	remaining: 1.91s
#&gt; 399:	learn: 0.0128169	total: 1.28s	remaining: 1.92s
#&gt; 400:	learn: 0.0128157	total: 1.28s	remaining: 1.92s
#&gt; 401:	learn: 0.0128157	total: 1.28s	remaining: 1.91s
#&gt; 402:	learn: 0.0128157	total: 1.28s	remaining: 1.9s
#&gt; 403:	learn: 0.0128157	total: 1.28s	remaining: 1.9s
#&gt; 404:	learn: 0.0127995	total: 1.29s	remaining: 1.9s
#&gt; 405:	learn: 0.0127995	total: 1.29s	remaining: 1.89s
#&gt; 406:	learn: 0.0127956	total: 1.3s	remaining: 1.89s
#&gt; 407:	learn: 0.0127719	total: 1.3s	remaining: 1.89s
#&gt; 408:	learn: 0.0127719	total: 1.3s	remaining: 1.88s
#&gt; 409:	learn: 0.0127719	total: 1.3s	remaining: 1.87s
#&gt; 410:	learn: 0.0127371	total: 1.31s	remaining: 1.87s
#&gt; 411:	learn: 0.0127152	total: 1.31s	remaining: 1.87s
#&gt; 412:	learn: 0.0127152	total: 1.31s	remaining: 1.86s
#&gt; 413:	learn: 0.0127143	total: 1.31s	remaining: 1.86s
#&gt; 414:	learn: 0.0127143	total: 1.31s	remaining: 1.85s
#&gt; 415:	learn: 0.0127143	total: 1.31s	remaining: 1.84s
#&gt; 416:	learn: 0.0127143	total: 1.31s	remaining: 1.83s
#&gt; 417:	learn: 0.0126912	total: 1.32s	remaining: 1.83s
#&gt; 418:	learn: 0.0126850	total: 1.32s	remaining: 1.83s
#&gt; 419:	learn: 0.0126850	total: 1.32s	remaining: 1.82s
#&gt; 420:	learn: 0.0126410	total: 1.32s	remaining: 1.82s
#&gt; 421:	learn: 0.0126410	total: 1.32s	remaining: 1.81s
#&gt; 422:	learn: 0.0126410	total: 1.32s	remaining: 1.8s
#&gt; 423:	learn: 0.0126290	total: 1.33s	remaining: 1.8s
#&gt; 424:	learn: 0.0126290	total: 1.33s	remaining: 1.79s
#&gt; 425:	learn: 0.0126290	total: 1.33s	remaining: 1.79s
#&gt; 426:	learn: 0.0126290	total: 1.33s	remaining: 1.78s
#&gt; 427:	learn: 0.0126290	total: 1.33s	remaining: 1.78s
#&gt; 428:	learn: 0.0126290	total: 1.33s	remaining: 1.77s
#&gt; 429:	learn: 0.0126290	total: 1.33s	remaining: 1.76s
#&gt; 430:	learn: 0.0126290	total: 1.33s	remaining: 1.76s
#&gt; 431:	learn: 0.0126109	total: 1.33s	remaining: 1.75s
#&gt; 432:	learn: 0.0126109	total: 1.33s	remaining: 1.75s
#&gt; 433:	learn: 0.0125960	total: 1.34s	remaining: 1.74s
#&gt; 434:	learn: 0.0124661	total: 1.34s	remaining: 1.74s
#&gt; 435:	learn: 0.0123375	total: 1.35s	remaining: 1.74s
#&gt; 436:	learn: 0.0123375	total: 1.35s	remaining: 1.74s
#&gt; 437:	learn: 0.0123274	total: 1.35s	remaining: 1.74s
#&gt; 438:	learn: 0.0123274	total: 1.35s	remaining: 1.73s
#&gt; 439:	learn: 0.0123274	total: 1.35s	remaining: 1.72s
#&gt; 440:	learn: 0.0122848	total: 1.36s	remaining: 1.72s
#&gt; 441:	learn: 0.0122848	total: 1.36s	remaining: 1.72s
#&gt; 442:	learn: 0.0122848	total: 1.36s	remaining: 1.71s
#&gt; 443:	learn: 0.0122802	total: 1.36s	remaining: 1.71s
#&gt; 444:	learn: 0.0122783	total: 1.36s	remaining: 1.7s
#&gt; 445:	learn: 0.0122783	total: 1.36s	remaining: 1.69s
#&gt; 446:	learn: 0.0122783	total: 1.36s	remaining: 1.69s
#&gt; 447:	learn: 0.0122783	total: 1.36s	remaining: 1.68s
#&gt; 448:	learn: 0.0122742	total: 1.37s	remaining: 1.68s
#&gt; 449:	learn: 0.0122742	total: 1.37s	remaining: 1.67s
#&gt; 450:	learn: 0.0122742	total: 1.37s	remaining: 1.67s
#&gt; 451:	learn: 0.0122364	total: 1.38s	remaining: 1.67s
#&gt; 452:	learn: 0.0122364	total: 1.38s	remaining: 1.66s
#&gt; 453:	learn: 0.0121785	total: 1.38s	remaining: 1.66s
#&gt; 454:	learn: 0.0121324	total: 1.39s	remaining: 1.66s
#&gt; 455:	learn: 0.0121307	total: 1.39s	remaining: 1.66s
#&gt; 456:	learn: 0.0121307	total: 1.39s	remaining: 1.65s
#&gt; 457:	learn: 0.0121306	total: 1.39s	remaining: 1.65s
#&gt; 458:	learn: 0.0121306	total: 1.39s	remaining: 1.64s
#&gt; 459:	learn: 0.0121273	total: 1.39s	remaining: 1.64s
#&gt; 460:	learn: 0.0121273	total: 1.39s	remaining: 1.63s
#&gt; 461:	learn: 0.0121136	total: 1.4s	remaining: 1.62s
#&gt; 462:	learn: 0.0120536	total: 1.4s	remaining: 1.62s
#&gt; 463:	learn: 0.0120536	total: 1.4s	remaining: 1.61s
#&gt; 464:	learn: 0.0120276	total: 1.4s	remaining: 1.61s
#&gt; 465:	learn: 0.0120276	total: 1.4s	remaining: 1.61s
#&gt; 466:	learn: 0.0119840	total: 1.41s	remaining: 1.61s
#&gt; 467:	learn: 0.0119840	total: 1.42s	remaining: 1.62s
#&gt; 468:	learn: 0.0119828	total: 1.42s	remaining: 1.61s
#&gt; 469:	learn: 0.0119828	total: 1.42s	remaining: 1.61s
#&gt; 470:	learn: 0.0119669	total: 1.45s	remaining: 1.62s
#&gt; 471:	learn: 0.0119669	total: 1.45s	remaining: 1.62s
#&gt; 472:	learn: 0.0119669	total: 1.45s	remaining: 1.61s
#&gt; 473:	learn: 0.0119480	total: 1.45s	remaining: 1.61s
#&gt; 474:	learn: 0.0119480	total: 1.45s	remaining: 1.6s
#&gt; 475:	learn: 0.0119480	total: 1.45s	remaining: 1.6s
#&gt; 476:	learn: 0.0119480	total: 1.45s	remaining: 1.59s
#&gt; 477:	learn: 0.0119297	total: 1.46s	remaining: 1.59s
#&gt; 478:	learn: 0.0119297	total: 1.46s	remaining: 1.58s
#&gt; 479:	learn: 0.0118906	total: 1.47s	remaining: 1.59s
#&gt; 480:	learn: 0.0118867	total: 1.47s	remaining: 1.59s
#&gt; 481:	learn: 0.0118867	total: 1.47s	remaining: 1.58s
#&gt; 482:	learn: 0.0118867	total: 1.47s	remaining: 1.58s
#&gt; 483:	learn: 0.0118822	total: 1.48s	remaining: 1.58s
#&gt; 484:	learn: 0.0118822	total: 1.48s	remaining: 1.57s
#&gt; 485:	learn: 0.0118813	total: 1.48s	remaining: 1.57s
#&gt; 486:	learn: 0.0118806	total: 1.48s	remaining: 1.56s
#&gt; 487:	learn: 0.0118778	total: 1.49s	remaining: 1.56s
#&gt; 488:	learn: 0.0118743	total: 1.49s	remaining: 1.55s
#&gt; 489:	learn: 0.0118489	total: 1.49s	remaining: 1.55s
#&gt; 490:	learn: 0.0118488	total: 1.5s	remaining: 1.55s
#&gt; 491:	learn: 0.0118488	total: 1.5s	remaining: 1.54s
#&gt; 492:	learn: 0.0118448	total: 1.5s	remaining: 1.54s
#&gt; 493:	learn: 0.0118447	total: 1.5s	remaining: 1.53s
#&gt; 494:	learn: 0.0118429	total: 1.5s	remaining: 1.53s
#&gt; 495:	learn: 0.0118418	total: 1.5s	remaining: 1.53s
#&gt; 496:	learn: 0.0118418	total: 1.5s	remaining: 1.52s
#&gt; 497:	learn: 0.0118418	total: 1.5s	remaining: 1.52s
#&gt; 498:	learn: 0.0118322	total: 1.51s	remaining: 1.51s
#&gt; 499:	learn: 0.0118322	total: 1.51s	remaining: 1.51s
#&gt; 500:	learn: 0.0118322	total: 1.51s	remaining: 1.5s
#&gt; 501:	learn: 0.0118041	total: 1.51s	remaining: 1.5s
#&gt; 502:	learn: 0.0118041	total: 1.51s	remaining: 1.5s
#&gt; 503:	learn: 0.0118041	total: 1.51s	remaining: 1.49s
#&gt; 504:	learn: 0.0118041	total: 1.51s	remaining: 1.49s
#&gt; 505:	learn: 0.0118019	total: 1.52s	remaining: 1.49s
#&gt; 506:	learn: 0.0118019	total: 1.52s	remaining: 1.48s
#&gt; 507:	learn: 0.0118019	total: 1.52s	remaining: 1.47s
#&gt; 508:	learn: 0.0118019	total: 1.52s	remaining: 1.47s
#&gt; 509:	learn: 0.0117987	total: 1.53s	remaining: 1.47s
#&gt; 510:	learn: 0.0117363	total: 1.53s	remaining: 1.47s
#&gt; 511:	learn: 0.0117363	total: 1.53s	remaining: 1.46s
#&gt; 512:	learn: 0.0117357	total: 1.54s	remaining: 1.46s
#&gt; 513:	learn: 0.0117340	total: 1.54s	remaining: 1.46s
#&gt; 514:	learn: 0.0117340	total: 1.54s	remaining: 1.45s
#&gt; 515:	learn: 0.0117256	total: 1.55s	remaining: 1.45s
#&gt; 516:	learn: 0.0117211	total: 1.55s	remaining: 1.45s
#&gt; 517:	learn: 0.0117194	total: 1.55s	remaining: 1.44s
#&gt; 518:	learn: 0.0117194	total: 1.55s	remaining: 1.44s
#&gt; 519:	learn: 0.0117194	total: 1.55s	remaining: 1.43s
#&gt; 520:	learn: 0.0117194	total: 1.55s	remaining: 1.43s
#&gt; 521:	learn: 0.0117176	total: 1.55s	remaining: 1.42s
#&gt; 522:	learn: 0.0117176	total: 1.55s	remaining: 1.42s
#&gt; 523:	learn: 0.0117047	total: 1.56s	remaining: 1.42s
#&gt; 524:	learn: 0.0117033	total: 1.56s	remaining: 1.41s
#&gt; 525:	learn: 0.0117016	total: 1.56s	remaining: 1.41s
#&gt; 526:	learn: 0.0116564	total: 1.57s	remaining: 1.41s
#&gt; 527:	learn: 0.0116564	total: 1.57s	remaining: 1.41s
#&gt; 528:	learn: 0.0116437	total: 1.58s	remaining: 1.4s
#&gt; 529:	learn: 0.0116414	total: 1.58s	remaining: 1.4s
#&gt; 530:	learn: 0.0116322	total: 1.58s	remaining: 1.4s
#&gt; 531:	learn: 0.0116322	total: 1.58s	remaining: 1.39s
#&gt; 532:	learn: 0.0116189	total: 1.58s	remaining: 1.39s
#&gt; 533:	learn: 0.0116046	total: 1.62s	remaining: 1.42s
#&gt; 534:	learn: 0.0115822	total: 1.63s	remaining: 1.41s
#&gt; 535:	learn: 0.0115706	total: 1.63s	remaining: 1.41s
#&gt; 536:	learn: 0.0115597	total: 1.63s	remaining: 1.41s
#&gt; 537:	learn: 0.0115574	total: 1.63s	remaining: 1.4s
#&gt; 538:	learn: 0.0115574	total: 1.63s	remaining: 1.4s
#&gt; 539:	learn: 0.0115574	total: 1.63s	remaining: 1.39s
#&gt; 540:	learn: 0.0115574	total: 1.63s	remaining: 1.39s
#&gt; 541:	learn: 0.0115574	total: 1.63s	remaining: 1.38s
#&gt; 542:	learn: 0.0115574	total: 1.63s	remaining: 1.38s
#&gt; 543:	learn: 0.0115560	total: 1.64s	remaining: 1.37s
#&gt; 544:	learn: 0.0115511	total: 1.64s	remaining: 1.37s
#&gt; 545:	learn: 0.0115511	total: 1.65s	remaining: 1.37s
#&gt; 546:	learn: 0.0115504	total: 1.65s	remaining: 1.36s
#&gt; 547:	learn: 0.0115169	total: 1.66s	remaining: 1.37s
#&gt; 548:	learn: 0.0115154	total: 1.66s	remaining: 1.36s
#&gt; 549:	learn: 0.0115137	total: 1.66s	remaining: 1.36s
#&gt; 550:	learn: 0.0115121	total: 1.66s	remaining: 1.35s
#&gt; 551:	learn: 0.0115091	total: 1.67s	remaining: 1.35s
#&gt; 552:	learn: 0.0115077	total: 1.67s	remaining: 1.35s
#&gt; 553:	learn: 0.0115077	total: 1.67s	remaining: 1.34s
#&gt; 554:	learn: 0.0114880	total: 1.67s	remaining: 1.34s
#&gt; 555:	learn: 0.0114880	total: 1.67s	remaining: 1.33s
#&gt; 556:	learn: 0.0114880	total: 1.67s	remaining: 1.33s
#&gt; 557:	learn: 0.0114880	total: 1.67s	remaining: 1.32s
#&gt; 558:	learn: 0.0114800	total: 1.68s	remaining: 1.32s
#&gt; 559:	learn: 0.0114800	total: 1.68s	remaining: 1.32s
#&gt; 560:	learn: 0.0114197	total: 1.68s	remaining: 1.32s
#&gt; 561:	learn: 0.0114197	total: 1.68s	remaining: 1.31s
#&gt; 562:	learn: 0.0114197	total: 1.68s	remaining: 1.31s
#&gt; 563:	learn: 0.0114143	total: 1.69s	remaining: 1.3s
#&gt; 564:	learn: 0.0113536	total: 1.69s	remaining: 1.3s
#&gt; 565:	learn: 0.0113419	total: 1.69s	remaining: 1.3s
#&gt; 566:	learn: 0.0113400	total: 1.7s	remaining: 1.29s
#&gt; 567:	learn: 0.0113307	total: 1.7s	remaining: 1.29s
#&gt; 568:	learn: 0.0113307	total: 1.7s	remaining: 1.28s
#&gt; 569:	learn: 0.0113307	total: 1.7s	remaining: 1.28s
#&gt; 570:	learn: 0.0113304	total: 1.7s	remaining: 1.28s
#&gt; 571:	learn: 0.0113304	total: 1.7s	remaining: 1.27s
#&gt; 572:	learn: 0.0113304	total: 1.7s	remaining: 1.27s
#&gt; 573:	learn: 0.0113304	total: 1.7s	remaining: 1.26s
#&gt; 574:	learn: 0.0113291	total: 1.71s	remaining: 1.26s
#&gt; 575:	learn: 0.0113291	total: 1.71s	remaining: 1.25s
#&gt; 576:	learn: 0.0113283	total: 1.71s	remaining: 1.25s
#&gt; 577:	learn: 0.0113283	total: 1.71s	remaining: 1.25s
#&gt; 578:	learn: 0.0113232	total: 1.72s	remaining: 1.25s
#&gt; 579:	learn: 0.0113232	total: 1.72s	remaining: 1.24s
#&gt; 580:	learn: 0.0113217	total: 1.72s	remaining: 1.24s
#&gt; 581:	learn: 0.0113098	total: 1.73s	remaining: 1.24s
#&gt; 582:	learn: 0.0112639	total: 1.73s	remaining: 1.24s
#&gt; 583:	learn: 0.0112537	total: 1.73s	remaining: 1.23s
#&gt; 584:	learn: 0.0111878	total: 1.74s	remaining: 1.23s
#&gt; 585:	learn: 0.0111878	total: 1.74s	remaining: 1.23s
#&gt; 586:	learn: 0.0111878	total: 1.74s	remaining: 1.22s
#&gt; 587:	learn: 0.0111878	total: 1.74s	remaining: 1.22s
#&gt; 588:	learn: 0.0111831	total: 1.74s	remaining: 1.21s
#&gt; 589:	learn: 0.0111831	total: 1.74s	remaining: 1.21s
#&gt; 590:	learn: 0.0111831	total: 1.74s	remaining: 1.21s
#&gt; 591:	learn: 0.0110985	total: 1.75s	remaining: 1.21s
#&gt; 592:	learn: 0.0110985	total: 1.75s	remaining: 1.2s
#&gt; 593:	learn: 0.0110982	total: 1.75s	remaining: 1.2s
#&gt; 594:	learn: 0.0110982	total: 1.75s	remaining: 1.19s
#&gt; 595:	learn: 0.0110929	total: 1.76s	remaining: 1.19s
#&gt; 596:	learn: 0.0110929	total: 1.76s	remaining: 1.19s
#&gt; 597:	learn: 0.0110929	total: 1.76s	remaining: 1.18s
#&gt; 598:	learn: 0.0110929	total: 1.76s	remaining: 1.18s
#&gt; 599:	learn: 0.0110917	total: 1.76s	remaining: 1.17s
#&gt; 600:	learn: 0.0110846	total: 1.76s	remaining: 1.17s
#&gt; 601:	learn: 0.0110831	total: 1.77s	remaining: 1.17s
#&gt; 602:	learn: 0.0110629	total: 1.77s	remaining: 1.16s
#&gt; 603:	learn: 0.0110629	total: 1.77s	remaining: 1.16s
#&gt; 604:	learn: 0.0110629	total: 1.77s	remaining: 1.15s
#&gt; 605:	learn: 0.0110535	total: 1.77s	remaining: 1.15s
#&gt; 606:	learn: 0.0110494	total: 1.78s	remaining: 1.15s
#&gt; 607:	learn: 0.0110480	total: 1.78s	remaining: 1.15s
#&gt; 608:	learn: 0.0110399	total: 1.78s	remaining: 1.14s
#&gt; 609:	learn: 0.0110260	total: 1.78s	remaining: 1.14s
#&gt; 610:	learn: 0.0109539	total: 1.8s	remaining: 1.15s
#&gt; 611:	learn: 0.0109539	total: 1.81s	remaining: 1.15s
#&gt; 612:	learn: 0.0109444	total: 1.81s	remaining: 1.14s
#&gt; 613:	learn: 0.0109294	total: 1.82s	remaining: 1.14s
#&gt; 614:	learn: 0.0109294	total: 1.82s	remaining: 1.14s
#&gt; 615:	learn: 0.0109294	total: 1.82s	remaining: 1.13s
#&gt; 616:	learn: 0.0109294	total: 1.82s	remaining: 1.13s
#&gt; 617:	learn: 0.0109294	total: 1.82s	remaining: 1.13s
#&gt; 618:	learn: 0.0109294	total: 1.82s	remaining: 1.12s
#&gt; 619:	learn: 0.0109258	total: 1.82s	remaining: 1.12s
#&gt; 620:	learn: 0.0109149	total: 1.83s	remaining: 1.12s
#&gt; 621:	learn: 0.0108999	total: 1.83s	remaining: 1.11s
#&gt; 622:	learn: 0.0108999	total: 1.83s	remaining: 1.11s
#&gt; 623:	learn: 0.0108999	total: 1.83s	remaining: 1.11s
#&gt; 624:	learn: 0.0108983	total: 1.83s	remaining: 1.1s
#&gt; 625:	learn: 0.0108983	total: 1.84s	remaining: 1.1s
#&gt; 626:	learn: 0.0108983	total: 1.84s	remaining: 1.09s
#&gt; 627:	learn: 0.0108979	total: 1.84s	remaining: 1.09s
#&gt; 628:	learn: 0.0108979	total: 1.84s	remaining: 1.08s
#&gt; 629:	learn: 0.0108938	total: 1.85s	remaining: 1.08s
#&gt; 630:	learn: 0.0108938	total: 1.85s	remaining: 1.08s
#&gt; 631:	learn: 0.0108938	total: 1.85s	remaining: 1.08s
#&gt; 632:	learn: 0.0108026	total: 1.85s	remaining: 1.07s
#&gt; 633:	learn: 0.0108015	total: 1.86s	remaining: 1.07s
#&gt; 634:	learn: 0.0108011	total: 1.86s	remaining: 1.07s
#&gt; 635:	learn: 0.0107992	total: 1.86s	remaining: 1.06s
#&gt; 636:	learn: 0.0107559	total: 1.87s	remaining: 1.06s
#&gt; 637:	learn: 0.0107380	total: 1.88s	remaining: 1.06s
#&gt; 638:	learn: 0.0107380	total: 1.88s	remaining: 1.06s
#&gt; 639:	learn: 0.0107380	total: 1.88s	remaining: 1.06s
#&gt; 640:	learn: 0.0107380	total: 1.88s	remaining: 1.05s
#&gt; 641:	learn: 0.0107380	total: 1.88s	remaining: 1.05s
#&gt; 642:	learn: 0.0107380	total: 1.88s	remaining: 1.04s
#&gt; 643:	learn: 0.0107247	total: 1.88s	remaining: 1.04s
#&gt; 644:	learn: 0.0107219	total: 1.89s	remaining: 1.04s
#&gt; 645:	learn: 0.0106998	total: 1.89s	remaining: 1.04s
#&gt; 646:	learn: 0.0106868	total: 1.89s	remaining: 1.03s
#&gt; 647:	learn: 0.0106868	total: 1.9s	remaining: 1.03s
#&gt; 648:	learn: 0.0106867	total: 1.9s	remaining: 1.03s
#&gt; 649:	learn: 0.0106848	total: 1.9s	remaining: 1.02s
#&gt; 650:	learn: 0.0106696	total: 1.9s	remaining: 1.02s
#&gt; 651:	learn: 0.0106696	total: 1.9s	remaining: 1.01s
#&gt; 652:	learn: 0.0106687	total: 1.9s	remaining: 1.01s
#&gt; 653:	learn: 0.0106679	total: 1.93s	remaining: 1.02s
#&gt; 654:	learn: 0.0106679	total: 1.93s	remaining: 1.01s
#&gt; 655:	learn: 0.0106679	total: 1.93s	remaining: 1.01s
#&gt; 656:	learn: 0.0106670	total: 1.93s	remaining: 1.01s
#&gt; 657:	learn: 0.0106670	total: 1.93s	remaining: 1s
#&gt; 658:	learn: 0.0106647	total: 1.93s	remaining: 999ms
#&gt; 659:	learn: 0.0106645	total: 1.93s	remaining: 995ms
#&gt; 660:	learn: 0.0106487	total: 1.94s	remaining: 995ms
#&gt; 661:	learn: 0.0106487	total: 1.94s	remaining: 991ms
#&gt; 662:	learn: 0.0106471	total: 1.94s	remaining: 986ms
#&gt; 663:	learn: 0.0106374	total: 1.94s	remaining: 984ms
#&gt; 664:	learn: 0.0106374	total: 1.95s	remaining: 980ms
#&gt; 665:	learn: 0.0106265	total: 1.95s	remaining: 978ms
#&gt; 666:	learn: 0.0106233	total: 1.95s	remaining: 974ms
#&gt; 667:	learn: 0.0106229	total: 1.95s	remaining: 971ms
#&gt; 668:	learn: 0.0106229	total: 1.95s	remaining: 967ms
#&gt; 669:	learn: 0.0106229	total: 1.95s	remaining: 963ms
#&gt; 670:	learn: 0.0106229	total: 1.95s	remaining: 958ms
#&gt; 671:	learn: 0.0106229	total: 1.96s	remaining: 954ms
#&gt; 672:	learn: 0.0106229	total: 1.96s	remaining: 950ms
#&gt; 673:	learn: 0.0106221	total: 1.97s	remaining: 953ms
#&gt; 674:	learn: 0.0106221	total: 1.97s	remaining: 949ms
#&gt; 675:	learn: 0.0106221	total: 1.97s	remaining: 945ms
#&gt; 676:	learn: 0.0106221	total: 1.97s	remaining: 941ms
#&gt; 677:	learn: 0.0106219	total: 1.97s	remaining: 936ms
#&gt; 678:	learn: 0.0106217	total: 1.98s	remaining: 935ms
#&gt; 679:	learn: 0.0106217	total: 1.98s	remaining: 931ms
#&gt; 680:	learn: 0.0105648	total: 1.98s	remaining: 928ms
#&gt; 681:	learn: 0.0105616	total: 1.98s	remaining: 925ms
#&gt; 682:	learn: 0.0105610	total: 1.99s	remaining: 923ms
#&gt; 683:	learn: 0.0105610	total: 1.99s	remaining: 919ms
#&gt; 684:	learn: 0.0105610	total: 1.99s	remaining: 915ms
#&gt; 685:	learn: 0.0105534	total: 2.03s	remaining: 930ms
#&gt; 686:	learn: 0.0105534	total: 2.03s	remaining: 927ms
#&gt; 687:	learn: 0.0105527	total: 2.03s	remaining: 922ms
#&gt; 688:	learn: 0.0105396	total: 2.04s	remaining: 920ms
#&gt; 689:	learn: 0.0104704	total: 2.04s	remaining: 918ms
#&gt; 690:	learn: 0.0104622	total: 2.05s	remaining: 916ms
#&gt; 691:	learn: 0.0104622	total: 2.05s	remaining: 912ms
#&gt; 692:	learn: 0.0104622	total: 2.05s	remaining: 908ms
#&gt; 693:	learn: 0.0104615	total: 2.05s	remaining: 905ms
#&gt; 694:	learn: 0.0104614	total: 2.05s	remaining: 901ms
#&gt; 695:	learn: 0.0104614	total: 2.06s	remaining: 898ms
#&gt; 696:	learn: 0.0104234	total: 2.06s	remaining: 897ms
#&gt; 697:	learn: 0.0104232	total: 2.06s	remaining: 894ms
#&gt; 698:	learn: 0.0104221	total: 2.06s	remaining: 890ms
#&gt; 699:	learn: 0.0104221	total: 2.07s	remaining: 885ms
#&gt; 700:	learn: 0.0104221	total: 2.07s	remaining: 882ms
#&gt; 701:	learn: 0.0104221	total: 2.07s	remaining: 878ms
#&gt; 702:	learn: 0.0104124	total: 2.07s	remaining: 875ms
#&gt; 703:	learn: 0.0104109	total: 2.07s	remaining: 871ms
#&gt; 704:	learn: 0.0104104	total: 2.07s	remaining: 867ms
#&gt; 705:	learn: 0.0104104	total: 2.07s	remaining: 863ms
#&gt; 706:	learn: 0.0104104	total: 2.07s	remaining: 859ms
#&gt; 707:	learn: 0.0104104	total: 2.07s	remaining: 855ms
#&gt; 708:	learn: 0.0103858	total: 2.08s	remaining: 853ms
#&gt; 709:	learn: 0.0103858	total: 2.08s	remaining: 849ms
#&gt; 710:	learn: 0.0103858	total: 2.08s	remaining: 845ms
#&gt; 711:	learn: 0.0103858	total: 2.08s	remaining: 841ms
#&gt; 712:	learn: 0.0103858	total: 2.08s	remaining: 837ms
#&gt; 713:	learn: 0.0103854	total: 2.08s	remaining: 833ms
#&gt; 714:	learn: 0.0103834	total: 2.08s	remaining: 830ms
#&gt; 715:	learn: 0.0102855	total: 2.08s	remaining: 827ms
#&gt; 716:	learn: 0.0102855	total: 2.08s	remaining: 823ms
#&gt; 717:	learn: 0.0102832	total: 2.09s	remaining: 820ms
#&gt; 718:	learn: 0.0102827	total: 2.09s	remaining: 817ms
#&gt; 719:	learn: 0.0102824	total: 2.09s	remaining: 814ms
#&gt; 720:	learn: 0.0102680	total: 2.1s	remaining: 812ms
#&gt; 721:	learn: 0.0102680	total: 2.1s	remaining: 809ms
#&gt; 722:	learn: 0.0102670	total: 2.1s	remaining: 805ms
#&gt; 723:	learn: 0.0102670	total: 2.1s	remaining: 801ms
#&gt; 724:	learn: 0.0102670	total: 2.1s	remaining: 797ms
#&gt; 725:	learn: 0.0102670	total: 2.1s	remaining: 794ms
#&gt; 726:	learn: 0.0102670	total: 2.1s	remaining: 791ms
#&gt; 727:	learn: 0.0102624	total: 2.11s	remaining: 788ms
#&gt; 728:	learn: 0.0102624	total: 2.11s	remaining: 784ms
#&gt; 729:	learn: 0.0101983	total: 2.12s	remaining: 783ms
#&gt; 730:	learn: 0.0101973	total: 2.12s	remaining: 779ms
#&gt; 731:	learn: 0.0101973	total: 2.12s	remaining: 775ms
#&gt; 732:	learn: 0.0101973	total: 2.12s	remaining: 772ms
#&gt; 733:	learn: 0.0101719	total: 2.13s	remaining: 771ms
#&gt; 734:	learn: 0.0101719	total: 2.13s	remaining: 767ms
#&gt; 735:	learn: 0.0101719	total: 2.13s	remaining: 763ms
#&gt; 736:	learn: 0.0101719	total: 2.13s	remaining: 759ms
#&gt; 737:	learn: 0.0101719	total: 2.13s	remaining: 755ms
#&gt; 738:	learn: 0.0101719	total: 2.13s	remaining: 752ms
#&gt; 739:	learn: 0.0101364	total: 2.13s	remaining: 749ms
#&gt; 740:	learn: 0.0101364	total: 2.13s	remaining: 746ms
#&gt; 741:	learn: 0.0101364	total: 2.13s	remaining: 742ms
#&gt; 742:	learn: 0.0101284	total: 2.14s	remaining: 741ms
#&gt; 743:	learn: 0.0101282	total: 2.14s	remaining: 737ms
#&gt; 744:	learn: 0.0101237	total: 2.14s	remaining: 734ms
#&gt; 745:	learn: 0.0101237	total: 2.14s	remaining: 730ms
#&gt; 746:	learn: 0.0101237	total: 2.14s	remaining: 726ms
#&gt; 747:	learn: 0.0101087	total: 2.15s	remaining: 725ms
#&gt; 748:	learn: 0.0101078	total: 2.15s	remaining: 722ms
#&gt; 749:	learn: 0.0100898	total: 2.16s	remaining: 720ms
#&gt; 750:	learn: 0.0100898	total: 2.16s	remaining: 717ms
#&gt; 751:	learn: 0.0100897	total: 2.16s	remaining: 713ms
#&gt; 752:	learn: 0.0100897	total: 2.16s	remaining: 710ms
#&gt; 753:	learn: 0.0100771	total: 2.17s	remaining: 709ms
#&gt; 754:	learn: 0.0100771	total: 2.17s	remaining: 705ms
#&gt; 755:	learn: 0.0100771	total: 2.17s	remaining: 701ms
#&gt; 756:	learn: 0.0100771	total: 2.17s	remaining: 698ms
#&gt; 757:	learn: 0.0100771	total: 2.17s	remaining: 694ms
#&gt; 758:	learn: 0.0100771	total: 2.17s	remaining: 690ms
#&gt; 759:	learn: 0.0100757	total: 2.17s	remaining: 687ms
#&gt; 760:	learn: 0.0100757	total: 2.18s	remaining: 683ms
#&gt; 761:	learn: 0.0100753	total: 2.18s	remaining: 681ms
#&gt; 762:	learn: 0.0100660	total: 2.19s	remaining: 679ms
#&gt; 763:	learn: 0.0100457	total: 2.19s	remaining: 675ms
#&gt; 764:	learn: 0.0100457	total: 2.19s	remaining: 672ms
#&gt; 765:	learn: 0.0100424	total: 2.19s	remaining: 668ms
#&gt; 766:	learn: 0.0100424	total: 2.19s	remaining: 665ms
#&gt; 767:	learn: 0.0100424	total: 2.19s	remaining: 661ms
#&gt; 768:	learn: 0.0100339	total: 2.19s	remaining: 659ms
#&gt; 769:	learn: 0.0100339	total: 2.19s	remaining: 655ms
#&gt; 770:	learn: 0.0100070	total: 2.21s	remaining: 656ms
#&gt; 771:	learn: 0.0100069	total: 2.21s	remaining: 653ms
#&gt; 772:	learn: 0.0100004	total: 2.22s	remaining: 652ms
#&gt; 773:	learn: 0.0099977	total: 2.22s	remaining: 649ms
#&gt; 774:	learn: 0.0099906	total: 2.23s	remaining: 647ms
#&gt; 775:	learn: 0.0099906	total: 2.23s	remaining: 644ms
#&gt; 776:	learn: 0.0099906	total: 2.23s	remaining: 640ms
#&gt; 777:	learn: 0.0099906	total: 2.23s	remaining: 636ms
#&gt; 778:	learn: 0.0099834	total: 2.23s	remaining: 634ms
#&gt; 779:	learn: 0.0099825	total: 2.23s	remaining: 631ms
#&gt; 780:	learn: 0.0099825	total: 2.24s	remaining: 628ms
#&gt; 781:	learn: 0.0099825	total: 2.24s	remaining: 624ms
#&gt; 782:	learn: 0.0099781	total: 2.24s	remaining: 622ms
#&gt; 783:	learn: 0.0099750	total: 2.25s	remaining: 619ms
#&gt; 784:	learn: 0.0099736	total: 2.25s	remaining: 616ms
#&gt; 785:	learn: 0.0099593	total: 2.25s	remaining: 613ms
#&gt; 786:	learn: 0.0099276	total: 2.26s	remaining: 611ms
#&gt; 787:	learn: 0.0099158	total: 2.26s	remaining: 607ms
#&gt; 788:	learn: 0.0099157	total: 2.26s	remaining: 604ms
#&gt; 789:	learn: 0.0099157	total: 2.26s	remaining: 601ms
#&gt; 790:	learn: 0.0098844	total: 2.27s	remaining: 601ms
#&gt; 791:	learn: 0.0098844	total: 2.28s	remaining: 598ms
#&gt; 792:	learn: 0.0098106	total: 2.29s	remaining: 597ms
#&gt; 793:	learn: 0.0098106	total: 2.29s	remaining: 594ms
#&gt; 794:	learn: 0.0098046	total: 2.29s	remaining: 590ms
#&gt; 795:	learn: 0.0097968	total: 2.29s	remaining: 587ms
#&gt; 796:	learn: 0.0097615	total: 2.3s	remaining: 586ms
#&gt; 797:	learn: 0.0097613	total: 2.31s	remaining: 584ms
#&gt; 798:	learn: 0.0097605	total: 2.31s	remaining: 580ms
#&gt; 799:	learn: 0.0097605	total: 2.31s	remaining: 577ms
#&gt; 800:	learn: 0.0097605	total: 2.31s	remaining: 574ms
#&gt; 801:	learn: 0.0097605	total: 2.31s	remaining: 570ms
#&gt; 802:	learn: 0.0097605	total: 2.31s	remaining: 567ms
#&gt; 803:	learn: 0.0097471	total: 2.32s	remaining: 565ms
#&gt; 804:	learn: 0.0097471	total: 2.32s	remaining: 561ms
#&gt; 805:	learn: 0.0097334	total: 2.33s	remaining: 560ms
#&gt; 806:	learn: 0.0097319	total: 2.33s	remaining: 557ms
#&gt; 807:	learn: 0.0097319	total: 2.33s	remaining: 554ms
#&gt; 808:	learn: 0.0096868	total: 2.33s	remaining: 551ms
#&gt; 809:	learn: 0.0096867	total: 2.34s	remaining: 548ms
#&gt; 810:	learn: 0.0096585	total: 2.34s	remaining: 546ms
#&gt; 811:	learn: 0.0096584	total: 2.35s	remaining: 544ms
#&gt; 812:	learn: 0.0096529	total: 2.35s	remaining: 541ms
#&gt; 813:	learn: 0.0096529	total: 2.35s	remaining: 538ms
#&gt; 814:	learn: 0.0096529	total: 2.35s	remaining: 535ms
#&gt; 815:	learn: 0.0096529	total: 2.35s	remaining: 531ms
#&gt; 816:	learn: 0.0096529	total: 2.35s	remaining: 528ms
#&gt; 817:	learn: 0.0096529	total: 2.36s	remaining: 525ms
#&gt; 818:	learn: 0.0096529	total: 2.36s	remaining: 521ms
#&gt; 819:	learn: 0.0096529	total: 2.36s	remaining: 518ms
#&gt; 820:	learn: 0.0096529	total: 2.36s	remaining: 514ms
#&gt; 821:	learn: 0.0096526	total: 2.36s	remaining: 511ms
#&gt; 822:	learn: 0.0096461	total: 2.36s	remaining: 508ms
#&gt; 823:	learn: 0.0096461	total: 2.36s	remaining: 505ms
#&gt; 824:	learn: 0.0096461	total: 2.36s	remaining: 501ms
#&gt; 825:	learn: 0.0096380	total: 2.37s	remaining: 499ms
#&gt; 826:	learn: 0.0096305	total: 2.38s	remaining: 497ms
#&gt; 827:	learn: 0.0096305	total: 2.38s	remaining: 493ms
#&gt; 828:	learn: 0.0096265	total: 2.38s	remaining: 491ms
#&gt; 829:	learn: 0.0096259	total: 2.38s	remaining: 488ms
#&gt; 830:	learn: 0.0096259	total: 2.38s	remaining: 485ms
#&gt; 831:	learn: 0.0095395	total: 2.41s	remaining: 487ms
#&gt; 832:	learn: 0.0095373	total: 2.42s	remaining: 484ms
#&gt; 833:	learn: 0.0095373	total: 2.42s	remaining: 481ms
#&gt; 834:	learn: 0.0095367	total: 2.42s	remaining: 477ms
#&gt; 835:	learn: 0.0095367	total: 2.42s	remaining: 474ms
#&gt; 836:	learn: 0.0095367	total: 2.42s	remaining: 471ms
#&gt; 837:	learn: 0.0095343	total: 2.42s	remaining: 468ms
#&gt; 838:	learn: 0.0095330	total: 2.42s	remaining: 465ms
#&gt; 839:	learn: 0.0095320	total: 2.43s	remaining: 462ms
#&gt; 840:	learn: 0.0094688	total: 2.44s	remaining: 462ms
#&gt; 841:	learn: 0.0094688	total: 2.45s	remaining: 459ms
#&gt; 842:	learn: 0.0094566	total: 2.45s	remaining: 456ms
#&gt; 843:	learn: 0.0094566	total: 2.45s	remaining: 453ms
#&gt; 844:	learn: 0.0094426	total: 2.46s	remaining: 451ms
#&gt; 845:	learn: 0.0094281	total: 2.47s	remaining: 449ms
#&gt; 846:	learn: 0.0094268	total: 2.47s	remaining: 446ms
#&gt; 847:	learn: 0.0094268	total: 2.47s	remaining: 443ms
#&gt; 848:	learn: 0.0094115	total: 2.48s	remaining: 441ms
#&gt; 849:	learn: 0.0094085	total: 2.48s	remaining: 437ms
#&gt; 850:	learn: 0.0093546	total: 2.49s	remaining: 436ms
#&gt; 851:	learn: 0.0093545	total: 2.49s	remaining: 433ms
#&gt; 852:	learn: 0.0093536	total: 2.5s	remaining: 430ms
#&gt; 853:	learn: 0.0093533	total: 2.5s	remaining: 427ms
#&gt; 854:	learn: 0.0093533	total: 2.5s	remaining: 424ms
#&gt; 855:	learn: 0.0093533	total: 2.5s	remaining: 421ms
#&gt; 856:	learn: 0.0093487	total: 2.5s	remaining: 418ms
#&gt; 857:	learn: 0.0093487	total: 2.5s	remaining: 414ms
#&gt; 858:	learn: 0.0093466	total: 2.5s	remaining: 411ms
#&gt; 859:	learn: 0.0093466	total: 2.5s	remaining: 408ms
#&gt; 860:	learn: 0.0093466	total: 2.51s	remaining: 405ms
#&gt; 861:	learn: 0.0093458	total: 2.51s	remaining: 401ms
#&gt; 862:	learn: 0.0093458	total: 2.51s	remaining: 398ms
#&gt; 863:	learn: 0.0093204	total: 2.51s	remaining: 396ms
#&gt; 864:	learn: 0.0093204	total: 2.51s	remaining: 393ms
#&gt; 865:	learn: 0.0092664	total: 2.52s	remaining: 390ms
#&gt; 866:	learn: 0.0092538	total: 2.53s	remaining: 388ms
#&gt; 867:	learn: 0.0092536	total: 2.53s	remaining: 385ms
#&gt; 868:	learn: 0.0092536	total: 2.53s	remaining: 382ms
#&gt; 869:	learn: 0.0092475	total: 2.53s	remaining: 379ms
#&gt; 870:	learn: 0.0092475	total: 2.54s	remaining: 376ms
#&gt; 871:	learn: 0.0092234	total: 2.54s	remaining: 373ms
#&gt; 872:	learn: 0.0092234	total: 2.54s	remaining: 369ms
#&gt; 873:	learn: 0.0092180	total: 2.54s	remaining: 367ms
#&gt; 874:	learn: 0.0092062	total: 2.55s	remaining: 364ms
#&gt; 875:	learn: 0.0091984	total: 2.55s	remaining: 361ms
#&gt; 876:	learn: 0.0091984	total: 2.55s	remaining: 358ms
#&gt; 877:	learn: 0.0091972	total: 2.56s	remaining: 355ms
#&gt; 878:	learn: 0.0091972	total: 2.56s	remaining: 352ms
#&gt; 879:	learn: 0.0091972	total: 2.56s	remaining: 349ms
#&gt; 880:	learn: 0.0091972	total: 2.56s	remaining: 346ms
#&gt; 881:	learn: 0.0091972	total: 2.56s	remaining: 342ms
#&gt; 882:	learn: 0.0091972	total: 2.56s	remaining: 339ms
#&gt; 883:	learn: 0.0091941	total: 2.56s	remaining: 336ms
#&gt; 884:	learn: 0.0091937	total: 2.56s	remaining: 333ms
#&gt; 885:	learn: 0.0091937	total: 2.56s	remaining: 330ms
#&gt; 886:	learn: 0.0091824	total: 2.57s	remaining: 327ms
#&gt; 887:	learn: 0.0091824	total: 2.57s	remaining: 324ms
#&gt; 888:	learn: 0.0091820	total: 2.57s	remaining: 321ms
#&gt; 889:	learn: 0.0091807	total: 2.57s	remaining: 318ms
#&gt; 890:	learn: 0.0091331	total: 2.58s	remaining: 315ms
#&gt; 891:	learn: 0.0091326	total: 2.58s	remaining: 312ms
#&gt; 892:	learn: 0.0091305	total: 2.61s	remaining: 313ms
#&gt; 893:	learn: 0.0091305	total: 2.61s	remaining: 310ms
#&gt; 894:	learn: 0.0091305	total: 2.61s	remaining: 306ms
#&gt; 895:	learn: 0.0091305	total: 2.61s	remaining: 303ms
#&gt; 896:	learn: 0.0091305	total: 2.61s	remaining: 300ms
#&gt; 897:	learn: 0.0091305	total: 2.62s	remaining: 297ms
#&gt; 898:	learn: 0.0091305	total: 2.62s	remaining: 294ms
#&gt; 899:	learn: 0.0091304	total: 2.62s	remaining: 291ms
#&gt; 900:	learn: 0.0091238	total: 2.62s	remaining: 288ms
#&gt; 901:	learn: 0.0091238	total: 2.62s	remaining: 285ms
#&gt; 902:	learn: 0.0091040	total: 2.63s	remaining: 282ms
#&gt; 903:	learn: 0.0091040	total: 2.63s	remaining: 279ms
#&gt; 904:	learn: 0.0091040	total: 2.63s	remaining: 276ms
#&gt; 905:	learn: 0.0091040	total: 2.63s	remaining: 273ms
#&gt; 906:	learn: 0.0091040	total: 2.63s	remaining: 269ms
#&gt; 907:	learn: 0.0091040	total: 2.63s	remaining: 266ms
#&gt; 908:	learn: 0.0091040	total: 2.63s	remaining: 263ms
#&gt; 909:	learn: 0.0091037	total: 2.63s	remaining: 260ms
#&gt; 910:	learn: 0.0091036	total: 2.63s	remaining: 257ms
#&gt; 911:	learn: 0.0091019	total: 2.63s	remaining: 254ms
#&gt; 912:	learn: 0.0091019	total: 2.63s	remaining: 251ms
#&gt; 913:	learn: 0.0090876	total: 2.63s	remaining: 248ms
#&gt; 914:	learn: 0.0090876	total: 2.64s	remaining: 245ms
#&gt; 915:	learn: 0.0090876	total: 2.64s	remaining: 242ms
#&gt; 916:	learn: 0.0090876	total: 2.64s	remaining: 239ms
#&gt; 917:	learn: 0.0090867	total: 2.64s	remaining: 236ms
#&gt; 918:	learn: 0.0090867	total: 2.64s	remaining: 233ms
#&gt; 919:	learn: 0.0090867	total: 2.64s	remaining: 230ms
#&gt; 920:	learn: 0.0090867	total: 2.64s	remaining: 227ms
#&gt; 921:	learn: 0.0090864	total: 2.65s	remaining: 224ms
#&gt; 922:	learn: 0.0090864	total: 2.65s	remaining: 221ms
#&gt; 923:	learn: 0.0090864	total: 2.65s	remaining: 218ms
#&gt; 924:	learn: 0.0090864	total: 2.65s	remaining: 215ms
#&gt; 925:	learn: 0.0090864	total: 2.65s	remaining: 212ms
#&gt; 926:	learn: 0.0090864	total: 2.65s	remaining: 209ms
#&gt; 927:	learn: 0.0090836	total: 2.65s	remaining: 206ms
#&gt; 928:	learn: 0.0090836	total: 2.65s	remaining: 203ms
#&gt; 929:	learn: 0.0090831	total: 2.66s	remaining: 200ms
#&gt; 930:	learn: 0.0090831	total: 2.66s	remaining: 197ms
#&gt; 931:	learn: 0.0090776	total: 2.66s	remaining: 194ms
#&gt; 932:	learn: 0.0090441	total: 2.66s	remaining: 191ms
#&gt; 933:	learn: 0.0090368	total: 2.67s	remaining: 189ms
#&gt; 934:	learn: 0.0090354	total: 2.67s	remaining: 186ms
#&gt; 935:	learn: 0.0090354	total: 2.67s	remaining: 183ms
#&gt; 936:	learn: 0.0090354	total: 2.67s	remaining: 180ms
#&gt; 937:	learn: 0.0090354	total: 2.67s	remaining: 177ms
#&gt; 938:	learn: 0.0090336	total: 2.68s	remaining: 174ms
#&gt; 939:	learn: 0.0090336	total: 2.68s	remaining: 171ms
#&gt; 940:	learn: 0.0090326	total: 2.68s	remaining: 168ms
#&gt; 941:	learn: 0.0090326	total: 2.68s	remaining: 165ms
#&gt; 942:	learn: 0.0090326	total: 2.68s	remaining: 162ms
#&gt; 943:	learn: 0.0090326	total: 2.68s	remaining: 159ms
#&gt; 944:	learn: 0.0090304	total: 2.69s	remaining: 156ms
#&gt; 945:	learn: 0.0090304	total: 2.69s	remaining: 153ms
#&gt; 946:	learn: 0.0090304	total: 2.69s	remaining: 150ms
#&gt; 947:	learn: 0.0090304	total: 2.69s	remaining: 147ms
#&gt; 948:	learn: 0.0090284	total: 2.69s	remaining: 144ms
#&gt; 949:	learn: 0.0090284	total: 2.69s	remaining: 141ms
#&gt; 950:	learn: 0.0090161	total: 2.69s	remaining: 139ms
#&gt; 951:	learn: 0.0090161	total: 2.69s	remaining: 136ms
#&gt; 952:	learn: 0.0090160	total: 2.69s	remaining: 133ms
#&gt; 953:	learn: 0.0090017	total: 2.7s	remaining: 130ms
#&gt; 954:	learn: 0.0089888	total: 2.71s	remaining: 127ms
#&gt; 955:	learn: 0.0089888	total: 2.71s	remaining: 125ms
#&gt; 956:	learn: 0.0089888	total: 2.71s	remaining: 122ms
#&gt; 957:	learn: 0.0089888	total: 2.71s	remaining: 119ms
#&gt; 958:	learn: 0.0089888	total: 2.71s	remaining: 116ms
#&gt; 959:	learn: 0.0089877	total: 2.71s	remaining: 113ms
#&gt; 960:	learn: 0.0089876	total: 2.71s	remaining: 110ms
#&gt; 961:	learn: 0.0089876	total: 2.72s	remaining: 107ms
#&gt; 962:	learn: 0.0089850	total: 2.72s	remaining: 105ms
#&gt; 963:	learn: 0.0089850	total: 2.72s	remaining: 102ms
#&gt; 964:	learn: 0.0089849	total: 2.72s	remaining: 98.7ms
#&gt; 965:	learn: 0.0089849	total: 2.72s	remaining: 95.8ms
#&gt; 966:	learn: 0.0089849	total: 2.72s	remaining: 92.9ms
#&gt; 967:	learn: 0.0089845	total: 2.72s	remaining: 90.1ms
#&gt; 968:	learn: 0.0089828	total: 2.73s	remaining: 87.3ms
#&gt; 969:	learn: 0.0089322	total: 2.74s	remaining: 84.6ms
#&gt; 970:	learn: 0.0089322	total: 2.74s	remaining: 81.7ms
#&gt; 971:	learn: 0.0089322	total: 2.74s	remaining: 78.8ms
#&gt; 972:	learn: 0.0089322	total: 2.74s	remaining: 75.9ms
#&gt; 973:	learn: 0.0089322	total: 2.74s	remaining: 73.1ms
#&gt; 974:	learn: 0.0089322	total: 2.74s	remaining: 70.2ms
#&gt; 975:	learn: 0.0089321	total: 2.74s	remaining: 67.4ms
#&gt; 976:	learn: 0.0089321	total: 2.74s	remaining: 64.5ms
#&gt; 977:	learn: 0.0089299	total: 2.74s	remaining: 61.7ms
#&gt; 978:	learn: 0.0089260	total: 2.75s	remaining: 58.9ms
#&gt; 979:	learn: 0.0088889	total: 2.75s	remaining: 56.1ms
#&gt; 980:	learn: 0.0088889	total: 2.75s	remaining: 53.4ms
#&gt; 981:	learn: 0.0088879	total: 2.77s	remaining: 50.7ms
#&gt; 982:	learn: 0.0088879	total: 2.77s	remaining: 47.8ms
#&gt; 983:	learn: 0.0088879	total: 2.77s	remaining: 45ms
#&gt; 984:	learn: 0.0088724	total: 2.79s	remaining: 42.5ms
#&gt; 985:	learn: 0.0088711	total: 2.79s	remaining: 39.7ms
#&gt; 986:	learn: 0.0088663	total: 2.8s	remaining: 36.9ms
#&gt; 987:	learn: 0.0088535	total: 2.8s	remaining: 34.1ms
#&gt; 988:	learn: 0.0088466	total: 2.81s	remaining: 31.2ms
#&gt; 989:	learn: 0.0088466	total: 2.81s	remaining: 28.4ms
#&gt; 990:	learn: 0.0088289	total: 2.82s	remaining: 25.6ms
#&gt; 991:	learn: 0.0088245	total: 2.82s	remaining: 22.7ms
#&gt; 992:	learn: 0.0088245	total: 2.82s	remaining: 19.9ms
#&gt; 993:	learn: 0.0088244	total: 2.82s	remaining: 17ms
#&gt; 994:	learn: 0.0088214	total: 2.83s	remaining: 14.2ms
#&gt; 995:	learn: 0.0088214	total: 2.83s	remaining: 11.4ms
#&gt; 996:	learn: 0.0088015	total: 2.83s	remaining: 8.53ms
#&gt; 997:	learn: 0.0088011	total: 2.83s	remaining: 5.68ms
#&gt; 998:	learn: 0.0087907	total: 2.84s	remaining: 2.84ms
#&gt; 999:	learn: 0.0087907	total: 2.84s	remaining: 0us</div><div class='input'><span class='va'>model_fit</span>
</div><div class='output co'>#&gt; parsnip model object
#&gt; 
#&gt; Fit time:  5.9s 
#&gt; Prophet Model w/ Catboost Error Specification
#&gt; ---
#&gt; Model 1: PROPHET
#&gt; $growth
#&gt; [1] "linear"
#&gt; 
#&gt; $changepoints
#&gt;  [1] "1990-09-01 GMT" "1991-05-01 GMT" "1991-12-01 GMT" "1992-08-01 GMT"
#&gt;  [5] "1993-04-01 GMT" "1993-12-01 GMT" "1994-07-01 GMT" "1995-03-01 GMT"
#&gt;  [9] "1995-11-01 GMT" "1996-07-01 GMT" "1997-02-01 GMT" "1997-10-01 GMT"
#&gt; [13] "1998-06-01 GMT" "1999-02-01 GMT" "1999-09-01 GMT" "2000-05-01 GMT"
#&gt; [17] "2001-01-01 GMT" "2001-09-01 GMT" "2002-04-01 GMT" "2002-12-01 GMT"
#&gt; [21] "2003-08-01 GMT" "2004-04-01 GMT" "2004-11-01 GMT" "2005-07-01 GMT"
#&gt; [25] "2006-03-01 GMT"
#&gt; 
#&gt; $n.changepoints
#&gt; [1] 25
#&gt; 
#&gt; $changepoint.range
#&gt; [1] 0.8
#&gt; 
#&gt; $yearly.seasonality
#&gt; [1] "auto"
#&gt; 
#&gt; $weekly.seasonality
#&gt; [1] "auto"
#&gt; 
#&gt; $daily.seasonality
#&gt; [1] "auto"
#&gt; 
#&gt; $holidays
#&gt; NULL
#&gt; 
#&gt; $seasonality.mode
#&gt; [1] "additive"
#&gt; 
#&gt; $seasonality.prior.scale
#&gt; [1] 10
#&gt; 
#&gt; $changepoint.prior.scale
#&gt; [1] 0.05
#&gt; 
#&gt; $holidays.prior.scale
#&gt; [1] 10
#&gt; 
#&gt; $mcmc.samples
#&gt; [1] 0
#&gt; 
#&gt; $interval.width
#&gt; [1] 0.8
#&gt; 
#&gt; $uncertainty.samples
#&gt; [1] 1000
#&gt; 
#&gt; $specified.changepoints
#&gt; [1] FALSE
#&gt; 
#&gt; $start
#&gt; [1] "1990-01-01 GMT"
#&gt; 
#&gt; $y.scale
#&gt; [1] 9.289152
#&gt; 
#&gt; $logistic.floor
#&gt; [1] FALSE
#&gt; 
#&gt; $t.scale
#&gt; [1] 638928000
#&gt; 
#&gt; $changepoints.t
#&gt;  [1] 0.03286004 0.06558485 0.09452333 0.12751859 0.16037863 0.19337390
#&gt;  [7] 0.22204192 0.25490196 0.28803245 0.32089249 0.34996619 0.38269101
#&gt; [13] 0.41555105 0.44868154 0.47734956 0.51020960 0.54334009 0.57620014
#&gt; [19] 0.60486815 0.63786342 0.67072346 0.70371873 0.73265720 0.76538201
#&gt; [25] 0.79824206
#&gt; 
#&gt; $seasonalities
#&gt; $seasonalities$yearly
#&gt; $seasonalities$yearly$period
#&gt; [1] 365.25
#&gt; 
#&gt; $seasonalities$yearly$fourier.order
#&gt; [1] 10
#&gt; 
#&gt; $seasonalities$yearly$prior.scale
#&gt; [1] 10
#&gt; 
#&gt; $seasonalities$yearly$mode
#&gt; [1] "additive"
#&gt; 
#&gt; $seasonalities$yearly$condition.name
#&gt; NULL
#&gt; 
#&gt; 
#&gt; 
#&gt; $extra_regressors
#&gt; list()
#&gt; 
#&gt; $country_holidays
#&gt; NULL
#&gt; 
#&gt; $stan.fit
#&gt; $stan.fit$par
#&gt; $stan.fit$par$k
#&gt; [1] 0.1334132
#&gt; 
#&gt; $stan.fit$par$m
#&gt; [1] 0.9415521
#&gt; 
#&gt; $stan.fit$par$delta
#&gt;  [1] -3.069999e-02 -3.836996e-02 -7.255556e-02 -7.495696e-03  5.416064e-02
#&gt;  [6]  9.030884e-03  5.452900e-03  3.015859e-02  2.952826e-02  1.192383e-06
#&gt; [11] -2.170273e-02  3.001226e-09  4.729257e-02 -2.482628e-10 -9.950836e-02
#&gt; [16] -2.494600e-09  1.328259e-01  1.609158e-02 -2.256983e-01 -1.135256e-04
#&gt; [21]  4.319331e-02  9.348530e-08 -1.233810e-02 -1.388182e-01  1.932328e-01
#&gt; 
#&gt; $stan.fit$par$sigma_obs
#&gt; [1] 0.001990938
#&gt; 
#&gt; $stan.fit$par$beta
#&gt;  [1]  0.0025547391  0.0048284804 -0.0029179125 -0.0053958307  0.0037308771
#&gt;  [6]  0.0035946331 -0.0040725564 -0.0022530524  0.0027582578  0.0019500875
#&gt; [11] -0.0044812421 -0.0002742276  0.0025300239 -0.0012144792 -0.0030948244
#&gt; [16]  0.0010920830  0.0024337117 -0.0021357125 -0.0003163449  0.0020500926
#&gt; 
#&gt; $stan.fit$par$trend
#&gt;   [1] 0.9415521 0.9421113 0.9426165 0.9431758 0.9437170 0.9442763 0.9448175
#&gt;   [8] 0.9453768 0.9459360 0.9463527 0.9467833 0.9472000 0.9476306 0.9480611
#&gt;  [15] 0.9484500 0.9488806 0.9492973 0.9495670 0.9498281 0.9500978 0.9503675
#&gt;  [22] 0.9506285 0.9508983 0.9511593 0.9511249 0.9510905 0.9510582 0.9510238
#&gt;  [29] 0.9509905 0.9509561 0.9509228 0.9508883 0.9508225 0.9507588 0.9506929
#&gt;  [36] 0.9506292 0.9505633 0.9504975 0.9504380 0.9503722 0.9505282 0.9506894
#&gt;  [43] 0.9508454 0.9510066 0.9511677 0.9513237 0.9514849 0.9516409 0.9518400
#&gt;  [50] 0.9520390 0.9522188 0.9524179 0.9526105 0.9528096 0.9530022 0.9532241
#&gt;  [57] 0.9534460 0.9536608 0.9538827 0.9540974 0.9543193 0.9545412 0.9547417
#&gt;  [64] 0.9550900 0.9554271 0.9557755 0.9561126 0.9564609 0.9568092 0.9571463
#&gt;  [71] 0.9574947 0.9579516 0.9584237 0.9588958 0.9593375 0.9598096 0.9602665
#&gt;  [78] 0.9607386 0.9611955 0.9616676 0.9621397 0.9625966 0.9630687 0.9635256
#&gt;  [85] 0.9639978 0.9644699 0.9648141 0.9651953 0.9655641 0.9659453 0.9663141
#&gt;  [92] 0.9666953 0.9670764 0.9674453 0.9678264 0.9681953 0.9685764 0.9689576
#&gt;  [99] 0.9693018 0.9696830 0.9700518 0.9704330 0.9709937 0.9715731 0.9721525
#&gt; [106] 0.9727132 0.9732926 0.9738533 0.9744327 0.9750121 0.9755354 0.9761148
#&gt; [113] 0.9766755 0.9772549 0.9778156 0.9783950 0.9789744 0.9791314 0.9792937
#&gt; [120] 0.9794507 0.9796130 0.9797752 0.9799270 0.9800893 0.9802463 0.9804085
#&gt; [127] 0.9805656 0.9807278 0.9808901 0.9810471 0.9812094 0.9813664 0.9815286
#&gt; [134] 0.9822477 0.9828972 0.9836162 0.9843121 0.9850312 0.9857270 0.9864461
#&gt; [141] 0.9871652 0.9879263 0.9887128 0.9894740 0.9902605 0.9910470 0.9917574
#&gt; [148] 0.9925440 0.9923895 0.9922299 0.9920754 0.9919158 0.9917562 0.9916017
#&gt; [155] 0.9914421 0.9912877 0.9911276 0.9909675 0.9908229 0.9906628 0.9905079
#&gt; [162] 0.9903478 0.9901929 0.9900328 0.9900538 0.9900741 0.9900951 0.9901154
#&gt; [169] 0.9901364 0.9901573 0.9901770 0.9901979 0.9902182 0.9902392 0.9902595
#&gt; [176] 0.9902805 0.9903015 0.9903218 0.9903428 0.9903130 0.9902823 0.9902515
#&gt; [183] 0.9902238 0.9901930 0.9901633 0.9901325 0.9901028 0.9894901 0.9888775
#&gt; [190] 0.9882846 0.9876719 0.9870790 0.9864663 0.9858536 0.9853003 0.9854976
#&gt; [197] 0.9856886 0.9858860 0.9860770 0.9862744 0.9864717 0.9866627 0.9868601
#&gt; [204] 0.9870511 0.9872485 0.9874458 0.9876241 0.9878215 0.9880125 0.9882098
#&gt; [211] 0.9884008 0.9885982 0.9887956 0.9889866 0.9891839 0.9893749 0.9895723
#&gt; [218] 0.9897697 0.9899543 0.9901517 0.9903427 0.9905400 0.9907310 0.9909284
#&gt; [225] 0.9911258 0.9913168 0.9915141 0.9917051 0.9919025 0.9920999 0.9922781
#&gt; [232] 0.9924755 0.9926665 0.9928638 0.9930548 0.9932522 0.9934496 0.9936406
#&gt; [239] 0.9938379 0.9940289 0.9942263 0.9944237 0.9946019 0.9947993
#&gt; 
#&gt; 
#&gt; $stan.fit$value
#&gt; [1] 1371.543
#&gt; 
#&gt; $stan.fit$return_code
#&gt; [1] 0
#&gt; 
#&gt; $stan.fit$theta_tilde
#&gt;              k         m    delta[1]    delta[2]    delta[3]     delta[4]
#&gt; [1,] 0.1334132 0.9415521 -0.03069999 -0.03836996 -0.07255556 -0.007495696
#&gt;        delta[5]    delta[6]  delta[7]   delta[8]   delta[9]    delta[10]
#&gt; [1,] 0.05416064 0.009030884 0.0054529 0.03015859 0.02952826 1.192383e-06
#&gt;        delta[11]    delta[12]  delta[13]     delta[14]   delta[15]   delta[16]
#&gt; [1,] -0.02170273 3.001226e-09 0.04729257 -2.482628e-10 -0.09950836 -2.4946e-09
#&gt;      delta[17]  delta[18]  delta[19]     delta[20]  delta[21]   delta[22]
#&gt; [1,] 0.1328259 0.01609158 -0.2256983 -0.0001135256 0.04319331 9.34853e-08
#&gt;       delta[23]  delta[24] delta[25]   sigma_obs     beta[1]    beta[2]
#&gt; [1,] -0.0123381 -0.1388182 0.1932328 0.001990938 0.002554739 0.00482848
#&gt;           beta[3]      beta[4]     beta[5]     beta[6]      beta[7]
#&gt; [1,] -0.002917912 -0.005395831 0.003730877 0.003594633 -0.004072556
#&gt;           beta[8]     beta[9]    beta[10]     beta[11]      beta[12]
#&gt; [1,] -0.002253052 0.002758258 0.001950088 -0.004481242 -0.0002742276
#&gt;         beta[13]     beta[14]     beta[15]    beta[16]    beta[17]     beta[18]
#&gt; [1,] 0.002530024 -0.001214479 -0.003094824 0.001092083 0.002433712 -0.002135713
#&gt;           beta[19]    beta[20]  trend[1]  trend[2]  trend[3]  trend[4] trend[5]
#&gt; [1,] -0.0003163449 0.002050093 0.9415521 0.9421113 0.9426165 0.9431758 0.943717
#&gt;       trend[6]  trend[7]  trend[8] trend[9] trend[10] trend[11] trend[12]
#&gt; [1,] 0.9442763 0.9448175 0.9453768 0.945936 0.9463527 0.9467833    0.9472
#&gt;      trend[13] trend[14] trend[15] trend[16] trend[17] trend[18] trend[19]
#&gt; [1,] 0.9476306 0.9480611   0.94845 0.9488806 0.9492973  0.949567 0.9498281
#&gt;      trend[20] trend[21] trend[22] trend[23] trend[24] trend[25] trend[26]
#&gt; [1,] 0.9500978 0.9503675 0.9506285 0.9508983 0.9511593 0.9511249 0.9510905
#&gt;      trend[27] trend[28] trend[29] trend[30] trend[31] trend[32] trend[33]
#&gt; [1,] 0.9510582 0.9510238 0.9509905 0.9509561 0.9509228 0.9508883 0.9508225
#&gt;      trend[34] trend[35] trend[36] trend[37] trend[38] trend[39] trend[40]
#&gt; [1,] 0.9507588 0.9506929 0.9506292 0.9505633 0.9504975  0.950438 0.9503722
#&gt;      trend[41] trend[42] trend[43] trend[44] trend[45] trend[46] trend[47]
#&gt; [1,] 0.9505282 0.9506894 0.9508454 0.9510066 0.9511677 0.9513237 0.9514849
#&gt;      trend[48] trend[49] trend[50] trend[51] trend[52] trend[53] trend[54]
#&gt; [1,] 0.9516409   0.95184  0.952039 0.9522188 0.9524179 0.9526105 0.9528096
#&gt;      trend[55] trend[56] trend[57] trend[58] trend[59] trend[60] trend[61]
#&gt; [1,] 0.9530022 0.9532241  0.953446 0.9536608 0.9538827 0.9540974 0.9543193
#&gt;      trend[62] trend[63] trend[64] trend[65] trend[66] trend[67] trend[68]
#&gt; [1,] 0.9545412 0.9547417   0.95509 0.9554271 0.9557755 0.9561126 0.9564609
#&gt;      trend[69] trend[70] trend[71] trend[72] trend[73] trend[74] trend[75]
#&gt; [1,] 0.9568092 0.9571463 0.9574947 0.9579516 0.9584237 0.9588958 0.9593375
#&gt;      trend[76] trend[77] trend[78] trend[79] trend[80] trend[81] trend[82]
#&gt; [1,] 0.9598096 0.9602665 0.9607386 0.9611955 0.9616676 0.9621397 0.9625966
#&gt;      trend[83] trend[84] trend[85] trend[86] trend[87] trend[88] trend[89]
#&gt; [1,] 0.9630687 0.9635256 0.9639978 0.9644699 0.9648141 0.9651953 0.9655641
#&gt;      trend[90] trend[91] trend[92] trend[93] trend[94] trend[95] trend[96]
#&gt; [1,] 0.9659453 0.9663141 0.9666953 0.9670764 0.9674453 0.9678264 0.9681953
#&gt;      trend[97] trend[98] trend[99] trend[100] trend[101] trend[102] trend[103]
#&gt; [1,] 0.9685764 0.9689576 0.9693018   0.969683  0.9700518   0.970433  0.9709937
#&gt;      trend[104] trend[105] trend[106] trend[107] trend[108] trend[109]
#&gt; [1,]  0.9715731  0.9721525  0.9727132  0.9732926  0.9738533  0.9744327
#&gt;      trend[110] trend[111] trend[112] trend[113] trend[114] trend[115]
#&gt; [1,]  0.9750121  0.9755354  0.9761148  0.9766755  0.9772549  0.9778156
#&gt;      trend[116] trend[117] trend[118] trend[119] trend[120] trend[121]
#&gt; [1,]   0.978395  0.9789744  0.9791314  0.9792937  0.9794507   0.979613
#&gt;      trend[122] trend[123] trend[124] trend[125] trend[126] trend[127]
#&gt; [1,]  0.9797752   0.979927  0.9800893  0.9802463  0.9804085  0.9805656
#&gt;      trend[128] trend[129] trend[130] trend[131] trend[132] trend[133]
#&gt; [1,]  0.9807278  0.9808901  0.9810471  0.9812094  0.9813664  0.9815286
#&gt;      trend[134] trend[135] trend[136] trend[137] trend[138] trend[139]
#&gt; [1,]  0.9822477  0.9828972  0.9836162  0.9843121  0.9850312   0.985727
#&gt;      trend[140] trend[141] trend[142] trend[143] trend[144] trend[145]
#&gt; [1,]  0.9864461  0.9871652  0.9879263  0.9887128   0.989474  0.9902605
#&gt;      trend[146] trend[147] trend[148] trend[149] trend[150] trend[151]
#&gt; [1,]   0.991047  0.9917574   0.992544  0.9923895  0.9922299  0.9920754
#&gt;      trend[152] trend[153] trend[154] trend[155] trend[156] trend[157]
#&gt; [1,]  0.9919158  0.9917562  0.9916017  0.9914421  0.9912877  0.9911276
#&gt;      trend[158] trend[159] trend[160] trend[161] trend[162] trend[163]
#&gt; [1,]  0.9909675  0.9908229  0.9906628  0.9905079  0.9903478  0.9901929
#&gt;      trend[164] trend[165] trend[166] trend[167] trend[168] trend[169]
#&gt; [1,]  0.9900328  0.9900538  0.9900741  0.9900951  0.9901154  0.9901364
#&gt;      trend[170] trend[171] trend[172] trend[173] trend[174] trend[175]
#&gt; [1,]  0.9901573   0.990177  0.9901979  0.9902182  0.9902392  0.9902595
#&gt;      trend[176] trend[177] trend[178] trend[179] trend[180] trend[181]
#&gt; [1,]  0.9902805  0.9903015  0.9903218  0.9903428   0.990313  0.9902823
#&gt;      trend[182] trend[183] trend[184] trend[185] trend[186] trend[187]
#&gt; [1,]  0.9902515  0.9902238   0.990193  0.9901633  0.9901325  0.9901028
#&gt;      trend[188] trend[189] trend[190] trend[191] trend[192] trend[193]
#&gt; [1,]  0.9894901  0.9888775  0.9882846  0.9876719   0.987079  0.9864663
#&gt;      trend[194] trend[195] trend[196] trend[197] trend[198] trend[199]
#&gt; [1,]  0.9858536  0.9853003  0.9854976  0.9856886   0.985886   0.986077
#&gt;      trend[200] trend[201] trend[202] trend[203] trend[204] trend[205]
#&gt; [1,]  0.9862744  0.9864717  0.9866627  0.9868601  0.9870511  0.9872485
#&gt;      trend[206] trend[207] trend[208] trend[209] trend[210] trend[211]
#&gt; [1,]  0.9874458  0.9876241  0.9878215  0.9880125  0.9882098  0.9884008
#&gt;      trend[212] trend[213] trend[214] trend[215] trend[216] trend[217]
#&gt; [1,]  0.9885982  0.9887956  0.9889866  0.9891839  0.9893749  0.9895723
#&gt;      trend[218] trend[219] trend[220] trend[221] trend[222] trend[223]
#&gt; [1,]  0.9897697  0.9899543  0.9901517  0.9903427    0.99054   0.990731
#&gt;      trend[224] trend[225] trend[226] trend[227] trend[228] trend[229]
#&gt; [1,]  0.9909284  0.9911258  0.9913168  0.9915141  0.9917051  0.9919025
#&gt;      trend[230] trend[231] trend[232] trend[233] trend[234] trend[235]
#&gt; [1,]  0.9920999  0.9922781  0.9924755  0.9926665  0.9928638  0.9930548
#&gt;      trend[236] trend[237] trend[238] trend[239] trend[240] trend[241]
#&gt; [1,]  0.9932522  0.9934496  0.9936406  0.9938379  0.9940289  0.9942263
#&gt;      trend[242] trend[243] trend[244]
#&gt; [1,]  0.9944237  0.9946019  0.9947993
#&gt; 
#&gt; 
#&gt; $params
#&gt; $params$k
#&gt; [1] 0.1334132
#&gt; 
#&gt; $params$m
#&gt; [1] 0.9415521
#&gt; 
#&gt; $params$delta
#&gt;             [,1]        [,2]        [,3]         [,4]       [,5]        [,6]
#&gt; [1,] -0.03069999 -0.03836996 -0.07255556 -0.007495696 0.05416064 0.009030884
#&gt;           [,7]       [,8]       [,9]        [,10]       [,11]        [,12]
#&gt; [1,] 0.0054529 0.03015859 0.02952826 1.192383e-06 -0.02170273 3.001226e-09
#&gt;           [,13]         [,14]       [,15]       [,16]     [,17]      [,18]
#&gt; [1,] 0.04729257 -2.482628e-10 -0.09950836 -2.4946e-09 0.1328259 0.01609158
#&gt;           [,19]         [,20]      [,21]       [,22]      [,23]      [,24]
#&gt; [1,] -0.2256983 -0.0001135256 0.04319331 9.34853e-08 -0.0123381 -0.1388182
#&gt;          [,25]
#&gt; [1,] 0.1932328
#&gt; 
#&gt; $params$sigma_obs
#&gt; [1] 0.001990938
#&gt; 
#&gt; $params$beta
#&gt;             [,1]       [,2]         [,3]         [,4]        [,5]        [,6]
#&gt; [1,] 0.002554739 0.00482848 -0.002917912 -0.005395831 0.003730877 0.003594633
#&gt;              [,7]         [,8]        [,9]       [,10]        [,11]
#&gt; [1,] -0.004072556 -0.002253052 0.002758258 0.001950088 -0.004481242
#&gt;              [,12]       [,13]        [,14]        [,15]       [,16]
#&gt; [1,] -0.0002742276 0.002530024 -0.001214479 -0.003094824 0.001092083
#&gt;            [,17]        [,18]         [,19]       [,20]
#&gt; [1,] 0.002433712 -0.002135713 -0.0003163449 0.002050093
#&gt; 
#&gt; $params$trend
#&gt;   [1] 0.9415521 0.9421113 0.9426165 0.9431758 0.9437170 0.9442763 0.9448175
#&gt;   [8] 0.9453768 0.9459360 0.9463527 0.9467833 0.9472000 0.9476306 0.9480611
#&gt;  [15] 0.9484500 0.9488806 0.9492973 0.9495670 0.9498281 0.9500978 0.9503675
#&gt;  [22] 0.9506285 0.9508983 0.9511593 0.9511249 0.9510905 0.9510582 0.9510238
#&gt;  [29] 0.9509905 0.9509561 0.9509228 0.9508883 0.9508225 0.9507588 0.9506929
#&gt;  [36] 0.9506292 0.9505633 0.9504975 0.9504380 0.9503722 0.9505282 0.9506894
#&gt;  [43] 0.9508454 0.9510066 0.9511677 0.9513237 0.9514849 0.9516409 0.9518400
#&gt;  [50] 0.9520390 0.9522188 0.9524179 0.9526105 0.9528096 0.9530022 0.9532241
#&gt;  [57] 0.9534460 0.9536608 0.9538827 0.9540974 0.9543193 0.9545412 0.9547417
#&gt;  [64] 0.9550900 0.9554271 0.9557755 0.9561126 0.9564609 0.9568092 0.9571463
#&gt;  [71] 0.9574947 0.9579516 0.9584237 0.9588958 0.9593375 0.9598096 0.9602665
#&gt;  [78] 0.9607386 0.9611955 0.9616676 0.9621397 0.9625966 0.9630687 0.9635256
#&gt;  [85] 0.9639978 0.9644699 0.9648141 0.9651953 0.9655641 0.9659453 0.9663141
#&gt;  [92] 0.9666953 0.9670764 0.9674453 0.9678264 0.9681953 0.9685764 0.9689576
#&gt;  [99] 0.9693018 0.9696830 0.9700518 0.9704330 0.9709937 0.9715731 0.9721525
#&gt; [106] 0.9727132 0.9732926 0.9738533 0.9744327 0.9750121 0.9755354 0.9761148
#&gt; [113] 0.9766755 0.9772549 0.9778156 0.9783950 0.9789744 0.9791314 0.9792937
#&gt; [120] 0.9794507 0.9796130 0.9797752 0.9799270 0.9800893 0.9802463 0.9804085
#&gt; [127] 0.9805656 0.9807278 0.9808901 0.9810471 0.9812094 0.9813664 0.9815286
#&gt; [134] 0.9822477 0.9828972 0.9836162 0.9843121 0.9850312 0.9857270 0.9864461
#&gt; [141] 0.9871652 0.9879263 0.9887128 0.9894740 0.9902605 0.9910470 0.9917574
#&gt; [148] 0.9925440 0.9923895 0.9922299 0.9920754 0.9919158 0.9917562 0.9916017
#&gt; [155] 0.9914421 0.9912877 0.9911276 0.9909675 0.9908229 0.9906628 0.9905079
#&gt; [162] 0.9903478 0.9901929 0.9900328 0.9900538 0.9900741 0.9900951 0.9901154
#&gt; [169] 0.9901364 0.9901573 0.9901770 0.9901979 0.9902182 0.9902392 0.9902595
#&gt; [176] 0.9902805 0.9903015 0.9903218 0.9903428 0.9903130 0.9902823 0.9902515
#&gt; [183] 0.9902238 0.9901930 0.9901633 0.9901325 0.9901028 0.9894901 0.9888775
#&gt; [190] 0.9882846 0.9876719 0.9870790 0.9864663 0.9858536 0.9853003 0.9854976
#&gt; [197] 0.9856886 0.9858860 0.9860770 0.9862744 0.9864717 0.9866627 0.9868601
#&gt; [204] 0.9870511 0.9872485 0.9874458 0.9876241 0.9878215 0.9880125 0.9882098
#&gt; [211] 0.9884008 0.9885982 0.9887956 0.9889866 0.9891839 0.9893749 0.9895723
#&gt; [218] 0.9897697 0.9899543 0.9901517 0.9903427 0.9905400 0.9907310 0.9909284
#&gt; [225] 0.9911258 0.9913168 0.9915141 0.9917051 0.9919025 0.9920999 0.9922781
#&gt; [232] 0.9924755 0.9926665 0.9928638 0.9930548 0.9932522 0.9934496 0.9936406
#&gt; [239] 0.9938379 0.9940289 0.9942263 0.9944237 0.9946019 0.9947993
#&gt; 
#&gt; 
#&gt; $history
#&gt; </span><span style='color: #949494;'># A tibble: 244 x 5</span><span>
#&gt;        y ds                  floor       t y_scaled
#&gt;    </span><span style='color: #949494;font-style: italic;'>&lt;dbl&gt;</span><span> </span><span style='color: #949494;font-style: italic;'>&lt;dttm&gt;</span><span>              </span><span style='color: #949494;font-style: italic;'>&lt;dbl&gt;</span><span>   </span><span style='color: #949494;font-style: italic;'>&lt;dbl&gt;</span><span>    </span><span style='color: #949494;font-style: italic;'>&lt;dbl&gt;</span><span>
#&gt; </span><span style='color: #BCBCBC;'> 1</span><span>  8.76 1990-01-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0          0.943
#&gt; </span><span style='color: #BCBCBC;'> 2</span><span>  8.77 1990-02-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.004</span><span style='text-decoration: underline;'>19</span><span>    0.944
#&gt; </span><span style='color: #BCBCBC;'> 3</span><span>  8.78 1990-03-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.007</span><span style='text-decoration: underline;'>98</span><span>    0.945
#&gt; </span><span style='color: #BCBCBC;'> 4</span><span>  8.79 1990-04-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.012</span><span style='text-decoration: underline;'>2</span><span>     0.946
#&gt; </span><span style='color: #BCBCBC;'> 5</span><span>  8.80 1990-05-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.016</span><span style='text-decoration: underline;'>2</span><span>     0.947
#&gt; </span><span style='color: #BCBCBC;'> 6</span><span>  8.81 1990-06-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.020</span><span style='text-decoration: underline;'>4</span><span>     0.948
#&gt; </span><span style='color: #BCBCBC;'> 7</span><span>  8.70 1990-07-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.024</span><span style='text-decoration: underline;'>5</span><span>     0.937
#&gt; </span><span style='color: #BCBCBC;'> 8</span><span>  8.60 1990-08-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.028</span><span style='text-decoration: underline;'>7</span><span>     0.926
#&gt; </span><span style='color: #BCBCBC;'> 9</span><span>  8.78 1990-09-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.032</span><span style='text-decoration: underline;'>9</span><span>     0.945
#&gt; </span><span style='color: #BCBCBC;'>10</span><span>  8.83 1990-10-01 </span><span style='color: #949494;'>00:00:00</span><span>     0 0.036</span><span style='text-decoration: underline;'>9</span><span>     0.950
#&gt; </span><span style='color: #949494;'># ... with 234 more rows</span><span>
#&gt; 
#&gt; $history.dates
#&gt;   [1] "1990-01-01 GMT" "1990-02-01 GMT" "1990-03-01 GMT" "1990-04-01 GMT"
#&gt;   [5] "1990-05-01 GMT" "1990-06-01 GMT" "1990-07-01 GMT" "1990-08-01 GMT"
#&gt;   [9] "1990-09-01 GMT" "1990-10-01 GMT" "1990-11-01 GMT" "1990-12-01 GMT"
#&gt;  [13] "1991-01-01 GMT" "1991-02-01 GMT" "1991-03-01 GMT" "1991-04-01 GMT"
#&gt;  [17] "1991-05-01 GMT" "1991-06-01 GMT" "1991-07-01 GMT" "1991-08-01 GMT"
#&gt;  [21] "1991-09-01 GMT" "1991-10-01 GMT" "1991-11-01 GMT" "1991-12-01 GMT"
#&gt;  [25] "1992-01-01 GMT" "1992-02-01 GMT" "1992-03-01 GMT" "1992-04-01 GMT"
#&gt;  [29] "1992-05-01 GMT" "1992-06-01 GMT" "1992-07-01 GMT" "1992-08-01 GMT"
#&gt;  [33] "1992-09-01 GMT" "1992-10-01 GMT" "1992-11-01 GMT" "1992-12-01 GMT"
#&gt;  [37] "1993-01-01 GMT" "1993-02-01 GMT" "1993-03-01 GMT" "1993-04-01 GMT"
#&gt;  [41] "1993-05-01 GMT" "1993-06-01 GMT" "1993-07-01 GMT" "1993-08-01 GMT"
#&gt;  [45] "1993-09-01 GMT" "1993-10-01 GMT" "1993-11-01 GMT" "1993-12-01 GMT"
#&gt;  [49] "1994-01-01 GMT" "1994-02-01 GMT" "1994-03-01 GMT" "1994-04-01 GMT"
#&gt;  [53] "1994-05-01 GMT" "1994-06-01 GMT" "1994-07-01 GMT" "1994-08-01 GMT"
#&gt;  [57] "1994-09-01 GMT" "1994-10-01 GMT" "1994-11-01 GMT" "1994-12-01 GMT"
#&gt;  [61] "1995-01-01 GMT" "1995-02-01 GMT" "1995-03-01 GMT" "1995-04-01 GMT"
#&gt;  [65] "1995-05-01 GMT" "1995-06-01 GMT" "1995-07-01 GMT" "1995-08-01 GMT"
#&gt;  [69] "1995-09-01 GMT" "1995-10-01 GMT" "1995-11-01 GMT" "1995-12-01 GMT"
#&gt;  [73] "1996-01-01 GMT" "1996-02-01 GMT" "1996-03-01 GMT" "1996-04-01 GMT"
#&gt;  [77] "1996-05-01 GMT" "1996-06-01 GMT" "1996-07-01 GMT" "1996-08-01 GMT"
#&gt;  [81] "1996-09-01 GMT" "1996-10-01 GMT" "1996-11-01 GMT" "1996-12-01 GMT"
#&gt;  [85] "1997-01-01 GMT" "1997-02-01 GMT" "1997-03-01 GMT" "1997-04-01 GMT"
#&gt;  [89] "1997-05-01 GMT" "1997-06-01 GMT" "1997-07-01 GMT" "1997-08-01 GMT"
#&gt;  [93] "1997-09-01 GMT" "1997-10-01 GMT" "1997-11-01 GMT" "1997-12-01 GMT"
#&gt;  [97] "1998-01-01 GMT" "1998-02-01 GMT" "1998-03-01 GMT" "1998-04-01 GMT"
#&gt; [101] "1998-05-01 GMT" "1998-06-01 GMT" "1998-07-01 GMT" "1998-08-01 GMT"
#&gt; [105] "1998-09-01 GMT" "1998-10-01 GMT" "1998-11-01 GMT" "1998-12-01 GMT"
#&gt; [109] "1999-01-01 GMT" "1999-02-01 GMT" "1999-03-01 GMT" "1999-04-01 GMT"
#&gt; [113] "1999-05-01 GMT" "1999-06-01 GMT" "1999-07-01 GMT" "1999-08-01 GMT"
#&gt; [117] "1999-09-01 GMT" "1999-10-01 GMT" "1999-11-01 GMT" "1999-12-01 GMT"
#&gt; [121] "2000-01-01 GMT" "2000-02-01 GMT" "2000-03-01 GMT" "2000-04-01 GMT"
#&gt; [125] "2000-05-01 GMT" "2000-06-01 GMT" "2000-07-01 GMT" "2000-08-01 GMT"
#&gt; [129] "2000-09-01 GMT" "2000-10-01 GMT" "2000-11-01 GMT" "2000-12-01 GMT"
#&gt; [133] "2001-01-01 GMT" "2001-02-01 GMT" "2001-03-01 GMT" "2001-04-01 GMT"
#&gt; [137] "2001-05-01 GMT" "2001-06-01 GMT" "2001-07-01 GMT" "2001-08-01 GMT"
#&gt; [141] "2001-09-01 GMT" "2001-10-01 GMT" "2001-11-01 GMT" "2001-12-01 GMT"
#&gt; [145] "2002-01-01 GMT" "2002-02-01 GMT" "2002-03-01 GMT" "2002-04-01 GMT"
#&gt; [149] "2002-05-01 GMT" "2002-06-01 GMT" "2002-07-01 GMT" "2002-08-01 GMT"
#&gt; [153] "2002-09-01 GMT" "2002-10-01 GMT" "2002-11-01 GMT" "2002-12-01 GMT"
#&gt; [157] "2003-01-01 GMT" "2003-02-01 GMT" "2003-03-01 GMT" "2003-04-01 GMT"
#&gt; [161] "2003-05-01 GMT" "2003-06-01 GMT" "2003-07-01 GMT" "2003-08-01 GMT"
#&gt; [165] "2003-09-01 GMT" "2003-10-01 GMT" "2003-11-01 GMT" "2003-12-01 GMT"
#&gt; [169] "2004-01-01 GMT" "2004-02-01 GMT" "2004-03-01 GMT" "2004-04-01 GMT"
#&gt; [173] "2004-05-01 GMT" "2004-06-01 GMT" "2004-07-01 GMT" "2004-08-01 GMT"
#&gt; [177] "2004-09-01 GMT" "2004-10-01 GMT" "2004-11-01 GMT" "2004-12-01 GMT"
#&gt; [181] "2005-01-01 GMT" "2005-02-01 GMT" "2005-03-01 GMT" "2005-04-01 GMT"
#&gt; [185] "2005-05-01 GMT" "2005-06-01 GMT" "2005-07-01 GMT" "2005-08-01 GMT"
#&gt; [189] "2005-09-01 GMT" "2005-10-01 GMT" "2005-11-01 GMT" "2005-12-01 GMT"
#&gt; [193] "2006-01-01 GMT" "2006-02-01 GMT" "2006-03-01 GMT" "2006-04-01 GMT"
#&gt; [197] "2006-05-01 GMT" "2006-06-01 GMT" "2006-07-01 GMT" "2006-08-01 GMT"
#&gt; [201] "2006-09-01 GMT" "2006-10-01 GMT" "2006-11-01 GMT" "2006-12-01 GMT"
#&gt; [205] "2007-01-01 GMT" "2007-02-01 GMT" "2007-03-01 GMT" "2007-04-01 GMT"
#&gt; [209] "2007-05-01 GMT" "2007-06-01 GMT" "2007-07-01 GMT" "2007-08-01 GMT"
#&gt; [213] "2007-09-01 GMT" "2007-10-01 GMT" "2007-11-01 GMT" "2007-12-01 GMT"
#&gt; [217] "2008-01-01 GMT" "2008-02-01 GMT" "2008-03-01 GMT" "2008-04-01 GMT"
#&gt; [221] "2008-05-01 GMT" "2008-06-01 GMT" "2008-07-01 GMT" "2008-08-01 GMT"
#&gt; [225] "2008-09-01 GMT" "2008-10-01 GMT" "2008-11-01 GMT" "2008-12-01 GMT"
#&gt; [229] "2009-01-01 GMT" "2009-02-01 GMT" "2009-03-01 GMT" "2009-04-01 GMT"
#&gt; [233] "2009-05-01 GMT" "2009-06-01 GMT" "2009-07-01 GMT" "2009-08-01 GMT"
#&gt; [237] "2009-09-01 GMT" "2009-10-01 GMT" "2009-11-01 GMT" "2009-12-01 GMT"
#&gt; [241] "2010-01-01 GMT" "2010-02-01 GMT" "2010-03-01 GMT" "2010-04-01 GMT"
#&gt; 
#&gt; $train.holiday.names
#&gt; NULL
#&gt; 
#&gt; $train.component.cols
#&gt;    additive_terms yearly multiplicative_terms
#&gt; 1               1      1                    0
#&gt; 2               1      1                    0
#&gt; 3               1      1                    0
#&gt; 4               1      1                    0
#&gt; 5               1      1                    0
#&gt; 6               1      1                    0
#&gt; 7               1      1                    0
#&gt; 8               1      1                    0
#&gt; 9               1      1                    0
#&gt; 10              1      1                    0
#&gt; 11              1      1                    0
#&gt; 12              1      1                    0
#&gt; 13              1      1                    0
#&gt; 14              1      1                    0
#&gt; 15              1      1                    0
#&gt; 16              1      1                    0
#&gt; 17              1      1                    0
#&gt; 18              1      1                    0
#&gt; 19              1      1                    0
#&gt; 20              1      1                    0
#&gt; 
#&gt; $component.modes
#&gt; $component.modes$additive
#&gt; [1] "yearly"                    "additive_terms"           
#&gt; [3] "extra_regressors_additive" "holidays"                 
#&gt; 
#&gt; $component.modes$multiplicative
#&gt; [1] "multiplicative_terms"            "extra_regressors_multiplicative"
#&gt; 
#&gt; 
#&gt; $fit.kwargs
#&gt; list()
#&gt; 
#&gt; attr(,"class")
#&gt; [1] "prophet" "list"   
#&gt; 
#&gt; ---
#&gt; Model 2: Catboost Errors
#&gt; 
#&gt; CatBoost model (1000 trees)
#&gt; Loss function: RMSE
#&gt; Fit to 2 features</div><div class='input'>
</div></span></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Alberto Almuiña.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


